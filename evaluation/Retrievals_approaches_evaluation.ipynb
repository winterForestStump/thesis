{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdG4J24kT6Ox9zubhGXDss",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winterForestStump/thesis/blob/main/evaluation/Retrievals_approaches_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqMqPVKIzJy0",
        "outputId": "9f6db3e8-2aa0-4d35-c37e-e8d26905f1a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.73.tar.gz (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.73-cp310-cp310-linux_x86_64.whl size=61754255 sha256=2dd74cfaaf5804f890c9f2fde646c93faca5f8bdef7a27eb79d0d4db1cb279eb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/96/43/7e194c954ac912d68301956136f2b931efadf8f56c71fd5400\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.73\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-fp16.gguf --local-dir ./models --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBCZobiVzKeq",
        "outputId": "d22e5895-e2af-48cc-bff3-63dd44a2c5a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf to /root/.cache/huggingface/hub/tmplxlp382v\n",
            "Phi-3-mini-4k-instruct-fp16.gguf: 100% 7.64G/7.64G [01:05<00:00, 117MB/s]\n",
            "./models/Phi-3-mini-4k-instruct-fp16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCqrdCON0gMn",
        "outputId": "71128601-b8f9-4951-d634-1a498243bde5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B4eLZEi3ZND",
        "outputId": "c3ed6ea3-b22f-4729-d193-63f82d1b054f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.8/120.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "TEMP = 0\n",
        "N_CTX = 4096\n",
        "N_GPU_L = -1\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/models/Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    temperature=TEMP,\n",
        "    n_ctx=N_CTX,\n",
        "    n_gpu_layers = N_GPU_L,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RRPjbl3zOj7",
        "outputId": "c13f0eab-85b6-4e16-adb6-af60e53d2370"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /content/models/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  130 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = phi3\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = Phi3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   187.88 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  7100.64 MiB\n",
            "....................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1536.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =    18.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.88 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "    <|assistant|> You are a grader assessing relevance of a retrieved document to a user question. If the document contains keywords related to the user question, grade it as relevant.\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination. <|end|>\n",
        "    <|user|> Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: {question} <|end|>\n",
        "    <|assistant|>\n",
        "    \"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n",
        "\n",
        "retrieval_grader = prompt | llm | JsonOutputParser()"
      ],
      "metadata": {
        "id": "B0cVnmRczRED"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "app1 = pd.read_json('/content/drive/MyDrive/Thesis/retrievers/results_approach1.json')\n",
        "len(app1)"
      ],
      "metadata": {
        "id": "KNCptS7q0RiW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d85270-1882-4de7-b037-6c282384d5fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "for i in range(len(app1)):\n",
        "  question = app1['question'][i]\n",
        "  doc_txt = app1['context_cosine'][i]['page_content']\n",
        "  answer = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
        "  results_list.append(pd.DataFrame({'question_name': question, 'answer': [answer]}))\n",
        "results = pd.concat(results_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "bmEQM-uf4Vxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02a9758a-69c0-4a0c-ceb3-826c6d486827"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.21 ms /     7 runs   (    0.60 ms per token,  1662.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4565.69 ms /   792 tokens (    5.76 ms per token,   173.47 tokens per second)\n",
            "llama_print_timings:        eval time =     336.35 ms /     6 runs   (   56.06 ms per token,    17.84 tokens per second)\n",
            "llama_print_timings:       total time =    5398.24 ms /   798 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.79 ms /     7 runs   (    0.68 ms per token,  1462.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3785.42 ms /   613 tokens (    6.18 ms per token,   161.94 tokens per second)\n",
            "llama_print_timings:        eval time =     234.74 ms /     6 runs   (   39.12 ms per token,    25.56 tokens per second)\n",
            "llama_print_timings:       total time =    4723.78 ms /   619 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.15 ms /     6 runs   (    0.53 ms per token,  1902.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4134.00 ms /   776 tokens (    5.33 ms per token,   187.71 tokens per second)\n",
            "llama_print_timings:        eval time =     191.74 ms /     5 runs   (   38.35 ms per token,    26.08 tokens per second)\n",
            "llama_print_timings:       total time =    4634.45 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.43 ms /     7 runs   (    0.49 ms per token,  2040.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3470.05 ms /   663 tokens (    5.23 ms per token,   191.06 tokens per second)\n",
            "llama_print_timings:        eval time =     231.25 ms /     6 runs   (   38.54 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    3884.13 ms /   669 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.43 ms /     7 runs   (    0.63 ms per token,  1580.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3001.46 ms /   572 tokens (    5.25 ms per token,   190.57 tokens per second)\n",
            "llama_print_timings:        eval time =     229.75 ms /     6 runs   (   38.29 ms per token,    26.12 tokens per second)\n",
            "llama_print_timings:       total time =    3482.85 ms /   578 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.43 ms /     7 runs   (    0.49 ms per token,  2039.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2693.61 ms /   516 tokens (    5.22 ms per token,   191.56 tokens per second)\n",
            "llama_print_timings:        eval time =     230.63 ms /     6 runs   (   38.44 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    3093.09 ms /   522 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.31 ms /     7 runs   (    0.47 ms per token,  2114.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2305.00 ms /   445 tokens (    5.18 ms per token,   193.06 tokens per second)\n",
            "llama_print_timings:        eval time =     226.38 ms /     6 runs   (   37.73 ms per token,    26.50 tokens per second)\n",
            "llama_print_timings:       total time =    2663.63 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.53 ms /     7 runs   (    0.50 ms per token,  1980.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1751.53 ms /   340 tokens (    5.15 ms per token,   194.12 tokens per second)\n",
            "llama_print_timings:        eval time =     223.97 ms /     6 runs   (   37.33 ms per token,    26.79 tokens per second)\n",
            "llama_print_timings:       total time =    2079.75 ms /   346 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.07 ms /     7 runs   (    0.58 ms per token,  1720.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3443.51 ms /   654 tokens (    5.27 ms per token,   189.92 tokens per second)\n",
            "llama_print_timings:        eval time =     234.86 ms /     6 runs   (   39.14 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    3906.13 ms /   660 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1907.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2763.59 ms /   523 tokens (    5.28 ms per token,   189.25 tokens per second)\n",
            "llama_print_timings:        eval time =     228.58 ms /     6 runs   (   38.10 ms per token,    26.25 tokens per second)\n",
            "llama_print_timings:       total time =    3239.89 ms /   529 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.37 ms /     6 runs   (    0.56 ms per token,  1781.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4128.07 ms /   774 tokens (    5.33 ms per token,   187.50 tokens per second)\n",
            "llama_print_timings:        eval time =     194.01 ms /     5 runs   (   38.80 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    4544.63 ms /   779 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.37 ms /     7 runs   (    0.62 ms per token,  1601.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2341.14 ms /   444 tokens (    5.27 ms per token,   189.65 tokens per second)\n",
            "llama_print_timings:        eval time =     231.69 ms /     6 runs   (   38.61 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    2784.06 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.73 ms /     7 runs   (    0.68 ms per token,  1478.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2077.42 ms /   398 tokens (    5.22 ms per token,   191.58 tokens per second)\n",
            "llama_print_timings:        eval time =     232.10 ms /     6 runs   (   38.68 ms per token,    25.85 tokens per second)\n",
            "llama_print_timings:       total time =    2523.28 ms /   404 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.61 ms /     7 runs   (    0.66 ms per token,  1519.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2214.89 ms /   422 tokens (    5.25 ms per token,   190.53 tokens per second)\n",
            "llama_print_timings:        eval time =     232.52 ms /     6 runs   (   38.75 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =    2664.62 ms /   428 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.64 ms /     7 runs   (    0.52 ms per token,  1924.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     890.33 ms /   173 tokens (    5.15 ms per token,   194.31 tokens per second)\n",
            "llama_print_timings:        eval time =     225.94 ms /     6 runs   (   37.66 ms per token,    26.56 tokens per second)\n",
            "llama_print_timings:       total time =    1179.90 ms /   179 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.71 ms /     7 runs   (    0.53 ms per token,  1889.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3462.96 ms /   651 tokens (    5.32 ms per token,   187.99 tokens per second)\n",
            "llama_print_timings:        eval time =     237.74 ms /     6 runs   (   39.62 ms per token,    25.24 tokens per second)\n",
            "llama_print_timings:       total time =    3882.25 ms /   657 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.33 ms /     7 runs   (    0.48 ms per token,  2100.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2285.87 ms /   440 tokens (    5.20 ms per token,   192.49 tokens per second)\n",
            "llama_print_timings:        eval time =     271.02 ms /     7 runs   (   38.72 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    2684.35 ms /   447 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     6 runs   (    0.60 ms per token,  1663.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3711.48 ms /   692 tokens (    5.36 ms per token,   186.45 tokens per second)\n",
            "llama_print_timings:        eval time =     197.95 ms /     5 runs   (   39.59 ms per token,    25.26 tokens per second)\n",
            "llama_print_timings:       total time =    4167.31 ms /   697 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.28 ms /     7 runs   (    0.47 ms per token,  2136.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2826.94 ms /   531 tokens (    5.32 ms per token,   187.84 tokens per second)\n",
            "llama_print_timings:        eval time =     232.06 ms /     6 runs   (   38.68 ms per token,    25.85 tokens per second)\n",
            "llama_print_timings:       total time =    3257.74 ms /   537 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.80 ms /     7 runs   (    0.54 ms per token,  1844.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3260.66 ms /   614 tokens (    5.31 ms per token,   188.31 tokens per second)\n",
            "llama_print_timings:        eval time =     232.21 ms /     6 runs   (   38.70 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    3663.74 ms /   620 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.24 ms /     7 runs   (    0.61 ms per token,  1652.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4112.57 ms /   751 tokens (    5.48 ms per token,   182.61 tokens per second)\n",
            "llama_print_timings:        eval time =     238.76 ms /     6 runs   (   39.79 ms per token,    25.13 tokens per second)\n",
            "llama_print_timings:       total time =    4653.82 ms /   757 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.15 ms /     6 runs   (    0.53 ms per token,  1904.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5998.35 ms /  1077 tokens (    5.57 ms per token,   179.55 tokens per second)\n",
            "llama_print_timings:        eval time =     203.35 ms /     5 runs   (   40.67 ms per token,    24.59 tokens per second)\n",
            "llama_print_timings:       total time =    6602.55 ms /  1082 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.75 ms /     7 runs   (    0.54 ms per token,  1867.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2090.32 ms /   397 tokens (    5.27 ms per token,   189.92 tokens per second)\n",
            "llama_print_timings:        eval time =     235.08 ms /     6 runs   (   39.18 ms per token,    25.52 tokens per second)\n",
            "llama_print_timings:       total time =    2441.49 ms /   403 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.58 ms /     7 runs   (    0.51 ms per token,  1955.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2066.84 ms /   390 tokens (    5.30 ms per token,   188.69 tokens per second)\n",
            "llama_print_timings:        eval time =     234.05 ms /     6 runs   (   39.01 ms per token,    25.64 tokens per second)\n",
            "llama_print_timings:       total time =    2418.38 ms /   396 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.73 ms /     7 runs   (    0.68 ms per token,  1481.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2363.20 ms /   444 tokens (    5.32 ms per token,   187.88 tokens per second)\n",
            "llama_print_timings:        eval time =     235.57 ms /     6 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    2776.91 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.73 ms /     7 runs   (    0.53 ms per token,  1879.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1932.55 ms /   365 tokens (    5.29 ms per token,   188.87 tokens per second)\n",
            "llama_print_timings:        eval time =     227.88 ms /     6 runs   (   37.98 ms per token,    26.33 tokens per second)\n",
            "llama_print_timings:       total time =    2337.43 ms /   371 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.84 ms /     7 runs   (    0.55 ms per token,  1824.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2532.52 ms /   479 tokens (    5.29 ms per token,   189.14 tokens per second)\n",
            "llama_print_timings:        eval time =     234.97 ms /     6 runs   (   39.16 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    2900.88 ms /   485 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.75 ms /     7 runs   (    0.54 ms per token,  1866.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1502.49 ms /   275 tokens (    5.46 ms per token,   183.03 tokens per second)\n",
            "llama_print_timings:        eval time =     233.88 ms /     6 runs   (   38.98 ms per token,    25.65 tokens per second)\n",
            "llama_print_timings:       total time =    1829.38 ms /   281 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.52 ms /     7 runs   (    0.50 ms per token,  1989.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2530.93 ms /   479 tokens (    5.28 ms per token,   189.26 tokens per second)\n",
            "llama_print_timings:        eval time =     231.56 ms /     6 runs   (   38.59 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2904.41 ms /   485 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.81 ms /     7 runs   (    0.69 ms per token,  1456.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2219.03 ms /   424 tokens (    5.23 ms per token,   191.07 tokens per second)\n",
            "llama_print_timings:        eval time =     273.19 ms /     7 runs   (   39.03 ms per token,    25.62 tokens per second)\n",
            "llama_print_timings:       total time =    2654.80 ms /   431 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.79 ms /     7 runs   (    0.54 ms per token,  1845.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2896.45 ms /   544 tokens (    5.32 ms per token,   187.82 tokens per second)\n",
            "llama_print_timings:        eval time =     270.80 ms /     7 runs   (   38.69 ms per token,    25.85 tokens per second)\n",
            "llama_print_timings:       total time =    3427.74 ms /   551 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.20 ms /     7 runs   (    0.46 ms per token,  2188.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2781.53 ms /   525 tokens (    5.30 ms per token,   188.75 tokens per second)\n",
            "llama_print_timings:        eval time =     235.24 ms /     6 runs   (   39.21 ms per token,    25.51 tokens per second)\n",
            "llama_print_timings:       total time =    3167.34 ms /   531 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.05 ms /     7 runs   (    0.44 ms per token,  2292.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1457.55 ms /   269 tokens (    5.42 ms per token,   184.56 tokens per second)\n",
            "llama_print_timings:        eval time =     230.27 ms /     6 runs   (   38.38 ms per token,    26.06 tokens per second)\n",
            "llama_print_timings:       total time =    1771.32 ms /   275 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.78 ms /     7 runs   (    0.54 ms per token,  1853.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2123.89 ms /   407 tokens (    5.22 ms per token,   191.63 tokens per second)\n",
            "llama_print_timings:        eval time =     230.43 ms /     6 runs   (   38.40 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    2473.86 ms /   413 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.49 ms /     7 runs   (    0.64 ms per token,  1557.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2082.41 ms /   399 tokens (    5.22 ms per token,   191.61 tokens per second)\n",
            "llama_print_timings:        eval time =     232.68 ms /     6 runs   (   38.78 ms per token,    25.79 tokens per second)\n",
            "llama_print_timings:       total time =    2459.91 ms /   405 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.73 ms /     7 runs   (    0.68 ms per token,  1481.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     483.43 ms /    93 tokens (    5.20 ms per token,   192.38 tokens per second)\n",
            "llama_print_timings:        eval time =     224.08 ms /     6 runs   (   37.35 ms per token,    26.78 tokens per second)\n",
            "llama_print_timings:       total time =     782.56 ms /    99 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.50 ms /     7 runs   (    0.50 ms per token,  2001.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2658.07 ms /   503 tokens (    5.28 ms per token,   189.24 tokens per second)\n",
            "llama_print_timings:        eval time =     232.18 ms /     6 runs   (   38.70 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    3090.78 ms /   509 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.57 ms /     7 runs   (    0.51 ms per token,  1960.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2199.07 ms /   420 tokens (    5.24 ms per token,   190.99 tokens per second)\n",
            "llama_print_timings:        eval time =     229.47 ms /     6 runs   (   38.25 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    2549.87 ms /   426 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.99 ms /     7 runs   (    0.57 ms per token,  1755.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2419.27 ms /   462 tokens (    5.24 ms per token,   190.97 tokens per second)\n",
            "llama_print_timings:        eval time =     232.52 ms /     6 runs   (   38.75 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =    2788.29 ms /   468 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.68 ms /     6 runs   (    0.61 ms per token,  1630.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3117.09 ms /   588 tokens (    5.30 ms per token,   188.64 tokens per second)\n",
            "llama_print_timings:        eval time =     194.57 ms /     5 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    3481.36 ms /   593 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.20 ms /     7 runs   (    0.60 ms per token,  1665.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2753.10 ms /   515 tokens (    5.35 ms per token,   187.06 tokens per second)\n",
            "llama_print_timings:        eval time =     233.22 ms /     6 runs   (   38.87 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    3249.31 ms /   521 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.29 ms /     7 runs   (    0.47 ms per token,  2124.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3382.81 ms /   640 tokens (    5.29 ms per token,   189.19 tokens per second)\n",
            "llama_print_timings:        eval time =     273.32 ms /     7 runs   (   39.05 ms per token,    25.61 tokens per second)\n",
            "llama_print_timings:       total time =    3836.04 ms /   647 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.71 ms /     6 runs   (    0.45 ms per token,  2213.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3071.17 ms /   579 tokens (    5.30 ms per token,   188.53 tokens per second)\n",
            "llama_print_timings:        eval time =     190.77 ms /     5 runs   (   38.15 ms per token,    26.21 tokens per second)\n",
            "llama_print_timings:       total time =    3417.88 ms /   584 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.04 ms /     7 runs   (    0.58 ms per token,  1731.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2683.64 ms /   507 tokens (    5.29 ms per token,   188.92 tokens per second)\n",
            "llama_print_timings:        eval time =     233.43 ms /     6 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    3079.56 ms /   513 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.74 ms /     7 runs   (    0.53 ms per token,  1871.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3322.04 ms /   621 tokens (    5.35 ms per token,   186.93 tokens per second)\n",
            "llama_print_timings:        eval time =     229.70 ms /     6 runs   (   38.28 ms per token,    26.12 tokens per second)\n",
            "llama_print_timings:       total time =    3827.21 ms /   627 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.65 ms /     7 runs   (    0.52 ms per token,  1918.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3971.40 ms /   741 tokens (    5.36 ms per token,   186.58 tokens per second)\n",
            "llama_print_timings:        eval time =     236.11 ms /     6 runs   (   39.35 ms per token,    25.41 tokens per second)\n",
            "llama_print_timings:       total time =    4406.30 ms /   747 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1939.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3298.69 ms /   621 tokens (    5.31 ms per token,   188.26 tokens per second)\n",
            "llama_print_timings:        eval time =     230.84 ms /     6 runs   (   38.47 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    3702.97 ms /   627 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.96 ms /     6 runs   (    0.66 ms per token,  1516.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1656.46 ms /   319 tokens (    5.19 ms per token,   192.58 tokens per second)\n",
            "llama_print_timings:        eval time =     192.56 ms /     5 runs   (   38.51 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    1995.02 ms /   324 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.91 ms /     6 runs   (    0.65 ms per token,  1535.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     229.60 ms /     6 runs   (   38.27 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =     260.79 ms /     6 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.63 ms /     7 runs   (    0.52 ms per token,  1930.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2528.31 ms /   478 tokens (    5.29 ms per token,   189.06 tokens per second)\n",
            "llama_print_timings:        eval time =     232.53 ms /     6 runs   (   38.75 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =    2966.74 ms /   484 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.72 ms /     7 runs   (    0.53 ms per token,  1882.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2161.09 ms /   412 tokens (    5.25 ms per token,   190.64 tokens per second)\n",
            "llama_print_timings:        eval time =     229.46 ms /     6 runs   (   38.24 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    2512.29 ms /   418 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.85 ms /     7 runs   (    0.55 ms per token,  1818.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2253.97 ms /   432 tokens (    5.22 ms per token,   191.66 tokens per second)\n",
            "llama_print_timings:        eval time =     232.27 ms /     6 runs   (   38.71 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    2615.34 ms /   438 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.95 ms /     7 runs   (    0.56 ms per token,  1772.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2341.35 ms /   443 tokens (    5.29 ms per token,   189.21 tokens per second)\n",
            "llama_print_timings:        eval time =     231.14 ms /     6 runs   (   38.52 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    2705.49 ms /   449 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.13 ms /     7 runs   (    0.59 ms per token,  1695.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2841.61 ms /   536 tokens (    5.30 ms per token,   188.63 tokens per second)\n",
            "llama_print_timings:        eval time =     273.69 ms /     7 runs   (   39.10 ms per token,    25.58 tokens per second)\n",
            "llama_print_timings:       total time =    3353.24 ms /   543 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.09 ms /     6 runs   (    0.51 ms per token,  1943.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4824.85 ms /   888 tokens (    5.43 ms per token,   184.05 tokens per second)\n",
            "llama_print_timings:        eval time =     198.35 ms /     5 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    5296.53 ms /   893 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1964.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2875.43 ms /   544 tokens (    5.29 ms per token,   189.19 tokens per second)\n",
            "llama_print_timings:        eval time =     272.24 ms /     7 runs   (   38.89 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    3302.91 ms /   551 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.57 ms /     7 runs   (    0.65 ms per token,  1533.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2569.47 ms /   488 tokens (    5.27 ms per token,   189.92 tokens per second)\n",
            "llama_print_timings:        eval time =     234.65 ms /     6 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    2965.71 ms /   494 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.75 ms /     7 runs   (    0.68 ms per token,  1472.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2230.91 ms /   423 tokens (    5.27 ms per token,   189.61 tokens per second)\n",
            "llama_print_timings:        eval time =     236.08 ms /     6 runs   (   39.35 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    2697.60 ms /   429 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1936.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2213.43 ms /   424 tokens (    5.22 ms per token,   191.56 tokens per second)\n",
            "llama_print_timings:        eval time =     233.96 ms /     6 runs   (   38.99 ms per token,    25.65 tokens per second)\n",
            "llama_print_timings:       total time =    2572.74 ms /   430 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.78 ms /     7 runs   (    0.54 ms per token,  1850.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2213.03 ms /   421 tokens (    5.26 ms per token,   190.24 tokens per second)\n",
            "llama_print_timings:        eval time =     234.01 ms /     6 runs   (   39.00 ms per token,    25.64 tokens per second)\n",
            "llama_print_timings:       total time =    2573.42 ms /   427 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.63 ms /     7 runs   (    0.52 ms per token,  1927.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2195.59 ms /   414 tokens (    5.30 ms per token,   188.56 tokens per second)\n",
            "llama_print_timings:        eval time =     234.28 ms /     6 runs   (   39.05 ms per token,    25.61 tokens per second)\n",
            "llama_print_timings:       total time =    2549.88 ms /   420 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.77 ms /     7 runs   (    0.68 ms per token,  1467.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2301.23 ms /   439 tokens (    5.24 ms per token,   190.77 tokens per second)\n",
            "llama_print_timings:        eval time =     234.41 ms /     6 runs   (   39.07 ms per token,    25.60 tokens per second)\n",
            "llama_print_timings:       total time =    2685.17 ms /   445 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.71 ms /     7 runs   (    0.67 ms per token,  1484.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2196.97 ms /   414 tokens (    5.31 ms per token,   188.44 tokens per second)\n",
            "llama_print_timings:        eval time =     234.12 ms /     6 runs   (   39.02 ms per token,    25.63 tokens per second)\n",
            "llama_print_timings:       total time =    2655.58 ms /   420 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.71 ms /     7 runs   (    0.53 ms per token,  1889.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2433.74 ms /   458 tokens (    5.31 ms per token,   188.19 tokens per second)\n",
            "llama_print_timings:        eval time =     233.71 ms /     6 runs   (   38.95 ms per token,    25.67 tokens per second)\n",
            "llama_print_timings:       total time =    2814.29 ms /   464 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "for i in range(len(results)):\n",
        "  if results['answer'][i]['score'] == 'yes':\n",
        "    score += 1\n",
        "total_score_app1 = score/len(results)\n",
        "print(f'Total score for Approach # 1 is {total_score_app1}')"
      ],
      "metadata": {
        "id": "9i39-i_L-6od",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24690b76-56fa-4780-daf3-adcaf842059e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total score for Approach # 1 is 0.703125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app2 = pd.read_json('/content/drive/MyDrive/Thesis/retrievers/results_approach2.json')\n",
        "len(app2)"
      ],
      "metadata": {
        "id": "TPCss6Pq_mRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f999eec8-65e2-4331-f406-aebb6ddbf9de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from json import JSONDecodeError\n",
        "from langchain_core.exceptions import OutputParserException"
      ],
      "metadata": {
        "id": "m4k3GKBxyzSm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "for i in range(len(app2)):\n",
        "  question = app2['question_name'][i]\n",
        "  doc_txt = app2['context_cosine_name'][i]['page_content']\n",
        "  try:\n",
        "    answer = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
        "    results_list.append(pd.DataFrame({'question_name': question, 'answer': [answer]}))\n",
        "  except JSONDecodeError as e:\n",
        "    print(f\"JSONDecodeError occurred for question: {question}. Skipping...\")\n",
        "    continue\n",
        "  except OutputParserException as e:\n",
        "    print(f\"OutputParserException occurred for question: {question}. Skipping...\")\n",
        "    continue\n",
        "results = pd.concat(results_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "jlDB8l_-AAeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247fe1ae-da19-446d-ae23-121e186562d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.62 ms /     6 runs   (    0.60 ms per token,  1657.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6070.49 ms /  1083 tokens (    5.61 ms per token,   178.40 tokens per second)\n",
            "llama_print_timings:        eval time =     203.38 ms /     5 runs   (   40.68 ms per token,    24.58 tokens per second)\n",
            "llama_print_timings:       total time =    6689.56 ms /  1088 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.51 ms /     7 runs   (    0.50 ms per token,  1992.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5093.22 ms /   926 tokens (    5.50 ms per token,   181.81 tokens per second)\n",
            "llama_print_timings:        eval time =     243.68 ms /     6 runs   (   40.61 ms per token,    24.62 tokens per second)\n",
            "llama_print_timings:       total time =    5602.18 ms /   932 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.84 ms /     7 runs   (    0.55 ms per token,  1821.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4442.18 ms /   814 tokens (    5.46 ms per token,   183.24 tokens per second)\n",
            "llama_print_timings:        eval time =     238.73 ms /     6 runs   (   39.79 ms per token,    25.13 tokens per second)\n",
            "llama_print_timings:       total time =    4907.84 ms /   820 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.83 ms /     7 runs   (    0.55 ms per token,  1826.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5149.55 ms /   930 tokens (    5.54 ms per token,   180.60 tokens per second)\n",
            "llama_print_timings:        eval time =     245.16 ms /     6 runs   (   40.86 ms per token,    24.47 tokens per second)\n",
            "llama_print_timings:       total time =    5767.25 ms /   936 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.37 ms /     7 runs   (    0.48 ms per token,  2076.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2753.76 ms /   520 tokens (    5.30 ms per token,   188.83 tokens per second)\n",
            "llama_print_timings:        eval time =     233.93 ms /     6 runs   (   38.99 ms per token,    25.65 tokens per second)\n",
            "llama_print_timings:       total time =    3138.29 ms /   526 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.73 ms /     7 runs   (    0.53 ms per token,  1876.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2746.18 ms /   516 tokens (    5.32 ms per token,   187.90 tokens per second)\n",
            "llama_print_timings:        eval time =     234.49 ms /     6 runs   (   39.08 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    3126.57 ms /   522 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.54 ms /     7 runs   (    0.51 ms per token,  1975.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5075.40 ms /   923 tokens (    5.50 ms per token,   181.86 tokens per second)\n",
            "llama_print_timings:        eval time =     241.31 ms /     6 runs   (   40.22 ms per token,    24.86 tokens per second)\n",
            "llama_print_timings:       total time =    5676.18 ms /   929 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.49 ms /     7 runs   (    0.50 ms per token,  2003.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4358.47 ms /   807 tokens (    5.40 ms per token,   185.16 tokens per second)\n",
            "llama_print_timings:        eval time =     236.40 ms /     6 runs   (   39.40 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    4815.64 ms /   813 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      53.13 ms /    96 runs   (    0.55 ms per token,  1807.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5947.72 ms /  1078 tokens (    5.52 ms per token,   181.25 tokens per second)\n",
            "llama_print_timings:        eval time =    3841.71 ms /    95 runs   (   40.44 ms per token,    24.73 tokens per second)\n",
            "llama_print_timings:       total time =   10466.77 ms /  1173 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1965.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4974.70 ms /   920 tokens (    5.41 ms per token,   184.94 tokens per second)\n",
            "llama_print_timings:        eval time =     281.37 ms /     7 runs   (   40.20 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =    5505.20 ms /   927 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.91 ms /     6 runs   (    0.48 ms per token,  2062.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5990.30 ms /  1087 tokens (    5.51 ms per token,   181.46 tokens per second)\n",
            "llama_print_timings:        eval time =     200.95 ms /     5 runs   (   40.19 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =    6592.30 ms /  1092 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.10 ms /     7 runs   (    0.59 ms per token,  1706.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4701.28 ms /   870 tokens (    5.40 ms per token,   185.06 tokens per second)\n",
            "llama_print_timings:        eval time =     254.25 ms /     6 runs   (   42.38 ms per token,    23.60 tokens per second)\n",
            "llama_print_timings:       total time =    5231.27 ms /   876 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.75 ms /     6 runs   (    0.62 ms per token,  1601.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4095.41 ms /   759 tokens (    5.40 ms per token,   185.33 tokens per second)\n",
            "llama_print_timings:        eval time =     197.78 ms /     5 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    4626.05 ms /   764 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.65 ms /     9 runs   (    0.52 ms per token,  1937.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3985.51 ms /   743 tokens (    5.36 ms per token,   186.43 tokens per second)\n",
            "llama_print_timings:        eval time =     312.91 ms /     8 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    4551.97 ms /   751 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.82 ms /     7 runs   (    0.55 ms per token,  1831.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5310.21 ms /   976 tokens (    5.44 ms per token,   183.80 tokens per second)\n",
            "llama_print_timings:        eval time =     240.72 ms /     6 runs   (   40.12 ms per token,    24.93 tokens per second)\n",
            "llama_print_timings:       total time =    5807.92 ms /   982 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.46 ms /     7 runs   (    0.49 ms per token,  2026.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5237.54 ms /   958 tokens (    5.47 ms per token,   182.91 tokens per second)\n",
            "llama_print_timings:        eval time =     239.66 ms /     6 runs   (   39.94 ms per token,    25.04 tokens per second)\n",
            "llama_print_timings:       total time =    5856.76 ms /   964 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      55.23 ms /   101 runs   (    0.55 ms per token,  1828.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1311.43 ms /   254 tokens (    5.16 ms per token,   193.68 tokens per second)\n",
            "llama_print_timings:        eval time =    3790.09 ms /   100 runs   (   37.90 ms per token,    26.38 tokens per second)\n",
            "llama_print_timings:       total time =    5480.47 ms /   354 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.90 ms /     6 runs   (    0.65 ms per token,  1536.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5379.36 ms /   978 tokens (    5.50 ms per token,   181.81 tokens per second)\n",
            "llama_print_timings:        eval time =     201.62 ms /     5 runs   (   40.32 ms per token,    24.80 tokens per second)\n",
            "llama_print_timings:       total time =    5950.37 ms /   983 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.19 ms /     6 runs   (    0.53 ms per token,  1879.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5993.14 ms /  1088 tokens (    5.51 ms per token,   181.54 tokens per second)\n",
            "llama_print_timings:        eval time =     240.73 ms /     6 runs   (   40.12 ms per token,    24.92 tokens per second)\n",
            "llama_print_timings:       total time =    6522.19 ms /  1094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.73 ms /     9 runs   (    0.64 ms per token,  1571.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4029.85 ms /   747 tokens (    5.39 ms per token,   185.37 tokens per second)\n",
            "llama_print_timings:        eval time =     315.84 ms /     8 runs   (   39.48 ms per token,    25.33 tokens per second)\n",
            "llama_print_timings:       total time =    4593.17 ms /   755 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.02 ms /     7 runs   (    0.57 ms per token,  1742.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2531.35 ms /   478 tokens (    5.30 ms per token,   188.83 tokens per second)\n",
            "llama_print_timings:        eval time =     231.01 ms /     6 runs   (   38.50 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    2992.83 ms /   484 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      57.52 ms /   107 runs   (    0.54 ms per token,  1860.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4129.49 ms /   768 tokens (    5.38 ms per token,   185.98 tokens per second)\n",
            "llama_print_timings:        eval time =    4185.76 ms /   106 runs   (   39.49 ms per token,    25.32 tokens per second)\n",
            "llama_print_timings:       total time =    8844.70 ms /   874 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =     135.13 ms /   256 runs   (    0.53 ms per token,  1894.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2918.37 ms /   546 tokens (    5.34 ms per token,   187.09 tokens per second)\n",
            "llama_print_timings:        eval time =   10003.79 ms /   255 runs   (   39.23 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =   14010.96 ms /   801 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      43.50 ms /    79 runs   (    0.55 ms per token,  1815.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6013.06 ms /  1086 tokens (    5.54 ms per token,   180.61 tokens per second)\n",
            "llama_print_timings:        eval time =    3158.35 ms /    78 runs   (   40.49 ms per token,    24.70 tokens per second)\n",
            "llama_print_timings:       total time =    9798.04 ms /  1164 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1964.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2777.82 ms /   525 tokens (    5.29 ms per token,   189.00 tokens per second)\n",
            "llama_print_timings:        eval time =     233.26 ms /     6 runs   (   38.88 ms per token,    25.72 tokens per second)\n",
            "llama_print_timings:       total time =    3158.50 ms /   531 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.92 ms /     7 runs   (    0.70 ms per token,  1423.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2707.62 ms /   507 tokens (    5.34 ms per token,   187.25 tokens per second)\n",
            "llama_print_timings:        eval time =     234.27 ms /     6 runs   (   39.05 ms per token,    25.61 tokens per second)\n",
            "llama_print_timings:       total time =    3207.76 ms /   513 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.58 ms /     7 runs   (    0.51 ms per token,  1954.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2562.85 ms /   487 tokens (    5.26 ms per token,   190.02 tokens per second)\n",
            "llama_print_timings:        eval time =     235.10 ms /     6 runs   (   39.18 ms per token,    25.52 tokens per second)\n",
            "llama_print_timings:       total time =    2939.86 ms /   493 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.59 ms /     7 runs   (    0.51 ms per token,  1949.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1535.23 ms /   283 tokens (    5.42 ms per token,   184.34 tokens per second)\n",
            "llama_print_timings:        eval time =     229.89 ms /     6 runs   (   38.32 ms per token,    26.10 tokens per second)\n",
            "llama_print_timings:       total time =    1854.12 ms /   289 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.96 ms /     6 runs   (    0.49 ms per token,  2026.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3800.16 ms /   710 tokens (    5.35 ms per token,   186.83 tokens per second)\n",
            "llama_print_timings:        eval time =     196.30 ms /     5 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    4187.20 ms /   715 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.06 ms /     6 runs   (    0.51 ms per token,  1959.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3999.70 ms /   740 tokens (    5.41 ms per token,   185.01 tokens per second)\n",
            "llama_print_timings:        eval time =     195.56 ms /     5 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    4516.28 ms /   745 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.74 ms /     7 runs   (    0.53 ms per token,  1873.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2247.60 ms /   429 tokens (    5.24 ms per token,   190.87 tokens per second)\n",
            "llama_print_timings:        eval time =     229.70 ms /     6 runs   (   38.28 ms per token,    26.12 tokens per second)\n",
            "llama_print_timings:       total time =    2609.64 ms /   435 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.54 ms /     7 runs   (    0.51 ms per token,  1980.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2905.56 ms /   549 tokens (    5.29 ms per token,   188.95 tokens per second)\n",
            "llama_print_timings:        eval time =     233.64 ms /     6 runs   (   38.94 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =    3291.51 ms /   555 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.57 ms /     7 runs   (    0.51 ms per token,  1961.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2382.25 ms /   456 tokens (    5.22 ms per token,   191.42 tokens per second)\n",
            "llama_print_timings:        eval time =     233.51 ms /     6 runs   (   38.92 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    2753.39 ms /   462 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.26 ms /     7 runs   (    0.61 ms per token,  1642.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2077.10 ms /   394 tokens (    5.27 ms per token,   189.69 tokens per second)\n",
            "llama_print_timings:        eval time =     232.98 ms /     6 runs   (   38.83 ms per token,    25.75 tokens per second)\n",
            "llama_print_timings:       total time =    2479.28 ms /   400 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1937.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3049.37 ms /   573 tokens (    5.32 ms per token,   187.91 tokens per second)\n",
            "llama_print_timings:        eval time =     232.25 ms /     6 runs   (   38.71 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    3509.05 ms /   579 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1936.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     266.95 ms /     7 runs   (   38.14 ms per token,    26.22 tokens per second)\n",
            "llama_print_timings:       total time =     290.44 ms /     7 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.57 ms /     7 runs   (    0.51 ms per token,  1961.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1856.32 ms /   355 tokens (    5.23 ms per token,   191.24 tokens per second)\n",
            "llama_print_timings:        eval time =     229.34 ms /     6 runs   (   38.22 ms per token,    26.16 tokens per second)\n",
            "llama_print_timings:       total time =    2195.97 ms /   361 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.52 ms /     7 runs   (    0.50 ms per token,  1987.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2030.98 ms /   392 tokens (    5.18 ms per token,   193.01 tokens per second)\n",
            "llama_print_timings:        eval time =     230.42 ms /     6 runs   (   38.40 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    2378.77 ms /   398 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.28 ms /     6 runs   (    0.55 ms per token,  1827.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5930.73 ms /  1070 tokens (    5.54 ms per token,   180.42 tokens per second)\n",
            "llama_print_timings:        eval time =     201.98 ms /     5 runs   (   40.40 ms per token,    24.76 tokens per second)\n",
            "llama_print_timings:       total time =    6635.77 ms /  1075 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.69 ms /     7 runs   (    0.53 ms per token,  1895.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2471.88 ms /   466 tokens (    5.30 ms per token,   188.52 tokens per second)\n",
            "llama_print_timings:        eval time =     233.97 ms /     6 runs   (   39.00 ms per token,    25.64 tokens per second)\n",
            "llama_print_timings:       total time =    2875.24 ms /   472 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       6.84 ms /    13 runs   (    0.53 ms per token,  1899.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3566.94 ms /   666 tokens (    5.36 ms per token,   186.71 tokens per second)\n",
            "llama_print_timings:        eval time =     475.53 ms /    12 runs   (   39.63 ms per token,    25.24 tokens per second)\n",
            "llama_print_timings:       total time =    4244.69 ms /   678 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OutputParserException occurred for question: Are there any tax-related risks or benefits for the COCA COLA CO mentioned?. Skipping...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.08 ms /     6 runs   (    0.51 ms per token,  1949.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3342.80 ms /   629 tokens (    5.31 ms per token,   188.17 tokens per second)\n",
            "llama_print_timings:        eval time =     193.61 ms /     5 runs   (   38.72 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    3706.30 ms /   634 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.70 ms /     9 runs   (    0.52 ms per token,  1914.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3962.08 ms /   735 tokens (    5.39 ms per token,   185.51 tokens per second)\n",
            "llama_print_timings:        eval time =     313.97 ms /     8 runs   (   39.25 ms per token,    25.48 tokens per second)\n",
            "llama_print_timings:       total time =    4608.40 ms /   743 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.51 ms /     7 runs   (    0.50 ms per token,  1993.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2814.20 ms /   530 tokens (    5.31 ms per token,   188.33 tokens per second)\n",
            "llama_print_timings:        eval time =     229.53 ms /     6 runs   (   38.25 ms per token,    26.14 tokens per second)\n",
            "llama_print_timings:       total time =    3192.56 ms /   536 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.82 ms /     7 runs   (    0.55 ms per token,  1831.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2378.14 ms /   453 tokens (    5.25 ms per token,   190.48 tokens per second)\n",
            "llama_print_timings:        eval time =     233.39 ms /     6 runs   (   38.90 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    2744.81 ms /   459 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.66 ms /     7 runs   (    0.67 ms per token,  1501.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4672.28 ms /   864 tokens (    5.41 ms per token,   184.92 tokens per second)\n",
            "llama_print_timings:        eval time =     277.67 ms /     7 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    5273.69 ms /   871 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.31 ms /     7 runs   (    0.47 ms per token,  2114.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4851.49 ms /   893 tokens (    5.43 ms per token,   184.07 tokens per second)\n",
            "llama_print_timings:        eval time =     241.05 ms /     6 runs   (   40.17 ms per token,    24.89 tokens per second)\n",
            "llama_print_timings:       total time =    5366.78 ms /   899 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.60 ms /     7 runs   (    0.51 ms per token,  1944.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2726.99 ms /   517 tokens (    5.27 ms per token,   189.59 tokens per second)\n",
            "llama_print_timings:        eval time =     234.12 ms /     6 runs   (   39.02 ms per token,    25.63 tokens per second)\n",
            "llama_print_timings:       total time =    3105.17 ms /   523 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.79 ms /     7 runs   (    0.54 ms per token,  1847.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1262.33 ms /   238 tokens (    5.30 ms per token,   188.54 tokens per second)\n",
            "llama_print_timings:        eval time =     231.90 ms /     6 runs   (   38.65 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    1573.76 ms /   244 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.86 ms /     6 runs   (    0.48 ms per token,  2100.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3950.65 ms /   731 tokens (    5.40 ms per token,   185.03 tokens per second)\n",
            "llama_print_timings:        eval time =     197.09 ms /     5 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    4457.16 ms /   736 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.92 ms /     6 runs   (    0.49 ms per token,  2056.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3484.32 ms /   653 tokens (    5.34 ms per token,   187.41 tokens per second)\n",
            "llama_print_timings:        eval time =     197.25 ms /     5 runs   (   39.45 ms per token,    25.35 tokens per second)\n",
            "llama_print_timings:       total time =    3862.83 ms /   658 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.63 ms /     7 runs   (    0.52 ms per token,  1929.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2156.70 ms /   413 tokens (    5.22 ms per token,   191.50 tokens per second)\n",
            "llama_print_timings:        eval time =     231.71 ms /     6 runs   (   38.62 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    2508.92 ms /   419 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1965.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1839.61 ms /   347 tokens (    5.30 ms per token,   188.63 tokens per second)\n",
            "llama_print_timings:        eval time =     233.65 ms /     6 runs   (   38.94 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =    2182.78 ms /   353 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.68 ms /     7 runs   (    0.67 ms per token,  1495.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2344.02 ms /   447 tokens (    5.24 ms per token,   190.70 tokens per second)\n",
            "llama_print_timings:        eval time =     233.18 ms /     6 runs   (   38.86 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    2777.25 ms /   453 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.80 ms /     7 runs   (    0.54 ms per token,  1840.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2300.62 ms /   436 tokens (    5.28 ms per token,   189.51 tokens per second)\n",
            "llama_print_timings:        eval time =     231.82 ms /     6 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2713.25 ms /   442 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.71 ms /     7 runs   (    0.53 ms per token,  1884.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5039.34 ms /   927 tokens (    5.44 ms per token,   183.95 tokens per second)\n",
            "llama_print_timings:        eval time =     240.88 ms /     6 runs   (   40.15 ms per token,    24.91 tokens per second)\n",
            "llama_print_timings:       total time =    5526.40 ms /   933 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.88 ms /     6 runs   (    0.65 ms per token,  1547.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5192.23 ms /   952 tokens (    5.45 ms per token,   183.35 tokens per second)\n",
            "llama_print_timings:        eval time =     201.56 ms /     5 runs   (   40.31 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =    5739.36 ms /   957 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1905.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2779.69 ms /   524 tokens (    5.30 ms per token,   188.51 tokens per second)\n",
            "llama_print_timings:        eval time =     231.77 ms /     6 runs   (   38.63 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    3191.55 ms /   530 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.95 ms /     7 runs   (    0.56 ms per token,  1771.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2294.12 ms /   440 tokens (    5.21 ms per token,   191.79 tokens per second)\n",
            "llama_print_timings:        eval time =     232.32 ms /     6 runs   (   38.72 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    2661.89 ms /   446 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.57 ms /     7 runs   (    0.51 ms per token,  1959.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2644.08 ms /   504 tokens (    5.25 ms per token,   190.61 tokens per second)\n",
            "llama_print_timings:        eval time =     270.19 ms /     7 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    3061.35 ms /   511 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.77 ms /     7 runs   (    0.68 ms per token,  1467.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1943.65 ms /   375 tokens (    5.18 ms per token,   192.94 tokens per second)\n",
            "llama_print_timings:        eval time =     231.59 ms /     6 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2311.42 ms /   381 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.60 ms /     7 runs   (    0.51 ms per token,  1945.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3826.70 ms /   711 tokens (    5.38 ms per token,   185.80 tokens per second)\n",
            "llama_print_timings:        eval time =     234.96 ms /     6 runs   (   39.16 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    4363.85 ms /   717 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.96 ms /     6 runs   (    0.49 ms per token,  2029.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4164.18 ms /   776 tokens (    5.37 ms per token,   186.35 tokens per second)\n",
            "llama_print_timings:        eval time =     197.70 ms /     5 runs   (   39.54 ms per token,    25.29 tokens per second)\n",
            "llama_print_timings:       total time =    4563.48 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.83 ms /     7 runs   (    0.55 ms per token,  1830.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1814.11 ms /   352 tokens (    5.15 ms per token,   194.03 tokens per second)\n",
            "llama_print_timings:        eval time =     267.53 ms /     7 runs   (   38.22 ms per token,    26.17 tokens per second)\n",
            "llama_print_timings:       total time =    2188.65 ms /   359 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.98 ms /     7 runs   (    0.71 ms per token,  1405.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1777.29 ms /   344 tokens (    5.17 ms per token,   193.55 tokens per second)\n",
            "llama_print_timings:        eval time =     229.24 ms /     6 runs   (   38.21 ms per token,    26.17 tokens per second)\n",
            "llama_print_timings:       total time =    2137.58 ms /   350 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.95 ms /     7 runs   (    0.56 ms per token,  1773.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2480.75 ms /   468 tokens (    5.30 ms per token,   188.65 tokens per second)\n",
            "llama_print_timings:        eval time =     231.21 ms /     6 runs   (   38.54 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    2944.32 ms /   474 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "for i in range(len(results)):\n",
        "  if (results['answer'][i] is not None) and (results['answer'][i]['score'] == 'yes'):\n",
        "    score += 1\n",
        "total_score_app2 = score/len(results)\n",
        "print(f'Total score for Approach # 2 is {total_score_app2}')"
      ],
      "metadata": {
        "id": "oAOJeBGCAKGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11905826-172f-4224-dc14-e68dc60090cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total score for Approach # 2 is 0.6615384615384615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app3 = pd.read_json('/content/drive/MyDrive/Thesis/retrievers/results_approach3.json')\n",
        "len(app3)"
      ],
      "metadata": {
        "id": "uCvh88ISAb79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef13c76-413b-4cba-9cba-1310473d80be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "for i in range(len(app3)):\n",
        "  question = app3['question'][i]\n",
        "  doc_txt = app3['context_ip'][i]['page_content']\n",
        "  answer = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
        "  results_list.append(pd.DataFrame({'question_name': question, 'answer': [answer]}))\n",
        "results = pd.concat(results_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "Isph2gguAgYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b4d3b7-6bde-4082-9b26-3aa1de16be7e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.24 ms /     7 runs   (    0.46 ms per token,  2161.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3589.35 ms /   659 tokens (    5.45 ms per token,   183.60 tokens per second)\n",
            "llama_print_timings:        eval time =     236.73 ms /     6 runs   (   39.45 ms per token,    25.35 tokens per second)\n",
            "llama_print_timings:       total time =    4092.24 ms /   665 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.91 ms /     7 runs   (    0.56 ms per token,  1792.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3336.90 ms /   613 tokens (    5.44 ms per token,   183.70 tokens per second)\n",
            "llama_print_timings:        eval time =     233.04 ms /     6 runs   (   38.84 ms per token,    25.75 tokens per second)\n",
            "llama_print_timings:       total time =    3803.56 ms /   619 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.60 ms /     6 runs   (    0.60 ms per token,  1665.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4211.46 ms /   776 tokens (    5.43 ms per token,   184.26 tokens per second)\n",
            "llama_print_timings:        eval time =     199.33 ms /     5 runs   (   39.87 ms per token,    25.08 tokens per second)\n",
            "llama_print_timings:       total time =    4633.02 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.35 ms /     7 runs   (    0.48 ms per token,  2087.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3583.84 ms /   663 tokens (    5.41 ms per token,   185.00 tokens per second)\n",
            "llama_print_timings:        eval time =     240.78 ms /     6 runs   (   40.13 ms per token,    24.92 tokens per second)\n",
            "llama_print_timings:       total time =    4114.08 ms /   669 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.69 ms /     7 runs   (    0.53 ms per token,  1899.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3066.80 ms /   572 tokens (    5.36 ms per token,   186.51 tokens per second)\n",
            "llama_print_timings:        eval time =     233.65 ms /     6 runs   (   38.94 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =    3465.11 ms /   578 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.45 ms /     7 runs   (    0.49 ms per token,  2030.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2754.96 ms /   516 tokens (    5.34 ms per token,   187.30 tokens per second)\n",
            "llama_print_timings:        eval time =     235.86 ms /     6 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    3149.77 ms /   522 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.19 ms /     7 runs   (    0.60 ms per token,  1669.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2350.89 ms /   445 tokens (    5.28 ms per token,   189.29 tokens per second)\n",
            "llama_print_timings:        eval time =     234.51 ms /     6 runs   (   39.09 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    2737.69 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.65 ms /     7 runs   (    0.66 ms per token,  1505.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1794.09 ms /   340 tokens (    5.28 ms per token,   189.51 tokens per second)\n",
            "llama_print_timings:        eval time =     229.73 ms /     6 runs   (   38.29 ms per token,    26.12 tokens per second)\n",
            "llama_print_timings:       total time =    2217.96 ms /   346 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.27 ms /     7 runs   (    0.47 ms per token,  2143.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3504.12 ms /   654 tokens (    5.36 ms per token,   186.64 tokens per second)\n",
            "llama_print_timings:        eval time =     235.94 ms /     6 runs   (   39.32 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    3948.43 ms /   660 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.74 ms /     7 runs   (    0.53 ms per token,  1871.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2783.05 ms /   523 tokens (    5.32 ms per token,   187.92 tokens per second)\n",
            "llama_print_timings:        eval time =     231.88 ms /     6 runs   (   38.65 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    3166.31 ms /   529 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.70 ms /     6 runs   (    0.62 ms per token,  1620.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4172.78 ms /   774 tokens (    5.39 ms per token,   185.49 tokens per second)\n",
            "llama_print_timings:        eval time =     197.98 ms /     5 runs   (   39.60 ms per token,    25.26 tokens per second)\n",
            "llama_print_timings:       total time =    4616.57 ms /   779 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.33 ms /     7 runs   (    0.48 ms per token,  2104.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2349.65 ms /   444 tokens (    5.29 ms per token,   188.96 tokens per second)\n",
            "llama_print_timings:        eval time =     230.33 ms /     6 runs   (   38.39 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    2785.30 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1906.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2070.45 ms /   398 tokens (    5.20 ms per token,   192.23 tokens per second)\n",
            "llama_print_timings:        eval time =     233.80 ms /     6 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    2422.57 ms /   404 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.58 ms /     7 runs   (    0.51 ms per token,  1954.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2200.12 ms /   422 tokens (    5.21 ms per token,   191.81 tokens per second)\n",
            "llama_print_timings:        eval time =     229.62 ms /     6 runs   (   38.27 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =    2555.31 ms /   428 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.57 ms /     7 runs   (    0.51 ms per token,  1960.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =     891.86 ms /   173 tokens (    5.16 ms per token,   193.98 tokens per second)\n",
            "llama_print_timings:        eval time =     228.21 ms /     6 runs   (   38.04 ms per token,    26.29 tokens per second)\n",
            "llama_print_timings:       total time =    1182.58 ms /   179 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       7.37 ms /     7 runs   (    1.05 ms per token,   950.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3473.62 ms /   651 tokens (    5.34 ms per token,   187.41 tokens per second)\n",
            "llama_print_timings:        eval time =     237.33 ms /     6 runs   (   39.55 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    3922.87 ms /   657 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.47 ms /     7 runs   (    0.50 ms per token,  2016.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2306.97 ms /   440 tokens (    5.24 ms per token,   190.73 tokens per second)\n",
            "llama_print_timings:        eval time =     269.97 ms /     7 runs   (   38.57 ms per token,    25.93 tokens per second)\n",
            "llama_print_timings:       total time =    2801.96 ms /   447 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.14 ms /     6 runs   (    0.52 ms per token,  1913.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3698.75 ms /   692 tokens (    5.35 ms per token,   187.09 tokens per second)\n",
            "llama_print_timings:        eval time =     197.22 ms /     5 runs   (   39.44 ms per token,    25.35 tokens per second)\n",
            "llama_print_timings:       total time =    4086.23 ms /   697 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.45 ms /     7 runs   (    0.49 ms per token,  2027.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2810.27 ms /   531 tokens (    5.29 ms per token,   188.95 tokens per second)\n",
            "llama_print_timings:        eval time =     232.51 ms /     6 runs   (   38.75 ms per token,    25.81 tokens per second)\n",
            "llama_print_timings:       total time =    3192.43 ms /   537 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.54 ms /     7 runs   (    0.65 ms per token,  1540.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3255.06 ms /   614 tokens (    5.30 ms per token,   188.63 tokens per second)\n",
            "llama_print_timings:        eval time =     233.29 ms /     6 runs   (   38.88 ms per token,    25.72 tokens per second)\n",
            "llama_print_timings:       total time =    3709.54 ms /   620 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.40 ms /     7 runs   (    0.49 ms per token,  2057.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4039.86 ms /   751 tokens (    5.38 ms per token,   185.90 tokens per second)\n",
            "llama_print_timings:        eval time =     234.46 ms /     6 runs   (   39.08 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    4559.78 ms /   757 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.17 ms /     6 runs   (    0.53 ms per token,  1894.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5922.16 ms /  1077 tokens (    5.50 ms per token,   181.86 tokens per second)\n",
            "llama_print_timings:        eval time =     199.38 ms /     5 runs   (   39.88 ms per token,    25.08 tokens per second)\n",
            "llama_print_timings:       total time =    6400.61 ms /  1082 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.72 ms /     7 runs   (    0.67 ms per token,  1481.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2084.16 ms /   397 tokens (    5.25 ms per token,   190.48 tokens per second)\n",
            "llama_print_timings:        eval time =     232.55 ms /     6 runs   (   38.76 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =    2495.02 ms /   403 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.75 ms /     7 runs   (    0.54 ms per token,  1864.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2056.60 ms /   390 tokens (    5.27 ms per token,   189.63 tokens per second)\n",
            "llama_print_timings:        eval time =     230.28 ms /     6 runs   (   38.38 ms per token,    26.06 tokens per second)\n",
            "llama_print_timings:       total time =    2468.39 ms /   396 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.39 ms /     7 runs   (    0.63 ms per token,  1595.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2330.22 ms /   444 tokens (    5.25 ms per token,   190.54 tokens per second)\n",
            "llama_print_timings:        eval time =     231.32 ms /     6 runs   (   38.55 ms per token,    25.94 tokens per second)\n",
            "llama_print_timings:       total time =    2698.60 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.90 ms /     7 runs   (    0.56 ms per token,  1794.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1896.25 ms /   365 tokens (    5.20 ms per token,   192.48 tokens per second)\n",
            "llama_print_timings:        eval time =     224.42 ms /     6 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
            "llama_print_timings:       total time =    2234.68 ms /   371 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.75 ms /     7 runs   (    0.54 ms per token,  1867.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2510.86 ms /   479 tokens (    5.24 ms per token,   190.77 tokens per second)\n",
            "llama_print_timings:        eval time =     234.17 ms /     6 runs   (   39.03 ms per token,    25.62 tokens per second)\n",
            "llama_print_timings:       total time =    2887.86 ms /   485 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.84 ms /     7 runs   (    0.69 ms per token,  1445.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1489.86 ms /   275 tokens (    5.42 ms per token,   184.58 tokens per second)\n",
            "llama_print_timings:        eval time =     233.13 ms /     6 runs   (   38.85 ms per token,    25.74 tokens per second)\n",
            "llama_print_timings:       total time =    1829.66 ms /   281 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.83 ms /     7 runs   (    0.55 ms per token,  1825.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2526.95 ms /   479 tokens (    5.28 ms per token,   189.56 tokens per second)\n",
            "llama_print_timings:        eval time =     234.51 ms /     6 runs   (   39.09 ms per token,    25.58 tokens per second)\n",
            "llama_print_timings:       total time =    3003.34 ms /   485 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.77 ms /     7 runs   (    0.54 ms per token,  1855.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2203.33 ms /   424 tokens (    5.20 ms per token,   192.44 tokens per second)\n",
            "llama_print_timings:        eval time =     271.51 ms /     7 runs   (   38.79 ms per token,    25.78 tokens per second)\n",
            "llama_print_timings:       total time =    2602.17 ms /   431 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.62 ms /     7 runs   (    0.52 ms per token,  1931.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2864.86 ms /   544 tokens (    5.27 ms per token,   189.89 tokens per second)\n",
            "llama_print_timings:        eval time =     270.23 ms /     7 runs   (   38.60 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    3287.78 ms /   551 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.21 ms /     7 runs   (    0.46 ms per token,  2182.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2773.48 ms /   525 tokens (    5.28 ms per token,   189.29 tokens per second)\n",
            "llama_print_timings:        eval time =     231.40 ms /     6 runs   (   38.57 ms per token,    25.93 tokens per second)\n",
            "llama_print_timings:       total time =    3156.22 ms /   531 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       6.28 ms /     7 runs   (    0.90 ms per token,  1114.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1464.48 ms /   269 tokens (    5.44 ms per token,   183.68 tokens per second)\n",
            "llama_print_timings:        eval time =     233.81 ms /     6 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    1837.66 ms /   275 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.39 ms /     7 runs   (    0.63 ms per token,  1593.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2132.61 ms /   407 tokens (    5.24 ms per token,   190.85 tokens per second)\n",
            "llama_print_timings:        eval time =     230.91 ms /     6 runs   (   38.48 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2571.67 ms /   413 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.65 ms /     7 runs   (    0.52 ms per token,  1918.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2078.95 ms /   399 tokens (    5.21 ms per token,   191.92 tokens per second)\n",
            "llama_print_timings:        eval time =     233.46 ms /     6 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    2431.29 ms /   405 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1937.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     479.76 ms /    93 tokens (    5.16 ms per token,   193.85 tokens per second)\n",
            "llama_print_timings:        eval time =     224.77 ms /     6 runs   (   37.46 ms per token,    26.69 tokens per second)\n",
            "llama_print_timings:       total time =     748.50 ms /    99 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.42 ms /     7 runs   (    0.63 ms per token,  1584.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2649.28 ms /   503 tokens (    5.27 ms per token,   189.86 tokens per second)\n",
            "llama_print_timings:        eval time =     234.36 ms /     6 runs   (   39.06 ms per token,    25.60 tokens per second)\n",
            "llama_print_timings:       total time =    3076.78 ms /   509 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.64 ms /     7 runs   (    0.52 ms per token,  1923.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2223.17 ms /   420 tokens (    5.29 ms per token,   188.92 tokens per second)\n",
            "llama_print_timings:        eval time =     233.17 ms /     6 runs   (   38.86 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    2670.43 ms /   426 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.51 ms /     7 runs   (    0.64 ms per token,  1551.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2439.55 ms /   462 tokens (    5.28 ms per token,   189.38 tokens per second)\n",
            "llama_print_timings:        eval time =     233.95 ms /     6 runs   (   38.99 ms per token,    25.65 tokens per second)\n",
            "llama_print_timings:       total time =    2898.60 ms /   468 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.06 ms /     6 runs   (    0.51 ms per token,  1958.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3142.54 ms /   588 tokens (    5.34 ms per token,   187.11 tokens per second)\n",
            "llama_print_timings:        eval time =     194.36 ms /     5 runs   (   38.87 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    3536.62 ms /   593 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.46 ms /     7 runs   (    0.49 ms per token,  2025.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2737.10 ms /   515 tokens (    5.31 ms per token,   188.16 tokens per second)\n",
            "llama_print_timings:        eval time =     231.87 ms /     6 runs   (   38.65 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    3120.43 ms /   521 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.26 ms /     7 runs   (    0.47 ms per token,  2146.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3402.26 ms /   640 tokens (    5.32 ms per token,   188.11 tokens per second)\n",
            "llama_print_timings:        eval time =     277.66 ms /     7 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    3856.88 ms /   647 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.25 ms /     6 runs   (    0.54 ms per token,  1844.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3109.12 ms /   579 tokens (    5.37 ms per token,   186.23 tokens per second)\n",
            "llama_print_timings:        eval time =     194.78 ms /     5 runs   (   38.96 ms per token,    25.67 tokens per second)\n",
            "llama_print_timings:       total time =    3579.74 ms /   584 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.35 ms /     7 runs   (    0.48 ms per token,  2088.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2692.60 ms /   507 tokens (    5.31 ms per token,   188.29 tokens per second)\n",
            "llama_print_timings:        eval time =     231.75 ms /     6 runs   (   38.62 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    3079.66 ms /   513 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.52 ms /     7 runs   (    0.50 ms per token,  1989.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3308.75 ms /   621 tokens (    5.33 ms per token,   187.68 tokens per second)\n",
            "llama_print_timings:        eval time =     233.83 ms /     6 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    3714.89 ms /   627 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.84 ms /     7 runs   (    0.69 ms per token,  1447.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3994.24 ms /   741 tokens (    5.39 ms per token,   185.52 tokens per second)\n",
            "llama_print_timings:        eval time =     238.81 ms /     6 runs   (   39.80 ms per token,    25.12 tokens per second)\n",
            "llama_print_timings:       total time =    4491.66 ms /   747 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.65 ms /     7 runs   (    0.52 ms per token,  1916.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3327.88 ms /   621 tokens (    5.36 ms per token,   186.61 tokens per second)\n",
            "llama_print_timings:        eval time =     232.82 ms /     6 runs   (   38.80 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    3825.79 ms /   627 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.12 ms /     6 runs   (    0.52 ms per token,  1923.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1648.08 ms /   319 tokens (    5.17 ms per token,   193.56 tokens per second)\n",
            "llama_print_timings:        eval time =     192.63 ms /     5 runs   (   38.53 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    1935.68 ms /   324 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.07 ms /     6 runs   (    0.51 ms per token,  1954.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     227.38 ms /     6 runs   (   37.90 ms per token,    26.39 tokens per second)\n",
            "llama_print_timings:       total time =     246.01 ms /     6 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.57 ms /     7 runs   (    0.51 ms per token,  1961.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2511.02 ms /   478 tokens (    5.25 ms per token,   190.36 tokens per second)\n",
            "llama_print_timings:        eval time =     230.73 ms /     6 runs   (   38.45 ms per token,    26.00 tokens per second)\n",
            "llama_print_timings:       total time =    2878.56 ms /   484 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.70 ms /     7 runs   (    0.53 ms per token,  1893.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2158.91 ms /   412 tokens (    5.24 ms per token,   190.84 tokens per second)\n",
            "llama_print_timings:        eval time =     233.14 ms /     6 runs   (   38.86 ms per token,    25.74 tokens per second)\n",
            "llama_print_timings:       total time =    2517.51 ms /   418 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.73 ms /     7 runs   (    0.68 ms per token,  1479.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2256.89 ms /   432 tokens (    5.22 ms per token,   191.41 tokens per second)\n",
            "llama_print_timings:        eval time =     233.38 ms /     6 runs   (   38.90 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    2685.39 ms /   438 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.73 ms /     7 runs   (    0.53 ms per token,  1879.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2344.89 ms /   443 tokens (    5.29 ms per token,   188.92 tokens per second)\n",
            "llama_print_timings:        eval time =     229.97 ms /     6 runs   (   38.33 ms per token,    26.09 tokens per second)\n",
            "llama_print_timings:       total time =    2763.07 ms /   449 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.33 ms /     7 runs   (    0.48 ms per token,  2100.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2815.83 ms /   536 tokens (    5.25 ms per token,   190.35 tokens per second)\n",
            "llama_print_timings:        eval time =     269.04 ms /     7 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    3237.38 ms /   543 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.04 ms /     6 runs   (    0.51 ms per token,  1970.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4798.25 ms /   888 tokens (    5.40 ms per token,   185.07 tokens per second)\n",
            "llama_print_timings:        eval time =     198.65 ms /     5 runs   (   39.73 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =    5236.32 ms /   893 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.46 ms /     7 runs   (    0.64 ms per token,  1567.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2878.97 ms /   544 tokens (    5.29 ms per token,   188.96 tokens per second)\n",
            "llama_print_timings:        eval time =     271.88 ms /     7 runs   (   38.84 ms per token,    25.75 tokens per second)\n",
            "llama_print_timings:       total time =    3429.15 ms /   551 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.49 ms /     7 runs   (    0.50 ms per token,  2005.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2553.64 ms /   488 tokens (    5.23 ms per token,   191.10 tokens per second)\n",
            "llama_print_timings:        eval time =     233.99 ms /     6 runs   (   39.00 ms per token,    25.64 tokens per second)\n",
            "llama_print_timings:       total time =    2935.61 ms /   494 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.62 ms /     7 runs   (    0.52 ms per token,  1931.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2203.20 ms /   423 tokens (    5.21 ms per token,   191.99 tokens per second)\n",
            "llama_print_timings:        eval time =     233.45 ms /     6 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    2563.66 ms /   429 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.63 ms /     7 runs   (    0.52 ms per token,  1926.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2203.13 ms /   424 tokens (    5.20 ms per token,   192.45 tokens per second)\n",
            "llama_print_timings:        eval time =     232.02 ms /     6 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    2557.20 ms /   430 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.50 ms /     7 runs   (    0.64 ms per token,  1554.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2203.82 ms /   421 tokens (    5.23 ms per token,   191.03 tokens per second)\n",
            "llama_print_timings:        eval time =     232.32 ms /     6 runs   (   38.72 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    2590.11 ms /   427 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.49 ms /     7 runs   (    0.64 ms per token,  1559.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2202.76 ms /   414 tokens (    5.32 ms per token,   187.95 tokens per second)\n",
            "llama_print_timings:        eval time =     232.18 ms /     6 runs   (   38.70 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    2661.77 ms /   420 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1905.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2292.11 ms /   439 tokens (    5.22 ms per token,   191.53 tokens per second)\n",
            "llama_print_timings:        eval time =     229.41 ms /     6 runs   (   38.24 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    2649.25 ms /   445 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.74 ms /     7 runs   (    0.53 ms per token,  1872.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2167.05 ms /   414 tokens (    5.23 ms per token,   191.04 tokens per second)\n",
            "llama_print_timings:        eval time =     232.40 ms /     6 runs   (   38.73 ms per token,    25.82 tokens per second)\n",
            "llama_print_timings:       total time =    2521.52 ms /   420 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.86 ms /     7 runs   (    0.55 ms per token,  1813.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2418.26 ms /   458 tokens (    5.28 ms per token,   189.39 tokens per second)\n",
            "llama_print_timings:        eval time =     232.32 ms /     6 runs   (   38.72 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    2787.77 ms /   464 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "for i in range(len(results)):\n",
        "  if results['answer'][i]['score'] == 'yes':\n",
        "    score += 1\n",
        "total_score_app3 = score/len(results)\n",
        "print(f'Total score for Approach # 3 is {total_score_app3}')"
      ],
      "metadata": {
        "id": "Eiwa7yvaAjDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f54f081-e057-41cf-c1e8-0ce30659d645"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total score for Approach # 3 is 0.703125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app4 = pd.read_json('/content/drive/MyDrive/Thesis/retrievers/results_approach4.json')\n",
        "len(app4)"
      ],
      "metadata": {
        "id": "n2WRqMQSAnF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25171a2f-537e-42d8-fbcf-d7b0d827b6e2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "for i in range(len(app4)):\n",
        "  question = app4['question_name'][i]\n",
        "  doc_txt = app4['context_ip_name'][i]['page_content']\n",
        "  try:\n",
        "    answer = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
        "    results_list.append(pd.DataFrame({'question_name': question, 'answer': [answer]}))\n",
        "  except JSONDecodeError as e:\n",
        "    print(f\"JSONDecodeError occurred for question: {question}. Skipping...\")\n",
        "    continue\n",
        "  except OutputParserException as e:\n",
        "    print(f\"OutputParserException occurred for question: {question}. Skipping...\")\n",
        "    continue\n",
        "  results_list.append(pd.DataFrame({'question_name': question, 'answer': [answer]}))\n",
        "results = pd.concat(results_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "C8ymFOhuAnF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47caf333-79ed-4ca5-a4a2-d2743c12a270"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.91 ms /     6 runs   (    0.48 ms per token,  2065.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6022.87 ms /  1083 tokens (    5.56 ms per token,   179.81 tokens per second)\n",
            "llama_print_timings:        eval time =     201.22 ms /     5 runs   (   40.24 ms per token,    24.85 tokens per second)\n",
            "llama_print_timings:       total time =    6520.75 ms /  1088 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.94 ms /     7 runs   (    0.56 ms per token,  1778.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5078.64 ms /   926 tokens (    5.48 ms per token,   182.33 tokens per second)\n",
            "llama_print_timings:        eval time =     242.84 ms /     6 runs   (   40.47 ms per token,    24.71 tokens per second)\n",
            "llama_print_timings:       total time =    5691.06 ms /   932 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.45 ms /     7 runs   (    0.49 ms per token,  2030.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4435.16 ms /   814 tokens (    5.45 ms per token,   183.53 tokens per second)\n",
            "llama_print_timings:        eval time =     237.90 ms /     6 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    4913.62 ms /   820 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.46 ms /     7 runs   (    0.64 ms per token,  1570.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5146.13 ms /   930 tokens (    5.53 ms per token,   180.72 tokens per second)\n",
            "llama_print_timings:        eval time =     245.21 ms /     6 runs   (   40.87 ms per token,    24.47 tokens per second)\n",
            "llama_print_timings:       total time =    5676.51 ms /   936 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.11 ms /     7 runs   (    0.73 ms per token,  1370.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2819.39 ms /   520 tokens (    5.42 ms per token,   184.44 tokens per second)\n",
            "llama_print_timings:        eval time =     235.66 ms /     6 runs   (   39.28 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =    3351.04 ms /   526 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.65 ms /     7 runs   (    0.52 ms per token,  1919.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2804.71 ms /   516 tokens (    5.44 ms per token,   183.98 tokens per second)\n",
            "llama_print_timings:        eval time =     233.59 ms /     6 runs   (   38.93 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    3250.14 ms /   522 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.57 ms /     7 runs   (    0.51 ms per token,  1959.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5207.01 ms /   923 tokens (    5.64 ms per token,   177.26 tokens per second)\n",
            "llama_print_timings:        eval time =     244.30 ms /     6 runs   (   40.72 ms per token,    24.56 tokens per second)\n",
            "llama_print_timings:       total time =    5915.42 ms /   929 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.43 ms /     7 runs   (    0.49 ms per token,  2042.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4391.04 ms /   807 tokens (    5.44 ms per token,   183.78 tokens per second)\n",
            "llama_print_timings:        eval time =     237.44 ms /     6 runs   (   39.57 ms per token,    25.27 tokens per second)\n",
            "llama_print_timings:       total time =    4964.68 ms /   813 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      57.77 ms /    96 runs   (    0.60 ms per token,  1661.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5955.37 ms /  1078 tokens (    5.52 ms per token,   181.01 tokens per second)\n",
            "llama_print_timings:        eval time =    3850.20 ms /    95 runs   (   40.53 ms per token,    24.67 tokens per second)\n",
            "llama_print_timings:       total time =   10475.82 ms /  1173 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1941.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4991.74 ms /   920 tokens (    5.43 ms per token,   184.30 tokens per second)\n",
            "llama_print_timings:        eval time =     280.59 ms /     7 runs   (   40.08 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    5565.73 ms /   927 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.68 ms /     6 runs   (    0.61 ms per token,  1629.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5973.12 ms /  1087 tokens (    5.50 ms per token,   181.98 tokens per second)\n",
            "llama_print_timings:        eval time =     201.20 ms /     5 runs   (   40.24 ms per token,    24.85 tokens per second)\n",
            "llama_print_timings:       total time =    6489.29 ms /  1092 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.60 ms /     7 runs   (    0.51 ms per token,  1945.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4710.06 ms /   870 tokens (    5.41 ms per token,   184.71 tokens per second)\n",
            "llama_print_timings:        eval time =     234.88 ms /     6 runs   (   39.15 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    5267.72 ms /   876 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.98 ms /     6 runs   (    0.50 ms per token,  2010.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4060.94 ms /   759 tokens (    5.35 ms per token,   186.90 tokens per second)\n",
            "llama_print_timings:        eval time =     196.43 ms /     5 runs   (   39.29 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =    4462.18 ms /   764 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.96 ms /     9 runs   (    0.66 ms per token,  1509.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3977.14 ms /   743 tokens (    5.35 ms per token,   186.82 tokens per second)\n",
            "llama_print_timings:        eval time =     316.11 ms /     8 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    4571.38 ms /   751 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.53 ms /     7 runs   (    0.50 ms per token,  1984.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5318.58 ms /   976 tokens (    5.45 ms per token,   183.51 tokens per second)\n",
            "llama_print_timings:        eval time =     239.99 ms /     6 runs   (   40.00 ms per token,    25.00 tokens per second)\n",
            "llama_print_timings:       total time =    5871.36 ms /   982 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.99 ms /     7 runs   (    0.57 ms per token,  1753.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5211.19 ms /   958 tokens (    5.44 ms per token,   183.84 tokens per second)\n",
            "llama_print_timings:        eval time =     243.42 ms /     6 runs   (   40.57 ms per token,    24.65 tokens per second)\n",
            "llama_print_timings:       total time =    5717.66 ms /   964 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      61.62 ms /   101 runs   (    0.61 ms per token,  1639.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1323.00 ms /   254 tokens (    5.21 ms per token,   191.99 tokens per second)\n",
            "llama_print_timings:        eval time =    3782.47 ms /   100 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
            "llama_print_timings:       total time =    5609.28 ms /   354 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.93 ms /     6 runs   (    0.49 ms per token,  2045.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5359.72 ms /   978 tokens (    5.48 ms per token,   182.47 tokens per second)\n",
            "llama_print_timings:        eval time =     199.72 ms /     5 runs   (   39.94 ms per token,    25.03 tokens per second)\n",
            "llama_print_timings:       total time =    5819.19 ms /   983 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.14 ms /     6 runs   (    0.52 ms per token,  1912.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6010.23 ms /  1088 tokens (    5.52 ms per token,   181.02 tokens per second)\n",
            "llama_print_timings:        eval time =     241.68 ms /     6 runs   (   40.28 ms per token,    24.83 tokens per second)\n",
            "llama_print_timings:       total time =    6654.42 ms /  1094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.77 ms /     9 runs   (    0.53 ms per token,  1887.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4030.65 ms /   747 tokens (    5.40 ms per token,   185.33 tokens per second)\n",
            "llama_print_timings:        eval time =     314.79 ms /     8 runs   (   39.35 ms per token,    25.41 tokens per second)\n",
            "llama_print_timings:       total time =    4554.89 ms /   755 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.62 ms /     7 runs   (    0.52 ms per token,  1931.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2512.94 ms /   478 tokens (    5.26 ms per token,   190.22 tokens per second)\n",
            "llama_print_timings:        eval time =     233.49 ms /     6 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    2884.30 ms /   484 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      57.51 ms /   107 runs   (    0.54 ms per token,  1860.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4273.35 ms /   768 tokens (    5.56 ms per token,   179.72 tokens per second)\n",
            "llama_print_timings:        eval time =    4192.60 ms /   106 runs   (   39.55 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    9265.70 ms /   874 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =     144.06 ms /   256 runs   (    0.56 ms per token,  1777.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2907.08 ms /   546 tokens (    5.32 ms per token,   187.82 tokens per second)\n",
            "llama_print_timings:        eval time =   10016.32 ms /   255 runs   (   39.28 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =   14031.25 ms /   801 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      45.61 ms /    79 runs   (    0.58 ms per token,  1731.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6007.15 ms /  1086 tokens (    5.53 ms per token,   180.78 tokens per second)\n",
            "llama_print_timings:        eval time =    3158.64 ms /    78 runs   (   40.50 ms per token,    24.69 tokens per second)\n",
            "llama_print_timings:       total time =    9805.97 ms /  1164 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.79 ms /     7 runs   (    0.54 ms per token,  1849.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2774.48 ms /   525 tokens (    5.28 ms per token,   189.22 tokens per second)\n",
            "llama_print_timings:        eval time =     231.22 ms /     6 runs   (   38.54 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    3156.52 ms /   531 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.63 ms /     7 runs   (    0.52 ms per token,  1929.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2687.33 ms /   507 tokens (    5.30 ms per token,   188.66 tokens per second)\n",
            "llama_print_timings:        eval time =     235.53 ms /     6 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    3068.31 ms /   513 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.63 ms /     7 runs   (    0.66 ms per token,  1512.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2563.62 ms /   487 tokens (    5.26 ms per token,   189.97 tokens per second)\n",
            "llama_print_timings:        eval time =     234.65 ms /     6 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    2997.23 ms /   493 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.42 ms /     7 runs   (    0.63 ms per token,  1583.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1547.43 ms /   283 tokens (    5.47 ms per token,   182.88 tokens per second)\n",
            "llama_print_timings:        eval time =     233.68 ms /     6 runs   (   38.95 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =    1949.65 ms /   289 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.88 ms /     6 runs   (    0.48 ms per token,  2081.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3803.39 ms /   710 tokens (    5.36 ms per token,   186.68 tokens per second)\n",
            "llama_print_timings:        eval time =     196.08 ms /     5 runs   (   39.22 ms per token,    25.50 tokens per second)\n",
            "llama_print_timings:       total time =    4187.96 ms /   715 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.03 ms /     6 runs   (    0.51 ms per token,  1979.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3984.81 ms /   740 tokens (    5.38 ms per token,   185.71 tokens per second)\n",
            "llama_print_timings:        eval time =     197.99 ms /     5 runs   (   39.60 ms per token,    25.25 tokens per second)\n",
            "llama_print_timings:       total time =    4390.87 ms /   745 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.66 ms /     7 runs   (    0.67 ms per token,  1501.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2255.69 ms /   429 tokens (    5.26 ms per token,   190.19 tokens per second)\n",
            "llama_print_timings:        eval time =     232.24 ms /     6 runs   (   38.71 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    2671.35 ms /   435 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1908.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2921.54 ms /   549 tokens (    5.32 ms per token,   187.91 tokens per second)\n",
            "llama_print_timings:        eval time =     233.42 ms /     6 runs   (   38.90 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    3381.45 ms /   555 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.71 ms /     7 runs   (    0.53 ms per token,  1884.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2381.16 ms /   456 tokens (    5.22 ms per token,   191.50 tokens per second)\n",
            "llama_print_timings:        eval time =     230.54 ms /     6 runs   (   38.42 ms per token,    26.03 tokens per second)\n",
            "llama_print_timings:       total time =    2744.21 ms /   462 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.53 ms /     7 runs   (    0.50 ms per token,  1985.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2069.72 ms /   394 tokens (    5.25 ms per token,   190.36 tokens per second)\n",
            "llama_print_timings:        eval time =     233.76 ms /     6 runs   (   38.96 ms per token,    25.67 tokens per second)\n",
            "llama_print_timings:       total time =    2421.29 ms /   400 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.93 ms /     7 runs   (    0.56 ms per token,  1780.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3036.47 ms /   573 tokens (    5.30 ms per token,   188.71 tokens per second)\n",
            "llama_print_timings:        eval time =     231.85 ms /     6 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    3439.16 ms /   579 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.54 ms /     7 runs   (    0.65 ms per token,  1542.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     272.00 ms /     7 runs   (   38.86 ms per token,    25.74 tokens per second)\n",
            "llama_print_timings:       total time =     308.75 ms /     7 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.40 ms /     7 runs   (    0.77 ms per token,  1295.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1869.79 ms /   355 tokens (    5.27 ms per token,   189.86 tokens per second)\n",
            "llama_print_timings:        eval time =     228.78 ms /     6 runs   (   38.13 ms per token,    26.23 tokens per second)\n",
            "llama_print_timings:       total time =    2294.40 ms /   361 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.66 ms /     7 runs   (    0.52 ms per token,  1914.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2033.33 ms /   392 tokens (    5.19 ms per token,   192.79 tokens per second)\n",
            "llama_print_timings:        eval time =     231.08 ms /     6 runs   (   38.51 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    2406.01 ms /   398 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.70 ms /     6 runs   (    0.45 ms per token,  2223.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5886.38 ms /  1070 tokens (    5.50 ms per token,   181.78 tokens per second)\n",
            "llama_print_timings:        eval time =     200.79 ms /     5 runs   (   40.16 ms per token,    24.90 tokens per second)\n",
            "llama_print_timings:       total time =    6370.11 ms /  1075 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.40 ms /     7 runs   (    0.63 ms per token,  1591.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2468.72 ms /   466 tokens (    5.30 ms per token,   188.76 tokens per second)\n",
            "llama_print_timings:        eval time =     233.52 ms /     6 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    2883.86 ms /   472 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       6.87 ms /    13 runs   (    0.53 ms per token,  1891.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3581.45 ms /   666 tokens (    5.38 ms per token,   185.96 tokens per second)\n",
            "llama_print_timings:        eval time =     472.85 ms /    12 runs   (   39.40 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    4343.45 ms /   678 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OutputParserException occurred for question: Are there any tax-related risks or benefits for the COCA COLA CO mentioned?. Skipping...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.36 ms /     6 runs   (    0.56 ms per token,  1785.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3341.14 ms /   629 tokens (    5.31 ms per token,   188.26 tokens per second)\n",
            "llama_print_timings:        eval time =     192.39 ms /     5 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    3708.22 ms /   634 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.46 ms /     9 runs   (    0.61 ms per token,  1649.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3936.57 ms /   735 tokens (    5.36 ms per token,   186.71 tokens per second)\n",
            "llama_print_timings:        eval time =     317.11 ms /     8 runs   (   39.64 ms per token,    25.23 tokens per second)\n",
            "llama_print_timings:       total time =    4471.88 ms /   743 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1969.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2831.24 ms /   530 tokens (    5.34 ms per token,   187.20 tokens per second)\n",
            "llama_print_timings:        eval time =     229.87 ms /     6 runs   (   38.31 ms per token,    26.10 tokens per second)\n",
            "llama_print_timings:       total time =    3313.67 ms /   536 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.71 ms /     7 runs   (    0.53 ms per token,  1888.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2376.54 ms /   453 tokens (    5.25 ms per token,   190.61 tokens per second)\n",
            "llama_print_timings:        eval time =     233.18 ms /     6 runs   (   38.86 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    2741.61 ms /   459 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.66 ms /     7 runs   (    0.52 ms per token,  1914.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4669.40 ms /   864 tokens (    5.40 ms per token,   185.03 tokens per second)\n",
            "llama_print_timings:        eval time =     276.73 ms /     7 runs   (   39.53 ms per token,    25.30 tokens per second)\n",
            "llama_print_timings:       total time =    5183.70 ms /   871 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.08 ms /     7 runs   (    0.58 ms per token,  1717.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4891.47 ms /   893 tokens (    5.48 ms per token,   182.56 tokens per second)\n",
            "llama_print_timings:        eval time =     243.08 ms /     6 runs   (   40.51 ms per token,    24.68 tokens per second)\n",
            "llama_print_timings:       total time =    5593.02 ms /   899 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1966.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2744.05 ms /   517 tokens (    5.31 ms per token,   188.41 tokens per second)\n",
            "llama_print_timings:        eval time =     231.78 ms /     6 runs   (   38.63 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    3165.79 ms /   523 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.89 ms /     7 runs   (    0.56 ms per token,  1800.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1266.34 ms /   238 tokens (    5.32 ms per token,   187.94 tokens per second)\n",
            "llama_print_timings:        eval time =     231.50 ms /     6 runs   (   38.58 ms per token,    25.92 tokens per second)\n",
            "llama_print_timings:       total time =    1582.25 ms /   244 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.13 ms /     6 runs   (    0.52 ms per token,  1919.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3941.10 ms /   731 tokens (    5.39 ms per token,   185.48 tokens per second)\n",
            "llama_print_timings:        eval time =     197.93 ms /     5 runs   (   39.59 ms per token,    25.26 tokens per second)\n",
            "llama_print_timings:       total time =    4338.96 ms /   736 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.70 ms /     6 runs   (    0.62 ms per token,  1623.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3497.00 ms /   653 tokens (    5.36 ms per token,   186.73 tokens per second)\n",
            "llama_print_timings:        eval time =     198.15 ms /     5 runs   (   39.63 ms per token,    25.23 tokens per second)\n",
            "llama_print_timings:       total time =    3945.75 ms /   658 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1940.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2171.26 ms /   413 tokens (    5.26 ms per token,   190.21 tokens per second)\n",
            "llama_print_timings:        eval time =     230.67 ms /     6 runs   (   38.44 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2579.10 ms /   419 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.62 ms /     7 runs   (    0.52 ms per token,  1935.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1840.82 ms /   347 tokens (    5.30 ms per token,   188.50 tokens per second)\n",
            "llama_print_timings:        eval time =     234.04 ms /     6 runs   (   39.01 ms per token,    25.64 tokens per second)\n",
            "llama_print_timings:       total time =    2183.93 ms /   353 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.71 ms /     7 runs   (    0.53 ms per token,  1887.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2338.79 ms /   447 tokens (    5.23 ms per token,   191.12 tokens per second)\n",
            "llama_print_timings:        eval time =     230.12 ms /     6 runs   (   38.35 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    2703.90 ms /   453 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.63 ms /     7 runs   (    0.52 ms per token,  1926.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2292.31 ms /   436 tokens (    5.26 ms per token,   190.20 tokens per second)\n",
            "llama_print_timings:        eval time =     229.54 ms /     6 runs   (   38.26 ms per token,    26.14 tokens per second)\n",
            "llama_print_timings:       total time =    2652.19 ms /   442 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.65 ms /     7 runs   (    0.52 ms per token,  1919.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5060.33 ms /   927 tokens (    5.46 ms per token,   183.19 tokens per second)\n",
            "llama_print_timings:        eval time =     241.52 ms /     6 runs   (   40.25 ms per token,    24.84 tokens per second)\n",
            "llama_print_timings:       total time =    5663.11 ms /   933 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.09 ms /     6 runs   (    0.52 ms per token,  1941.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5186.28 ms /   952 tokens (    5.45 ms per token,   183.56 tokens per second)\n",
            "llama_print_timings:        eval time =     202.12 ms /     5 runs   (   40.42 ms per token,    24.74 tokens per second)\n",
            "llama_print_timings:       total time =    5642.50 ms /   957 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.00 ms /     7 runs   (    0.57 ms per token,  1750.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2771.94 ms /   524 tokens (    5.29 ms per token,   189.04 tokens per second)\n",
            "llama_print_timings:        eval time =     233.96 ms /     6 runs   (   38.99 ms per token,    25.65 tokens per second)\n",
            "llama_print_timings:       total time =    3155.67 ms /   530 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.68 ms /     7 runs   (    0.67 ms per token,  1496.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2307.57 ms /   440 tokens (    5.24 ms per token,   190.68 tokens per second)\n",
            "llama_print_timings:        eval time =     233.95 ms /     6 runs   (   38.99 ms per token,    25.65 tokens per second)\n",
            "llama_print_timings:       total time =    2783.13 ms /   446 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.54 ms /     7 runs   (    0.51 ms per token,  1975.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2649.94 ms /   504 tokens (    5.26 ms per token,   190.19 tokens per second)\n",
            "llama_print_timings:        eval time =     273.73 ms /     7 runs   (   39.10 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    3086.82 ms /   511 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1906.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1941.29 ms /   375 tokens (    5.18 ms per token,   193.17 tokens per second)\n",
            "llama_print_timings:        eval time =     231.72 ms /     6 runs   (   38.62 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    2285.52 ms /   381 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.60 ms /     7 runs   (    0.51 ms per token,  1943.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3805.61 ms /   711 tokens (    5.35 ms per token,   186.83 tokens per second)\n",
            "llama_print_timings:        eval time =     237.65 ms /     6 runs   (   39.61 ms per token,    25.25 tokens per second)\n",
            "llama_print_timings:       total time =    4234.27 ms /   717 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.96 ms /     6 runs   (    0.49 ms per token,  2024.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4186.97 ms /   776 tokens (    5.40 ms per token,   185.34 tokens per second)\n",
            "llama_print_timings:        eval time =     196.16 ms /     5 runs   (   39.23 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    4706.62 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.68 ms /     7 runs   (    0.53 ms per token,  1904.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1816.45 ms /   352 tokens (    5.16 ms per token,   193.78 tokens per second)\n",
            "llama_print_timings:        eval time =     268.54 ms /     7 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    2191.50 ms /   359 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.66 ms /     7 runs   (    0.52 ms per token,  1911.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1776.46 ms /   344 tokens (    5.16 ms per token,   193.64 tokens per second)\n",
            "llama_print_timings:        eval time =     227.58 ms /     6 runs   (   37.93 ms per token,    26.36 tokens per second)\n",
            "llama_print_timings:       total time =    2109.05 ms /   350 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.64 ms /     7 runs   (    0.52 ms per token,  1922.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2468.08 ms /   468 tokens (    5.27 ms per token,   189.62 tokens per second)\n",
            "llama_print_timings:        eval time =     231.67 ms /     6 runs   (   38.61 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    2838.61 ms /   474 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "for i in range(len(results)):\n",
        "  if (results['answer'][i] is not None) and (results['answer'][i]['score'] == 'yes'):\n",
        "    score += 1\n",
        "total_score_app4 = score/len(results)\n",
        "print(f'Total score for Approach # 4 is {total_score_app4}')"
      ],
      "metadata": {
        "id": "BvMYDpufAnF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a467dc7a-61b5-483f-c92a-8de6617ce4e9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total score for Approach # 4 is 0.6615384615384615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app5 = pd.read_json('/content/drive/MyDrive/Thesis/retrievers/results_approach5.json')\n",
        "len(app5)"
      ],
      "metadata": {
        "id": "o61JhQ-KAwAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa9cdf3-9a29-49a3-b8f7-ce5bd80b07d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "for i in range(len(app5)):\n",
        "  question = app5['question'][i]\n",
        "  doc_txt = app5['context_l2'][i]['page_content']\n",
        "  answer = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
        "  results_list.append(pd.DataFrame({'question_name': question, 'answer': [answer]}))\n",
        "results = pd.concat(results_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "9SSrdyP6AwAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14769e8a-c0ab-451d-bf9c-f301fa53dcb3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.30 ms /     7 runs   (    0.47 ms per token,  2121.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3570.87 ms /   659 tokens (    5.42 ms per token,   184.55 tokens per second)\n",
            "llama_print_timings:        eval time =     235.41 ms /     6 runs   (   39.24 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    4023.16 ms /   665 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.80 ms /     7 runs   (    0.54 ms per token,  1841.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3263.85 ms /   613 tokens (    5.32 ms per token,   187.81 tokens per second)\n",
            "llama_print_timings:        eval time =     233.52 ms /     6 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    3675.53 ms /   619 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.00 ms /     6 runs   (    0.67 ms per token,  1498.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4195.73 ms /   776 tokens (    5.41 ms per token,   184.95 tokens per second)\n",
            "llama_print_timings:        eval time =     199.50 ms /     5 runs   (   39.90 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =    4665.09 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.51 ms /     7 runs   (    0.50 ms per token,  1992.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3576.23 ms /   663 tokens (    5.39 ms per token,   185.39 tokens per second)\n",
            "llama_print_timings:        eval time =     239.64 ms /     6 runs   (   39.94 ms per token,    25.04 tokens per second)\n",
            "llama_print_timings:       total time =    4108.69 ms /   669 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1909.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3070.12 ms /   572 tokens (    5.37 ms per token,   186.31 tokens per second)\n",
            "llama_print_timings:        eval time =     237.11 ms /     6 runs   (   39.52 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    3478.53 ms /   578 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.80 ms /     7 runs   (    0.54 ms per token,  1840.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2762.32 ms /   516 tokens (    5.35 ms per token,   186.80 tokens per second)\n",
            "llama_print_timings:        eval time =     237.20 ms /     6 runs   (   39.53 ms per token,    25.30 tokens per second)\n",
            "llama_print_timings:       total time =    3153.28 ms /   522 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.38 ms /     7 runs   (    0.63 ms per token,  1598.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2371.61 ms /   445 tokens (    5.33 ms per token,   187.64 tokens per second)\n",
            "llama_print_timings:        eval time =     235.73 ms /     6 runs   (   39.29 ms per token,    25.45 tokens per second)\n",
            "llama_print_timings:       total time =    2805.90 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1905.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1805.65 ms /   340 tokens (    5.31 ms per token,   188.30 tokens per second)\n",
            "llama_print_timings:        eval time =     227.89 ms /     6 runs   (   37.98 ms per token,    26.33 tokens per second)\n",
            "llama_print_timings:       total time =    2214.30 ms /   346 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.72 ms /     7 runs   (    0.53 ms per token,  1880.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3531.38 ms /   654 tokens (    5.40 ms per token,   185.20 tokens per second)\n",
            "llama_print_timings:        eval time =     238.54 ms /     6 runs   (   39.76 ms per token,    25.15 tokens per second)\n",
            "llama_print_timings:       total time =    4015.63 ms /   660 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.87 ms /     7 runs   (    0.55 ms per token,  1810.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2802.70 ms /   523 tokens (    5.36 ms per token,   186.61 tokens per second)\n",
            "llama_print_timings:        eval time =     236.61 ms /     6 runs   (   39.43 ms per token,    25.36 tokens per second)\n",
            "llama_print_timings:       total time =    3209.76 ms /   529 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.96 ms /     6 runs   (    0.66 ms per token,  1514.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4215.10 ms /   774 tokens (    5.45 ms per token,   183.63 tokens per second)\n",
            "llama_print_timings:        eval time =     200.29 ms /     5 runs   (   40.06 ms per token,    24.96 tokens per second)\n",
            "llama_print_timings:       total time =    4762.55 ms /   779 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.88 ms /     7 runs   (    0.55 ms per token,  1805.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2360.39 ms /   444 tokens (    5.32 ms per token,   188.10 tokens per second)\n",
            "llama_print_timings:        eval time =     234.21 ms /     6 runs   (   39.03 ms per token,    25.62 tokens per second)\n",
            "llama_print_timings:       total time =    2791.51 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.88 ms /     7 runs   (    0.55 ms per token,  1806.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2086.01 ms /   398 tokens (    5.24 ms per token,   190.79 tokens per second)\n",
            "llama_print_timings:        eval time =     232.66 ms /     6 runs   (   38.78 ms per token,    25.79 tokens per second)\n",
            "llama_print_timings:       total time =    2451.19 ms /   404 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.85 ms /     7 runs   (    0.55 ms per token,  1819.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2213.92 ms /   422 tokens (    5.25 ms per token,   190.61 tokens per second)\n",
            "llama_print_timings:        eval time =     232.18 ms /     6 runs   (   38.70 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    2578.66 ms /   428 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.81 ms /     7 runs   (    0.54 ms per token,  1839.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     896.83 ms /   173 tokens (    5.18 ms per token,   192.90 tokens per second)\n",
            "llama_print_timings:        eval time =     230.40 ms /     6 runs   (   38.40 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    1197.01 ms /   179 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.60 ms /     7 runs   (    0.66 ms per token,  1521.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3505.08 ms /   651 tokens (    5.38 ms per token,   185.73 tokens per second)\n",
            "llama_print_timings:        eval time =     238.87 ms /     6 runs   (   39.81 ms per token,    25.12 tokens per second)\n",
            "llama_print_timings:       total time =    4033.03 ms /   657 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.83 ms /     7 runs   (    0.55 ms per token,  1828.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2310.49 ms /   440 tokens (    5.25 ms per token,   190.44 tokens per second)\n",
            "llama_print_timings:        eval time =     271.82 ms /     7 runs   (   38.83 ms per token,    25.75 tokens per second)\n",
            "llama_print_timings:       total time =    2810.26 ms /   447 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.03 ms /     6 runs   (    0.51 ms per token,  1978.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3713.96 ms /   692 tokens (    5.37 ms per token,   186.32 tokens per second)\n",
            "llama_print_timings:        eval time =     195.82 ms /     5 runs   (   39.16 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    4158.40 ms /   697 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.53 ms /     7 runs   (    0.50 ms per token,  1985.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2822.36 ms /   531 tokens (    5.32 ms per token,   188.14 tokens per second)\n",
            "llama_print_timings:        eval time =     230.11 ms /     6 runs   (   38.35 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    3256.47 ms /   537 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       6.88 ms /     7 runs   (    0.98 ms per token,  1017.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3286.61 ms /   614 tokens (    5.35 ms per token,   186.82 tokens per second)\n",
            "llama_print_timings:        eval time =     234.17 ms /     6 runs   (   39.03 ms per token,    25.62 tokens per second)\n",
            "llama_print_timings:       total time =    3861.58 ms /   620 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.73 ms /     7 runs   (    0.53 ms per token,  1877.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4037.91 ms /   751 tokens (    5.38 ms per token,   185.99 tokens per second)\n",
            "llama_print_timings:        eval time =     235.85 ms /     6 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    4582.02 ms /   757 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.20 ms /     6 runs   (    0.70 ms per token,  1427.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5955.20 ms /  1077 tokens (    5.53 ms per token,   180.85 tokens per second)\n",
            "llama_print_timings:        eval time =     200.97 ms /     5 runs   (   40.19 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =    6614.22 ms /  1082 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.92 ms /     7 runs   (    0.70 ms per token,  1422.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2089.90 ms /   397 tokens (    5.26 ms per token,   189.96 tokens per second)\n",
            "llama_print_timings:        eval time =     234.01 ms /     6 runs   (   39.00 ms per token,    25.64 tokens per second)\n",
            "llama_print_timings:       total time =    2549.75 ms /   403 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.87 ms /     7 runs   (    0.55 ms per token,  1809.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2043.29 ms /   390 tokens (    5.24 ms per token,   190.87 tokens per second)\n",
            "llama_print_timings:        eval time =     229.47 ms /     6 runs   (   38.25 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    2399.19 ms /   396 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.74 ms /     7 runs   (    0.53 ms per token,  1872.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2327.59 ms /   444 tokens (    5.24 ms per token,   190.76 tokens per second)\n",
            "llama_print_timings:        eval time =     230.50 ms /     6 runs   (   38.42 ms per token,    26.03 tokens per second)\n",
            "llama_print_timings:       total time =    2691.63 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.75 ms /     7 runs   (    0.54 ms per token,  1867.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1895.72 ms /   365 tokens (    5.19 ms per token,   192.54 tokens per second)\n",
            "llama_print_timings:        eval time =     226.52 ms /     6 runs   (   37.75 ms per token,    26.49 tokens per second)\n",
            "llama_print_timings:       total time =    2235.81 ms /   371 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.92 ms /     7 runs   (    0.56 ms per token,  1784.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2506.86 ms /   479 tokens (    5.23 ms per token,   191.08 tokens per second)\n",
            "llama_print_timings:        eval time =     230.15 ms /     6 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    2879.40 ms /   485 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.02 ms /     7 runs   (    0.72 ms per token,  1393.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1502.34 ms /   275 tokens (    5.46 ms per token,   183.05 tokens per second)\n",
            "llama_print_timings:        eval time =     232.90 ms /     6 runs   (   38.82 ms per token,    25.76 tokens per second)\n",
            "llama_print_timings:       total time =    1897.21 ms /   281 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.70 ms /     7 runs   (    0.53 ms per token,  1893.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2520.75 ms /   479 tokens (    5.26 ms per token,   190.02 tokens per second)\n",
            "llama_print_timings:        eval time =     231.06 ms /     6 runs   (   38.51 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    2955.17 ms /   485 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.69 ms /     7 runs   (    0.53 ms per token,  1898.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2206.72 ms /   424 tokens (    5.20 ms per token,   192.14 tokens per second)\n",
            "llama_print_timings:        eval time =     268.09 ms /     7 runs   (   38.30 ms per token,    26.11 tokens per second)\n",
            "llama_print_timings:       total time =    2612.22 ms /   431 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.80 ms /     7 runs   (    0.54 ms per token,  1842.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2862.55 ms /   544 tokens (    5.26 ms per token,   190.04 tokens per second)\n",
            "llama_print_timings:        eval time =     272.83 ms /     7 runs   (   38.98 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    3296.86 ms /   551 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.98 ms /     7 runs   (    0.57 ms per token,  1759.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2778.93 ms /   525 tokens (    5.29 ms per token,   188.92 tokens per second)\n",
            "llama_print_timings:        eval time =     236.49 ms /     6 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    3194.94 ms /   531 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.01 ms /     7 runs   (    0.57 ms per token,  1743.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1468.08 ms /   269 tokens (    5.46 ms per token,   183.23 tokens per second)\n",
            "llama_print_timings:        eval time =     234.23 ms /     6 runs   (   39.04 ms per token,    25.62 tokens per second)\n",
            "llama_print_timings:       total time =    1860.76 ms /   275 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.80 ms /     7 runs   (    0.54 ms per token,  1841.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2127.51 ms /   407 tokens (    5.23 ms per token,   191.30 tokens per second)\n",
            "llama_print_timings:        eval time =     233.08 ms /     6 runs   (   38.85 ms per token,    25.74 tokens per second)\n",
            "llama_print_timings:       total time =    2521.38 ms /   413 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.58 ms /     7 runs   (    0.51 ms per token,  1953.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2079.34 ms /   399 tokens (    5.21 ms per token,   191.89 tokens per second)\n",
            "llama_print_timings:        eval time =     231.09 ms /     6 runs   (   38.51 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    2433.03 ms /   405 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.70 ms /     7 runs   (    0.53 ms per token,  1894.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     479.07 ms /    93 tokens (    5.15 ms per token,   194.13 tokens per second)\n",
            "llama_print_timings:        eval time =     222.35 ms /     6 runs   (   37.06 ms per token,    26.98 tokens per second)\n",
            "llama_print_timings:       total time =     746.18 ms /    99 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.59 ms /     7 runs   (    0.51 ms per token,  1948.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2645.81 ms /   503 tokens (    5.26 ms per token,   190.11 tokens per second)\n",
            "llama_print_timings:        eval time =     230.86 ms /     6 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    3021.76 ms /   509 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       6.34 ms /     7 runs   (    0.91 ms per token,  1103.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2208.34 ms /   420 tokens (    5.26 ms per token,   190.19 tokens per second)\n",
            "llama_print_timings:        eval time =     233.53 ms /     6 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    2598.00 ms /   426 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.17 ms /     7 runs   (    0.60 ms per token,  1679.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2448.91 ms /   462 tokens (    5.30 ms per token,   188.66 tokens per second)\n",
            "llama_print_timings:        eval time =     233.82 ms /     6 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    2934.36 ms /   468 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.04 ms /     6 runs   (    0.51 ms per token,  1970.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3129.01 ms /   588 tokens (    5.32 ms per token,   187.92 tokens per second)\n",
            "llama_print_timings:        eval time =     192.59 ms /     5 runs   (   38.52 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    3484.65 ms /   593 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.62 ms /     7 runs   (    0.52 ms per token,  1934.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2738.67 ms /   515 tokens (    5.32 ms per token,   188.05 tokens per second)\n",
            "llama_print_timings:        eval time =     231.85 ms /     6 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    3134.34 ms /   521 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.58 ms /     7 runs   (    0.65 ms per token,  1530.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3406.80 ms /   640 tokens (    5.32 ms per token,   187.86 tokens per second)\n",
            "llama_print_timings:        eval time =     281.34 ms /     7 runs   (   40.19 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =    3929.82 ms /   647 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.68 ms /     6 runs   (    0.45 ms per token,  2239.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3106.96 ms /   579 tokens (    5.37 ms per token,   186.36 tokens per second)\n",
            "llama_print_timings:        eval time =     193.37 ms /     5 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    3565.02 ms /   584 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.26 ms /     7 runs   (    0.47 ms per token,  2145.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2690.24 ms /   507 tokens (    5.31 ms per token,   188.46 tokens per second)\n",
            "llama_print_timings:        eval time =     231.95 ms /     6 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    3080.16 ms /   513 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.60 ms /     7 runs   (    0.51 ms per token,  1943.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3311.69 ms /   621 tokens (    5.33 ms per token,   187.52 tokens per second)\n",
            "llama_print_timings:        eval time =     230.32 ms /     6 runs   (   38.39 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    3744.16 ms /   627 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.23 ms /     7 runs   (    0.75 ms per token,  1338.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4006.93 ms /   741 tokens (    5.41 ms per token,   184.93 tokens per second)\n",
            "llama_print_timings:        eval time =     238.02 ms /     6 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    4564.85 ms /   747 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.85 ms /     7 runs   (    0.55 ms per token,  1816.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3314.52 ms /   621 tokens (    5.34 ms per token,   187.36 tokens per second)\n",
            "llama_print_timings:        eval time =     233.22 ms /     6 runs   (   38.87 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    3774.56 ms /   627 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.12 ms /     6 runs   (    0.52 ms per token,  1921.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1650.67 ms /   319 tokens (    5.17 ms per token,   193.25 tokens per second)\n",
            "llama_print_timings:        eval time =     190.00 ms /     5 runs   (   38.00 ms per token,    26.32 tokens per second)\n",
            "llama_print_timings:       total time =    1949.56 ms /   324 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.40 ms /     6 runs   (    0.57 ms per token,  1765.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     225.62 ms /     6 runs   (   37.60 ms per token,    26.59 tokens per second)\n",
            "llama_print_timings:       total time =     252.62 ms /     6 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.66 ms /     7 runs   (    0.52 ms per token,  1910.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2514.28 ms /   478 tokens (    5.26 ms per token,   190.11 tokens per second)\n",
            "llama_print_timings:        eval time =     232.49 ms /     6 runs   (   38.75 ms per token,    25.81 tokens per second)\n",
            "llama_print_timings:       total time =    2909.20 ms /   484 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.00 ms /     7 runs   (    0.71 ms per token,  1398.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2164.89 ms /   412 tokens (    5.25 ms per token,   190.31 tokens per second)\n",
            "llama_print_timings:        eval time =     233.84 ms /     6 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    2569.79 ms /   418 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       5.43 ms /     7 runs   (    0.78 ms per token,  1289.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2276.34 ms /   432 tokens (    5.27 ms per token,   189.78 tokens per second)\n",
            "llama_print_timings:        eval time =     235.40 ms /     6 runs   (   39.23 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    2797.71 ms /   438 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.24 ms /     7 runs   (    0.61 ms per token,  1650.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2343.28 ms /   443 tokens (    5.29 ms per token,   189.05 tokens per second)\n",
            "llama_print_timings:        eval time =     230.91 ms /     6 runs   (   38.48 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2754.95 ms /   449 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.87 ms /     7 runs   (    0.55 ms per token,  1808.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2828.35 ms /   536 tokens (    5.28 ms per token,   189.51 tokens per second)\n",
            "llama_print_timings:        eval time =     270.41 ms /     7 runs   (   38.63 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    3307.04 ms /   543 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.24 ms /     6 runs   (    0.71 ms per token,  1416.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4825.21 ms /   888 tokens (    5.43 ms per token,   184.03 tokens per second)\n",
            "llama_print_timings:        eval time =     199.97 ms /     5 runs   (   39.99 ms per token,    25.00 tokens per second)\n",
            "llama_print_timings:       total time =    5397.02 ms /   893 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.25 ms /     7 runs   (    0.61 ms per token,  1647.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2892.04 ms /   544 tokens (    5.32 ms per token,   188.10 tokens per second)\n",
            "llama_print_timings:        eval time =     271.08 ms /     7 runs   (   38.73 ms per token,    25.82 tokens per second)\n",
            "llama_print_timings:       total time =    3451.63 ms /   551 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.57 ms /     7 runs   (    0.51 ms per token,  1962.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2564.59 ms /   488 tokens (    5.26 ms per token,   190.28 tokens per second)\n",
            "llama_print_timings:        eval time =     234.44 ms /     6 runs   (   39.07 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    2986.61 ms /   494 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.76 ms /     7 runs   (    0.54 ms per token,  1861.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2211.88 ms /   423 tokens (    5.23 ms per token,   191.24 tokens per second)\n",
            "llama_print_timings:        eval time =     231.78 ms /     6 runs   (   38.63 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    2593.86 ms /   429 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.78 ms /     7 runs   (    0.54 ms per token,  1850.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2212.94 ms /   424 tokens (    5.22 ms per token,   191.60 tokens per second)\n",
            "llama_print_timings:        eval time =     231.72 ms /     6 runs   (   38.62 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    2596.33 ms /   430 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.62 ms /     7 runs   (    0.66 ms per token,  1513.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2227.68 ms /   421 tokens (    5.29 ms per token,   188.99 tokens per second)\n",
            "llama_print_timings:        eval time =     234.65 ms /     6 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    2706.91 ms /   427 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1939.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2204.16 ms /   414 tokens (    5.32 ms per token,   187.83 tokens per second)\n",
            "llama_print_timings:        eval time =     233.32 ms /     6 runs   (   38.89 ms per token,    25.72 tokens per second)\n",
            "llama_print_timings:       total time =    2630.66 ms /   420 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.28 ms /     7 runs   (    0.61 ms per token,  1636.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2298.50 ms /   439 tokens (    5.24 ms per token,   190.99 tokens per second)\n",
            "llama_print_timings:        eval time =     231.95 ms /     6 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    2679.63 ms /   445 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.77 ms /     7 runs   (    0.54 ms per token,  1857.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2171.99 ms /   414 tokens (    5.25 ms per token,   190.61 tokens per second)\n",
            "llama_print_timings:        eval time =     230.90 ms /     6 runs   (   38.48 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2540.36 ms /   420 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.91 ms /     7 runs   (    0.56 ms per token,  1791.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2423.46 ms /   458 tokens (    5.29 ms per token,   188.99 tokens per second)\n",
            "llama_print_timings:        eval time =     230.70 ms /     6 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2807.73 ms /   464 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "for i in range(len(results)):\n",
        "  if results['answer'][i]['score'] == 'yes':\n",
        "    score += 1\n",
        "total_score_app5 = score/len(results)\n",
        "print(f'Total score for Approach # 5 is {total_score_app5}')"
      ],
      "metadata": {
        "id": "Q5pkvTKKAwAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86468890-af1a-4141-ce8a-e97a12f831b4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total score for Approach # 5 is 0.703125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app6 = pd.read_json('/content/drive/MyDrive/Thesis/retrievers/results_approach6.json')\n",
        "len(app6)"
      ],
      "metadata": {
        "id": "8EZZjnGOA3lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef658e90-7e5d-408a-a4fe-6b3edda22efb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_list = []\n",
        "for i in range(len(app6)):\n",
        "  question = app6['question_name'][i]\n",
        "  doc_txt = app6['context_ip_name'][i]['page_content']\n",
        "  try:\n",
        "    answer = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
        "    results_list.append(pd.DataFrame({'question_name': question, 'answer': [answer]}))\n",
        "  except JSONDecodeError as e:\n",
        "    print(f\"JSONDecodeError occurred for question: {question}. Skipping...\")\n",
        "    continue\n",
        "  except OutputParserException as e:\n",
        "    print(f\"OutputParserException occurred for question: {question}. Skipping...\")\n",
        "    continue\n",
        "results = pd.concat(results_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "W-vzPH1CA3lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e05535-61c2-40c3-af05-1ff59e1fd9ae"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.89 ms /     6 runs   (    0.48 ms per token,  2079.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6047.28 ms /  1083 tokens (    5.58 ms per token,   179.09 tokens per second)\n",
            "llama_print_timings:        eval time =     202.07 ms /     5 runs   (   40.41 ms per token,    24.74 tokens per second)\n",
            "llama_print_timings:       total time =    6697.32 ms /  1088 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1965.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5063.39 ms /   926 tokens (    5.47 ms per token,   182.88 tokens per second)\n",
            "llama_print_timings:        eval time =     243.18 ms /     6 runs   (   40.53 ms per token,    24.67 tokens per second)\n",
            "llama_print_timings:       total time =    5591.61 ms /   932 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.49 ms /     7 runs   (    0.64 ms per token,  1559.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4431.59 ms /   814 tokens (    5.44 ms per token,   183.68 tokens per second)\n",
            "llama_print_timings:        eval time =     239.11 ms /     6 runs   (   39.85 ms per token,    25.09 tokens per second)\n",
            "llama_print_timings:       total time =    4933.01 ms /   820 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.61 ms /     7 runs   (    0.52 ms per token,  1940.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5152.65 ms /   930 tokens (    5.54 ms per token,   180.49 tokens per second)\n",
            "llama_print_timings:        eval time =     244.11 ms /     6 runs   (   40.68 ms per token,    24.58 tokens per second)\n",
            "llama_print_timings:       total time =    5750.44 ms /   936 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.34 ms /     7 runs   (    0.48 ms per token,  2095.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2765.37 ms /   520 tokens (    5.32 ms per token,   188.04 tokens per second)\n",
            "llama_print_timings:        eval time =     234.41 ms /     6 runs   (   39.07 ms per token,    25.60 tokens per second)\n",
            "llama_print_timings:       total time =    3154.97 ms /   526 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.79 ms /     7 runs   (    0.54 ms per token,  1847.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2760.21 ms /   516 tokens (    5.35 ms per token,   186.94 tokens per second)\n",
            "llama_print_timings:        eval time =     235.64 ms /     6 runs   (   39.27 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =    3154.20 ms /   522 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.84 ms /     7 runs   (    0.55 ms per token,  1821.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5104.80 ms /   923 tokens (    5.53 ms per token,   180.81 tokens per second)\n",
            "llama_print_timings:        eval time =     243.90 ms /     6 runs   (   40.65 ms per token,    24.60 tokens per second)\n",
            "llama_print_timings:       total time =    5729.89 ms /   929 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.83 ms /     7 runs   (    0.55 ms per token,  1825.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4382.75 ms /   807 tokens (    5.43 ms per token,   184.13 tokens per second)\n",
            "llama_print_timings:        eval time =     238.24 ms /     6 runs   (   39.71 ms per token,    25.18 tokens per second)\n",
            "llama_print_timings:       total time =    4858.45 ms /   813 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      52.71 ms /    96 runs   (    0.55 ms per token,  1821.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5995.50 ms /  1078 tokens (    5.56 ms per token,   179.80 tokens per second)\n",
            "llama_print_timings:        eval time =    3860.04 ms /    95 runs   (   40.63 ms per token,    24.61 tokens per second)\n",
            "llama_print_timings:       total time =   10580.99 ms /  1173 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.96 ms /     7 runs   (    0.57 ms per token,  1766.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5006.07 ms /   920 tokens (    5.44 ms per token,   183.78 tokens per second)\n",
            "llama_print_timings:        eval time =     281.78 ms /     7 runs   (   40.25 ms per token,    24.84 tokens per second)\n",
            "llama_print_timings:       total time =    5563.62 ms /   927 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.03 ms /     6 runs   (    0.51 ms per token,  1980.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6019.26 ms /  1087 tokens (    5.54 ms per token,   180.59 tokens per second)\n",
            "llama_print_timings:        eval time =     200.48 ms /     5 runs   (   40.10 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =    6664.58 ms /  1092 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.77 ms /     7 runs   (    0.54 ms per token,  1855.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4706.75 ms /   870 tokens (    5.41 ms per token,   184.84 tokens per second)\n",
            "llama_print_timings:        eval time =     237.87 ms /     6 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    5193.57 ms /   876 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.95 ms /     6 runs   (    0.66 ms per token,  1520.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4085.51 ms /   759 tokens (    5.38 ms per token,   185.78 tokens per second)\n",
            "llama_print_timings:        eval time =     197.78 ms /     5 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    4616.17 ms /   764 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.69 ms /     9 runs   (    0.52 ms per token,  1918.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3980.01 ms /   743 tokens (    5.36 ms per token,   186.68 tokens per second)\n",
            "llama_print_timings:        eval time =     314.46 ms /     8 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    4532.91 ms /   751 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.81 ms /     7 runs   (    0.55 ms per token,  1834.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5307.77 ms /   976 tokens (    5.44 ms per token,   183.88 tokens per second)\n",
            "llama_print_timings:        eval time =     241.17 ms /     6 runs   (   40.20 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =    5825.34 ms /   982 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.68 ms /     7 runs   (    0.53 ms per token,  1902.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5238.62 ms /   958 tokens (    5.47 ms per token,   182.87 tokens per second)\n",
            "llama_print_timings:        eval time =     239.76 ms /     6 runs   (   39.96 ms per token,    25.02 tokens per second)\n",
            "llama_print_timings:       total time =    5876.40 ms /   964 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      55.20 ms /   101 runs   (    0.55 ms per token,  1829.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1308.73 ms /   254 tokens (    5.15 ms per token,   194.08 tokens per second)\n",
            "llama_print_timings:        eval time =    3781.38 ms /   100 runs   (   37.81 ms per token,    26.45 tokens per second)\n",
            "llama_print_timings:       total time =    5474.04 ms /   354 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.17 ms /     6 runs   (    0.53 ms per token,  1890.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5378.55 ms /   978 tokens (    5.50 ms per token,   181.83 tokens per second)\n",
            "llama_print_timings:        eval time =     198.99 ms /     5 runs   (   39.80 ms per token,    25.13 tokens per second)\n",
            "llama_print_timings:       total time =    5972.86 ms /   983 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.05 ms /     6 runs   (    0.51 ms per token,  1969.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5993.66 ms /  1088 tokens (    5.51 ms per token,   181.52 tokens per second)\n",
            "llama_print_timings:        eval time =     240.69 ms /     6 runs   (   40.11 ms per token,    24.93 tokens per second)\n",
            "llama_print_timings:       total time =    6533.56 ms /  1094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       6.35 ms /     9 runs   (    0.71 ms per token,  1417.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4041.60 ms /   747 tokens (    5.41 ms per token,   184.83 tokens per second)\n",
            "llama_print_timings:        eval time =     319.84 ms /     8 runs   (   39.98 ms per token,    25.01 tokens per second)\n",
            "llama_print_timings:       total time =    4648.46 ms /   755 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.82 ms /     7 runs   (    0.55 ms per token,  1830.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2532.99 ms /   478 tokens (    5.30 ms per token,   188.71 tokens per second)\n",
            "llama_print_timings:        eval time =     232.44 ms /     6 runs   (   38.74 ms per token,    25.81 tokens per second)\n",
            "llama_print_timings:       total time =    2982.10 ms /   484 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      58.56 ms /   107 runs   (    0.55 ms per token,  1827.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4139.33 ms /   768 tokens (    5.39 ms per token,   185.54 tokens per second)\n",
            "llama_print_timings:        eval time =    4194.34 ms /   106 runs   (   39.57 ms per token,    25.27 tokens per second)\n",
            "llama_print_timings:       total time =    8882.98 ms /   874 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =     140.16 ms /   256 runs   (    0.55 ms per token,  1826.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2934.70 ms /   546 tokens (    5.37 ms per token,   186.05 tokens per second)\n",
            "llama_print_timings:        eval time =   10028.53 ms /   255 runs   (   39.33 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =   14125.51 ms /   801 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =      43.69 ms /    79 runs   (    0.55 ms per token,  1808.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6027.65 ms /  1086 tokens (    5.55 ms per token,   180.17 tokens per second)\n",
            "llama_print_timings:        eval time =    3160.12 ms /    78 runs   (   40.51 ms per token,    24.68 tokens per second)\n",
            "llama_print_timings:       total time =    9819.86 ms /  1164 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.80 ms /     7 runs   (    0.69 ms per token,  1458.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2782.69 ms /   525 tokens (    5.30 ms per token,   188.67 tokens per second)\n",
            "llama_print_timings:        eval time =     233.38 ms /     6 runs   (   38.90 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    3205.98 ms /   531 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.71 ms /     7 runs   (    0.53 ms per token,  1888.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2712.06 ms /   507 tokens (    5.35 ms per token,   186.94 tokens per second)\n",
            "llama_print_timings:        eval time =     233.45 ms /     6 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    3199.34 ms /   513 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1908.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2557.89 ms /   487 tokens (    5.25 ms per token,   190.39 tokens per second)\n",
            "llama_print_timings:        eval time =     234.56 ms /     6 runs   (   39.09 ms per token,    25.58 tokens per second)\n",
            "llama_print_timings:       total time =    2936.95 ms /   493 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.79 ms /     7 runs   (    0.54 ms per token,  1847.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1533.29 ms /   283 tokens (    5.42 ms per token,   184.57 tokens per second)\n",
            "llama_print_timings:        eval time =     234.03 ms /     6 runs   (   39.00 ms per token,    25.64 tokens per second)\n",
            "llama_print_timings:       total time =    1860.95 ms /   289 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.10 ms /     6 runs   (    0.52 ms per token,  1932.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3798.09 ms /   710 tokens (    5.35 ms per token,   186.94 tokens per second)\n",
            "llama_print_timings:        eval time =     198.19 ms /     5 runs   (   39.64 ms per token,    25.23 tokens per second)\n",
            "llama_print_timings:       total time =    4207.84 ms /   715 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.92 ms /     6 runs   (    0.49 ms per token,  2053.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4002.95 ms /   740 tokens (    5.41 ms per token,   184.86 tokens per second)\n",
            "llama_print_timings:        eval time =     196.94 ms /     5 runs   (   39.39 ms per token,    25.39 tokens per second)\n",
            "llama_print_timings:       total time =    4533.34 ms /   745 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.76 ms /     7 runs   (    0.54 ms per token,  1860.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2245.52 ms /   429 tokens (    5.23 ms per token,   191.05 tokens per second)\n",
            "llama_print_timings:        eval time =     231.80 ms /     6 runs   (   38.63 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2607.43 ms /   435 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.62 ms /     7 runs   (    0.52 ms per token,  1932.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2905.22 ms /   549 tokens (    5.29 ms per token,   188.97 tokens per second)\n",
            "llama_print_timings:        eval time =     230.86 ms /     6 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    3301.74 ms /   555 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.74 ms /     7 runs   (    0.53 ms per token,  1870.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2382.21 ms /   456 tokens (    5.22 ms per token,   191.42 tokens per second)\n",
            "llama_print_timings:        eval time =     231.96 ms /     6 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    2751.10 ms /   462 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.57 ms /     7 runs   (    0.65 ms per token,  1532.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2079.39 ms /   394 tokens (    5.28 ms per token,   189.48 tokens per second)\n",
            "llama_print_timings:        eval time =     233.33 ms /     6 runs   (   38.89 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    2508.20 ms /   400 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1967.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3053.70 ms /   573 tokens (    5.33 ms per token,   187.64 tokens per second)\n",
            "llama_print_timings:        eval time =     231.99 ms /     6 runs   (   38.66 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    3525.56 ms /   579 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1967.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =     270.55 ms /     7 runs   (   38.65 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =     294.96 ms /     7 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.46 ms /     7 runs   (    0.64 ms per token,  1568.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1862.95 ms /   355 tokens (    5.25 ms per token,   190.56 tokens per second)\n",
            "llama_print_timings:        eval time =     228.37 ms /     6 runs   (   38.06 ms per token,    26.27 tokens per second)\n",
            "llama_print_timings:       total time =    2245.75 ms /   361 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.37 ms /     7 runs   (    0.62 ms per token,  1601.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2046.96 ms /   392 tokens (    5.22 ms per token,   191.50 tokens per second)\n",
            "llama_print_timings:        eval time =     233.53 ms /     6 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    2488.68 ms /   398 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.84 ms /     6 runs   (    0.47 ms per token,  2114.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5913.47 ms /  1070 tokens (    5.53 ms per token,   180.94 tokens per second)\n",
            "llama_print_timings:        eval time =     199.93 ms /     5 runs   (   39.99 ms per token,    25.01 tokens per second)\n",
            "llama_print_timings:       total time =    6539.06 ms /  1075 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.58 ms /     7 runs   (    0.51 ms per token,  1956.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2463.26 ms /   466 tokens (    5.29 ms per token,   189.18 tokens per second)\n",
            "llama_print_timings:        eval time =     233.83 ms /     6 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    2839.28 ms /   472 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       6.70 ms /    13 runs   (    0.52 ms per token,  1940.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3567.59 ms /   666 tokens (    5.36 ms per token,   186.68 tokens per second)\n",
            "llama_print_timings:        eval time =     473.58 ms /    12 runs   (   39.46 ms per token,    25.34 tokens per second)\n",
            "llama_print_timings:       total time =    4249.18 ms /   678 tokens\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OutputParserException occurred for question: Are there any tax-related risks or benefits for the COCA COLA CO mentioned?. Skipping...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.99 ms /     6 runs   (    0.66 ms per token,  1504.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3350.80 ms /   629 tokens (    5.33 ms per token,   187.72 tokens per second)\n",
            "llama_print_timings:        eval time =     195.73 ms /     5 runs   (   39.15 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    3769.27 ms /   634 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.56 ms /     9 runs   (    0.51 ms per token,  1972.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3957.98 ms /   735 tokens (    5.39 ms per token,   185.70 tokens per second)\n",
            "llama_print_timings:        eval time =     314.66 ms /     8 runs   (   39.33 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    4566.19 ms /   743 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.63 ms /     7 runs   (    0.52 ms per token,  1926.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2820.96 ms /   530 tokens (    5.32 ms per token,   187.88 tokens per second)\n",
            "llama_print_timings:        eval time =     232.37 ms /     6 runs   (   38.73 ms per token,    25.82 tokens per second)\n",
            "llama_print_timings:       total time =    3211.65 ms /   536 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.71 ms /     7 runs   (    0.53 ms per token,  1884.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2379.61 ms /   453 tokens (    5.25 ms per token,   190.37 tokens per second)\n",
            "llama_print_timings:        eval time =     233.17 ms /     6 runs   (   38.86 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    2746.79 ms /   459 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.04 ms /     7 runs   (    0.58 ms per token,  1732.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4699.76 ms /   864 tokens (    5.44 ms per token,   183.84 tokens per second)\n",
            "llama_print_timings:        eval time =     276.53 ms /     7 runs   (   39.50 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    5342.89 ms /   871 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.33 ms /     7 runs   (    0.48 ms per token,  2103.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4858.70 ms /   893 tokens (    5.44 ms per token,   183.79 tokens per second)\n",
            "llama_print_timings:        eval time =     241.97 ms /     6 runs   (   40.33 ms per token,    24.80 tokens per second)\n",
            "llama_print_timings:       total time =    5348.53 ms /   899 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.74 ms /     7 runs   (    0.53 ms per token,  1872.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2736.57 ms /   517 tokens (    5.29 ms per token,   188.92 tokens per second)\n",
            "llama_print_timings:        eval time =     234.86 ms /     6 runs   (   39.14 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    3126.91 ms /   523 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.87 ms /     7 runs   (    0.70 ms per token,  1436.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1269.05 ms /   238 tokens (    5.33 ms per token,   187.54 tokens per second)\n",
            "llama_print_timings:        eval time =     233.25 ms /     6 runs   (   38.88 ms per token,    25.72 tokens per second)\n",
            "llama_print_timings:       total time =    1613.59 ms /   244 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       2.91 ms /     6 runs   (    0.49 ms per token,  2061.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3974.83 ms /   731 tokens (    5.44 ms per token,   183.91 tokens per second)\n",
            "llama_print_timings:        eval time =     196.15 ms /     5 runs   (   39.23 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    4481.90 ms /   736 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.06 ms /     6 runs   (    0.51 ms per token,  1958.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3492.27 ms /   653 tokens (    5.35 ms per token,   186.98 tokens per second)\n",
            "llama_print_timings:        eval time =     198.68 ms /     5 runs   (   39.74 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =    3879.07 ms /   658 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.56 ms /     7 runs   (    0.51 ms per token,  1966.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2163.95 ms /   413 tokens (    5.24 ms per token,   190.85 tokens per second)\n",
            "llama_print_timings:        eval time =     234.39 ms /     6 runs   (   39.06 ms per token,    25.60 tokens per second)\n",
            "llama_print_timings:       total time =    2526.05 ms /   419 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       6.19 ms /     7 runs   (    0.88 ms per token,  1131.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1842.96 ms /   347 tokens (    5.31 ms per token,   188.28 tokens per second)\n",
            "llama_print_timings:        eval time =     233.30 ms /     6 runs   (   38.88 ms per token,    25.72 tokens per second)\n",
            "llama_print_timings:       total time =    2216.13 ms /   353 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.78 ms /     7 runs   (    0.68 ms per token,  1464.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2363.73 ms /   447 tokens (    5.29 ms per token,   189.11 tokens per second)\n",
            "llama_print_timings:        eval time =     233.52 ms /     6 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    2847.06 ms /   453 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.01 ms /     7 runs   (    0.57 ms per token,  1743.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2291.57 ms /   436 tokens (    5.26 ms per token,   190.26 tokens per second)\n",
            "llama_print_timings:        eval time =     232.88 ms /     6 runs   (   38.81 ms per token,    25.76 tokens per second)\n",
            "llama_print_timings:       total time =    2656.81 ms /   442 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.67 ms /     7 runs   (    0.52 ms per token,  1909.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5047.27 ms /   927 tokens (    5.44 ms per token,   183.66 tokens per second)\n",
            "llama_print_timings:        eval time =     241.81 ms /     6 runs   (   40.30 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =    5555.81 ms /   933 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.27 ms /     6 runs   (    0.54 ms per token,  1837.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5211.96 ms /   952 tokens (    5.47 ms per token,   182.66 tokens per second)\n",
            "llama_print_timings:        eval time =     200.84 ms /     5 runs   (   40.17 ms per token,    24.90 tokens per second)\n",
            "llama_print_timings:       total time =    5795.03 ms /   957 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.66 ms /     7 runs   (    0.52 ms per token,  1914.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2772.34 ms /   524 tokens (    5.29 ms per token,   189.01 tokens per second)\n",
            "llama_print_timings:        eval time =     233.50 ms /     6 runs   (   38.92 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    3165.08 ms /   530 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.73 ms /     7 runs   (    0.53 ms per token,  1876.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2291.71 ms /   440 tokens (    5.21 ms per token,   192.00 tokens per second)\n",
            "llama_print_timings:        eval time =     231.83 ms /     6 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2655.59 ms /   446 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.53 ms /     7 runs   (    0.50 ms per token,  1983.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2643.58 ms /   504 tokens (    5.25 ms per token,   190.65 tokens per second)\n",
            "llama_print_timings:        eval time =     273.63 ms /     7 runs   (   39.09 ms per token,    25.58 tokens per second)\n",
            "llama_print_timings:       total time =    3069.87 ms /   511 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.98 ms /     7 runs   (    0.71 ms per token,  1406.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1957.03 ms /   375 tokens (    5.22 ms per token,   191.62 tokens per second)\n",
            "llama_print_timings:        eval time =     231.88 ms /     6 runs   (   38.65 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2390.73 ms /   381 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.63 ms /     7 runs   (    0.52 ms per token,  1928.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3811.45 ms /   711 tokens (    5.36 ms per token,   186.54 tokens per second)\n",
            "llama_print_timings:        eval time =     234.26 ms /     6 runs   (   39.04 ms per token,    25.61 tokens per second)\n",
            "llama_print_timings:       total time =    4303.56 ms /   717 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.35 ms /     6 runs   (    0.56 ms per token,  1789.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4161.42 ms /   776 tokens (    5.36 ms per token,   186.47 tokens per second)\n",
            "llama_print_timings:        eval time =     196.23 ms /     5 runs   (   39.25 ms per token,    25.48 tokens per second)\n",
            "llama_print_timings:       total time =    4572.38 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.81 ms /     7 runs   (    0.54 ms per token,  1835.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1815.44 ms /   352 tokens (    5.16 ms per token,   193.89 tokens per second)\n",
            "llama_print_timings:        eval time =     268.56 ms /     7 runs   (   38.37 ms per token,    26.06 tokens per second)\n",
            "llama_print_timings:       total time =    2202.85 ms /   359 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       4.76 ms /     7 runs   (    0.68 ms per token,  1471.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1788.40 ms /   344 tokens (    5.20 ms per token,   192.35 tokens per second)\n",
            "llama_print_timings:        eval time =     229.04 ms /     6 runs   (   38.17 ms per token,    26.20 tokens per second)\n",
            "llama_print_timings:       total time =    2208.21 ms /   350 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     412.01 ms\n",
            "llama_print_timings:      sample time =       3.94 ms /     7 runs   (    0.56 ms per token,  1778.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2475.02 ms /   468 tokens (    5.29 ms per token,   189.09 tokens per second)\n",
            "llama_print_timings:        eval time =     230.51 ms /     6 runs   (   38.42 ms per token,    26.03 tokens per second)\n",
            "llama_print_timings:       total time =    2894.50 ms /   474 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "for i in range(len(results)):\n",
        "  if (results['answer'][i] is not None) and (results['answer'][i]['score'] == 'yes'):\n",
        "    score += 1\n",
        "total_score_app6 = score/len(results)\n",
        "print(f'Total score for Approach # 6 is {total_score_app6}')"
      ],
      "metadata": {
        "id": "MzbqZx4BA3lG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf35d1c7-26de-432d-91e6-ac437bf62f17"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total score for Approach # 6 is 0.6615384615384615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final results:"
      ],
      "metadata": {
        "id": "pFbrbEPrBBTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total score for Approach # 1 is {total_score_app1}')\n",
        "print(f'Total score for Approach # 2 is {total_score_app2}')\n",
        "print(f'Total score for Approach # 3 is {total_score_app3}')\n",
        "print(f'Total score for Approach # 4 is {total_score_app4}')\n",
        "print(f'Total score for Approach # 5 is {total_score_app5}')\n",
        "print(f'Total score for Approach # 6 is {total_score_app6}')"
      ],
      "metadata": {
        "id": "-B_f4PGfBCo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e014e6b-9469-42a0-c975-2a7332a76d9a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total score for Approach # 1 is 0.703125\n",
            "Total score for Approach # 2 is 0.6615384615384615\n",
            "Total score for Approach # 3 is 0.703125\n",
            "Total score for Approach # 4 is 0.6615384615384615\n",
            "Total score for Approach # 5 is 0.703125\n",
            "Total score for Approach # 6 is 0.6615384615384615\n"
          ]
        }
      ]
    }
  ]
}