{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9R93slN2aLBUJjyiRY2Me",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "543e6a78a566481d8d9a2c0db729bf23": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_5f92e0be9e174d4c8047c20123d47e1f",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Pushing records to Argilla... \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[35m 91%\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pushing records to Argilla... <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 91%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "5f92e0be9e174d4c8047c20123d47e1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52744fd2db4548bf8ad2c19e94ec3397": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e3089230d5fa4d3c8d221ede8de9f3d2",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Pushing records to Argilla... \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[35m 85%\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pushing records to Argilla... <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 85%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "e3089230d5fa4d3c8d221ede8de9f3d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d135e80d9bd847578452bef9e4b67177": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_bd8f98908bda49a79a2c20b9eba01456",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Pushing records to Argilla... \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━\u001b[0m \u001b[35m 91%\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pushing records to Argilla... <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 91%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "bd8f98908bda49a79a2c20b9eba01456": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "428ae5f77847477eb3ff26ea9cc105c8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d3410b6bc0a64ef89cf2fff708912ada",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Pushing records to Argilla... \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[35m 85%\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Pushing records to Argilla... <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 85%</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "d3410b6bc0a64ef89cf2fff708912ada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winterForestStump/thesis/blob/main/evaluation/argilla_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rJATBBhiyssj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install argilla -U\n",
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argilla as rg\n",
        "\n",
        "rg.init(\n",
        "    api_url=\"https://winterForestStump-thesis.hf.space\",\n",
        "    api_key=\"admin.apikey\",\n",
        "    workspace=\"admin\")"
      ],
      "metadata": {
        "id": "fmu7ZTFIy2Oy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "pwZm4XPbcUcx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding datasets GeneralQA"
      ],
      "metadata": {
        "id": "MYeFhIKFcATw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds=pd.read_csv('https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_3M%20CO.json.csv')\n",
        "data_set = Dataset.from_pandas(ds)\n",
        "data_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IihMLmlBrrqv",
        "outputId": "2b8e35cb-3fb3-4532-e5da-63288b904d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Unnamed: 0', 'question', 'company', 'response', 'context', 'retrieval_grade', 'hallucination_grade', 'answer_grade'],\n",
              "    num_rows: 35\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feedback_dataset = rg.FeedbackDataset(\n",
        "    guidelines=\"Grade the retrieval and generation results of the RAG chain\",\n",
        "    fields=[\n",
        "        rg.TextField(name=\"query\", title=\"User's question (query)\"),\n",
        "        rg.TextField(name=\"retrieved_document\", title=\"Retrieved by the RAG chain documents (context)\"),\n",
        "        rg.TextField(name=\"generated_response\", title=\"Generated LLM response (answer) to the question\"),\n",
        "    ],\n",
        "    questions=[\n",
        "        rg.LabelQuestion(\n",
        "            name=\"relevancy\",\n",
        "            title=\"Are the retrieved documents relevant to the given question?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "        rg.LabelQuestion(\n",
        "            name=\"faithfulness\",\n",
        "            title=\"Is the generated answer grounded in / supported by a context (retrieved documents)?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "        rg.LabelQuestion(\n",
        "            name=\"usefulness\",\n",
        "            title=\"Is generated answer useful to resolve a question?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "    ]\n",
        ")\n",
        "feedback_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqIy52E-qCk8",
        "outputId": "e3d8d229-13b0-47f5-9711-a56624fbbe4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedbackDataset(\n",
              "   fields=[TextField(name='query', title=\"User's question (query)\", required=True, type='text', use_markdown=False), TextField(name='retrieved_document', title='Retrieved by the RAG chain documents (context)', required=True, type='text', use_markdown=False), TextField(name='generated_response', title='Generated LLM response (answer) to the question', required=True, type='text', use_markdown=False)]\n",
              "   questions=[LabelQuestion(name='relevancy', title='Are the retrieved documents relevant to the given question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), LabelQuestion(name='faithfulness', title='Is the generated answer grounded in / supported by a context (retrieved documents)?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), LabelQuestion(name='usefulness', title='Is generated answer useful to resolve a question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None)]\n",
              "   guidelines=Grade the retrieval and generation results of the RAG chain)\n",
              "   metadata_properties=[])\n",
              "   vectors_settings=[])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "for i, item in enumerate(data_set):\n",
        "    records.append(\n",
        "        rg.FeedbackRecord(\n",
        "            fields={\n",
        "                \"query\": item[\"question\"],\n",
        "                \"retrieved_document\": item['context'],\n",
        "                \"generated_response\": item[\"response\"],\n",
        "            },\n",
        "            external_id=f\"record-{i}\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "feedback_dataset.add_records(records)"
      ],
      "metadata": {
        "id": "xVQf0RYyiCSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remote_dataset = feedback_dataset.push_to_argilla(name=\"my-dataset\")"
      ],
      "metadata": {
        "id": "SiVuZxdBgEg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding records to the dataset General QA"
      ],
      "metadata": {
        "id": "VSFXUJnTiaoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = rg.FeedbackDataset.from_argilla(name=\"my-dataset\", workspace=\"admin\")"
      ],
      "metadata": {
        "id": "B0Sk6s3xhzRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "         \"https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_COCA%20COLA%20CO.json.csv\",\n",
        "         \"https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_AMAZON%20COM%20INC.json.csv\",\n",
        "         \"https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_JPMORGAN%20CHASE%20%26%20CO.json.csv\",\n",
        "         \"https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_LOCKHEED%20MARTIN%20CORP.json.csv\",\n",
        "         \"https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_MICROSOFT%20CORP.json.csv\",\n",
        "         \"https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_NIKE%2C%20Inc..json.csv\",\n",
        "         \"https://github.com/winterForestStump/thesis/blob/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_PayPal%20Holdings%2C%20Inc..json.csv\",\n",
        "         \"https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_VERIZON%20COMMUNICATIONS%20INC.json.csv\",\n",
        "         \"https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_Walmart%20Inc..json.csv\"\n",
        "         ]"
      ],
      "metadata": {
        "id": "KSPRm4tch2az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "length = 35\n",
        "for file in files:\n",
        "    try:\n",
        "        # Attempt to read the CSV file\n",
        "        ds = pd.read_csv(file)\n",
        "        data_set = Dataset.from_pandas(ds)\n",
        "\n",
        "        # Process each record in the dataset\n",
        "        for i, item in enumerate(data_set):\n",
        "            records.append(rg.FeedbackRecord(fields={\"query\": item[\"question\"],\n",
        "                                                     \"retrieved_document\": item['context'],\n",
        "                                                     \"generated_response\": item[\"response\"]\n",
        "                                                     },\n",
        "                                            ))\n",
        "        length += len(data_set)\n",
        "\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(f\"Error parsing {file}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred with {file}: {e}\")\n",
        "\n",
        "\n",
        "# Output the results\n",
        "print(f\"Processed {len(records)} records.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wmZsk33ihnJ",
        "outputId": "f7415ba4-13be-462a-8efe-e9551271124e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error parsing https://github.com/winterForestStump/thesis/blob/main/evaluation/bge-reranker_x_phi3-4k/csv/eval_PayPal%20Holdings%2C%20Inc..json.csv: Error tokenizing data. C error: Expected 1 fields in line 40, saw 20\n",
            "\n",
            "Processed 280 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M65kj88uUSY",
        "outputId": "3356fa15-72f1-4863-8aa1-464edbe9eb9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "315"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.add_records(records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33,
          "referenced_widgets": [
            "543e6a78a566481d8d9a2c0db729bf23",
            "5f92e0be9e174d4c8047c20123d47e1f"
          ]
        },
        "id": "eZIuNahvoNJn",
        "outputId": "131f4a13-da4b-4e77-827d-f412e874f658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "543e6a78a566481d8d9a2c0db729bf23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect responses General QA"
      ],
      "metadata": {
        "id": "Ne5qJzESKj9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = rg.FeedbackDataset.from_argilla(\"my-dataset\", workspace=\"admin\")"
      ],
      "metadata": {
        "id": "X2fTBKKLcpcg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_values = []\n",
        "for record_ix,record in enumerate(feedback):\n",
        "  list_values.append({\"record\": str(record_ix+1),\n",
        "                      \"id\": feedback.records[record_ix].id,\n",
        "                      \"query\": feedback.records[record_ix].fields['query'],\n",
        "                      \"retrieved_document\": feedback.records[record_ix].fields['retrieved_document'],\n",
        "                      \"generated_response\": feedback.records[record_ix].fields['generated_response'],\n",
        "                      \"relevancy_value\": feedback.records[record_ix].responses[0].values[\"relevancy\"].value,\n",
        "                      \"faithfulness_value\": feedback.records[record_ix].responses[0].values[\"faithfulness\"].value,\n",
        "                      \"usefulness_value\": feedback.records[record_ix].responses[0].values[\"usefulness\"].value\n",
        "                      })"
      ],
      "metadata": {
        "id": "dPUsUeQq-Fxp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D92akUH6ShfI",
        "outputId": "9841cc0f-a9dd-45c2-95e2-59d87fffdfef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "347"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list_values)\n",
        "df.to_csv('eval_resuts_argilla_generalQA.csv')"
      ],
      "metadata": {
        "id": "OCyLoomFPBQD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating dataset Financebench150"
      ],
      "metadata": {
        "id": "31Of35ZVnqnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds=pd.read_csv('https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/financebench150/csv/eval_v2.json.csv')\n",
        "data_set = Dataset.from_pandas(ds)\n",
        "data_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVdwZvAfnrTM",
        "outputId": "615b966c-4ead-43ac-e3b4-f564640416f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Unnamed: 0', 'question', 'response', 'context', 'hallucination_grade', 'answer_grade', 'ground_truth', 'evidence'],\n",
              "    num_rows: 150\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feedback_dataset = rg.FeedbackDataset(\n",
        "    guidelines=\"Grade the retrieval and generation results of the RAG chain\",\n",
        "    fields=[\n",
        "        rg.TextField(name=\"query\", title=\"User's question (query)\"),\n",
        "        rg.TextField(name=\"retrieved_document\", title=\"Retrieved by the RAG chain documents (context)\"),\n",
        "        rg.TextField(name=\"generated_response\", title=\"Generated LLM response (answer) to the question\"),\n",
        "        rg.TextField(name=\"ground_truth\", title=\"Ground Truth (Correct Answer)\"),\n",
        "        rg.TextField(name=\"evidence\", title=\"Evidence context for the ground truth answer\")\n",
        "    ],\n",
        "    questions=[\n",
        "        rg.LabelQuestion(\n",
        "            name=\"relevancy\",\n",
        "            title=\"Are the retrieved documents relevant to the given question?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "        rg.LabelQuestion(\n",
        "            name=\"faithfulness\",\n",
        "            title=\"Is the generated answer grounded in / supported by a context (retrieved documents)?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "        rg.LabelQuestion(\n",
        "            name=\"usefulness\",\n",
        "            title=\"Is generated answer useful to resolve a question?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "        rg.LabelQuestion(\n",
        "            name=\"evidence\",\n",
        "            title=\"Are the retrieved documents relevant to the provided evidence context?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "        rg.LabelQuestion(\n",
        "            name=\"correctness\",\n",
        "            title=\"Does the generated answer match the ground truth answer?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "    ]\n",
        ")\n",
        "feedback_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQT__SSGn8QE",
        "outputId": "a3cf3ef8-65fd-4d3a-f19a-a2ade1d98629"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedbackDataset(\n",
              "   fields=[TextField(name='query', title=\"User's question (query)\", required=True, type='text', use_markdown=False), TextField(name='retrieved_document', title='Retrieved by the RAG chain documents (context)', required=True, type='text', use_markdown=False), TextField(name='generated_response', title='Generated LLM response (answer) to the question', required=True, type='text', use_markdown=False), TextField(name='ground_truth', title='Ground Truth (Correct Answer)', required=True, type='text', use_markdown=False), TextField(name='evidence', title='Evidence context for the ground truth answer', required=True, type='text', use_markdown=False)]\n",
              "   questions=[LabelQuestion(name='relevancy', title='Are the retrieved documents relevant to the given question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), LabelQuestion(name='faithfulness', title='Is the generated answer grounded in / supported by a context (retrieved documents)?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), LabelQuestion(name='usefulness', title='Is generated answer useful to resolve a question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), LabelQuestion(name='evidence', title='Are the retrieved documents relevant to the provided evidence context?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), LabelQuestion(name='correctness', title='Does the generated answer match the ground truth answer?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None)]\n",
              "   guidelines=Grade the retrieval and generation results of the RAG chain)\n",
              "   metadata_properties=[])\n",
              "   vectors_settings=[])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "for i, item in enumerate(data_set):\n",
        "    records.append(\n",
        "        rg.FeedbackRecord(\n",
        "            fields={\n",
        "                \"query\": item[\"question\"],\n",
        "                \"retrieved_document\": item['context'],\n",
        "                \"generated_response\": item[\"response\"],\n",
        "                \"ground_truth\": item[\"ground_truth\"],\n",
        "                \"evidence\": item[\"evidence\"]\n",
        "            },\n",
        "            external_id=f\"record-{i}\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "feedback_dataset.add_records(records)"
      ],
      "metadata": {
        "id": "N5VQeSiRn_Qj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remote_dataset = feedback_dataset.push_to_argilla(name=\"financebench150\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "52744fd2db4548bf8ad2c19e94ec3397",
            "e3089230d5fa4d3c8d221ede8de9f3d2"
          ]
        },
        "id": "4_iPqoeCoBJE",
        "outputId": "30996035-1ea2-479b-a5ff-d78f56e18ff2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52744fd2db4548bf8ad2c19e94ec3397"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:argilla.client.feedback.dataset.local.mixins:✓ Dataset succesfully pushed to Argilla\n",
            "INFO:argilla.client.feedback.dataset.local.mixins:RemoteFeedbackDataset(\n",
            "   id=75b8c025-0fde-4b0a-b522-728c3319b631\n",
            "   name=financebench150\n",
            "   workspace=Workspace(id=2afdc5dd-7ac5-49bb-99c9-73816b3e107a, name=admin, inserted_at=2024-06-22 19:41:07.791321, updated_at=2024-06-22 19:41:07.791321)\n",
            "   url=https://winterforeststump-thesis.hf.space/dataset/75b8c025-0fde-4b0a-b522-728c3319b631/annotation-mode\n",
            "   fields=[RemoteTextField(id=UUID('ea0bbadf-3ac4-4b75-b2f4-61c05e2678e1'), client=None, name='query', title=\"User's question (query)\", required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('38dee56f-8bc3-47ba-9716-d0865bc666c4'), client=None, name='retrieved_document', title='Retrieved by the RAG chain documents (context)', required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('1640515a-6cd3-499e-90dc-836718b11cc2'), client=None, name='generated_response', title='Generated LLM response (answer) to the question', required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('dcb57ffc-9c50-4422-824b-f80ba7f11408'), client=None, name='ground_truth', title='Ground Truth (Correct Answer)', required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('fe07d2e3-22f4-41a0-b9a9-bc8e9f006660'), client=None, name='evidence', title='Evidence context for the ground truth answer', required=True, type='text', use_markdown=False)]\n",
            "   questions=[RemoteLabelQuestion(id=UUID('0a3cf090-83fe-4dd8-9078-028648ca8938'), client=None, name='relevancy', title='Are the retrieved documents relevant to the given question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), RemoteLabelQuestion(id=UUID('8ec36cc3-3107-4caa-aea5-1f8e4c7cffb7'), client=None, name='faithfulness', title='Is the generated answer grounded in / supported by a context (retrieved documents)?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), RemoteLabelQuestion(id=UUID('deab759c-2229-49f6-b90e-4eeb8e9ed9e5'), client=None, name='usefulness', title='Is generated answer useful to resolve a question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), RemoteLabelQuestion(id=UUID('407be2ba-d919-4ca4-b9dc-c286b285c41e'), client=None, name='evidence', title='Are the retrieved documents relevant to the provided evidence context?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), RemoteLabelQuestion(id=UUID('b00dadac-7cd3-4bff-9cc2-e9c2a9dde43b'), client=None, name='correctness', title='Does the generated answer match the ground truth answer?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None)]\n",
            "   guidelines=Grade the retrieval and generation results of the RAG chain\n",
            "   metadata_properties=[]\n",
            "   vectors_settings=[]\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect responses Financebench150"
      ],
      "metadata": {
        "id": "V8YBLprKqPm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = rg.FeedbackDataset.from_argilla(\"financebench150\", workspace=\"admin\")"
      ],
      "metadata": {
        "id": "buQNKo6yqWgu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_values = []\n",
        "for record_ix,record in enumerate(feedback):\n",
        "  list_values.append({\"record\": str(record_ix+1),\n",
        "                      \"id\": feedback.records[record_ix].id,\n",
        "                      \"query\": feedback.records[record_ix].fields['query'],\n",
        "                      \"retrieved_document\": feedback.records[record_ix].fields['retrieved_document'],\n",
        "                      \"generated_response\": feedback.records[record_ix].fields['generated_response'],\n",
        "                      \"relevancy_value\": feedback.records[record_ix].responses[0].values[\"relevancy\"].value,\n",
        "                      \"faithfulness_value\": feedback.records[record_ix].responses[0].values[\"faithfulness\"].value,\n",
        "                      \"usefulness_value\": feedback.records[record_ix].responses[0].values[\"usefulness\"].value,\n",
        "                      \"evidence_value\": feedback.records[record_ix].responses[0].values[\"evidence\"].value,\n",
        "                      \"correctness_value\": feedback.records[record_ix].responses[0].values[\"correctness\"].value\n",
        "                      })"
      ],
      "metadata": {
        "id": "Wgms-fcFqbId"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZVkshmDqdIF",
        "outputId": "5fec5bc5-eed6-474e-d031-a4868a75d46d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list_values)\n",
        "df.to_csv('eval_resuts_argilla_financebench150.csv')"
      ],
      "metadata": {
        "id": "rNaLLU4NqfBV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding datasets NoRag RegQA"
      ],
      "metadata": {
        "id": "GPaRizshaZx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds=pd.read_csv('https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/phi-3_x_no_rag/csv/eval_no_rag.json.csv')\n",
        "data_set = Dataset.from_pandas(ds)\n",
        "data_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eelc8kz-aY-P",
        "outputId": "198d29f1-cc8e-455c-885f-9d799014a15a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Unnamed: 0', 'question', 'company', 'response'],\n",
              "    num_rows: 350\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feedback_dataset = rg.FeedbackDataset(\n",
        "    guidelines=\"Grade the generated answers\",\n",
        "    fields=[\n",
        "        rg.TextField(name=\"query\", title=\"User's question (query)\"),\n",
        "        rg.TextField(name='company_name', title='The name of the company'),\n",
        "        rg.TextField(name=\"generated_response\", title=\"Generated LLM response (answer) to the question\"),\n",
        "    ],\n",
        "    questions=[\n",
        "        rg.LabelQuestion(\n",
        "            name=\"faithfulness\",\n",
        "            title=\"Does the generated answer contain truthful information?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "        rg.LabelQuestion(\n",
        "            name=\"usefulness\",\n",
        "            title=\"Is the generated answer useful to resolve a question?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "    ]\n",
        ")\n",
        "feedback_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22t1J1ojai5C",
        "outputId": "ae688cc6-22df-4ab2-870d-082fd2090717"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedbackDataset(\n",
              "   fields=[TextField(name='query', title=\"User's question (query)\", required=True, type='text', use_markdown=False), TextField(name='company_name', title='The name of the company', required=True, type='text', use_markdown=False), TextField(name='generated_response', title='Generated LLM response (answer) to the question', required=True, type='text', use_markdown=False)]\n",
              "   questions=[LabelQuestion(name='faithfulness', title='Does the generated answer contain truthful information?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), LabelQuestion(name='usefulness', title='Is the generated answer useful to resolve a question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None)]\n",
              "   guidelines=Grade the generated answers)\n",
              "   metadata_properties=[])\n",
              "   vectors_settings=[])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "for i, item in enumerate(data_set):\n",
        "    records.append(\n",
        "        rg.FeedbackRecord(\n",
        "            fields={\n",
        "                \"query\": item[\"question\"],\n",
        "                \"company_name\": item[\"company\"],\n",
        "                \"generated_response\": item[\"response\"],\n",
        "            },\n",
        "            external_id=f\"record-{i}\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "feedback_dataset.add_records(records)"
      ],
      "metadata": {
        "id": "AL9o5AKJak-h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remote_dataset = feedback_dataset.push_to_argilla(name=\"noRag-generalQA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "d135e80d9bd847578452bef9e4b67177",
            "bd8f98908bda49a79a2c20b9eba01456"
          ]
        },
        "id": "j555ydO8amrD",
        "outputId": "94f17bdf-c3bf-495b-c640-01835c2e52c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d135e80d9bd847578452bef9e4b67177"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:argilla.client.feedback.dataset.local.mixins:✓ Dataset succesfully pushed to Argilla\n",
            "INFO:argilla.client.feedback.dataset.local.mixins:RemoteFeedbackDataset(\n",
            "   id=b70e2bd8-d2b6-40d0-9b9c-a24eea6bffd3\n",
            "   name=noRag-generalQA\n",
            "   workspace=Workspace(id=142a9c2d-f1dc-4704-8188-daf87f726164, name=admin, inserted_at=2024-07-03 09:01:38.246747, updated_at=2024-07-03 09:01:38.246747)\n",
            "   url=https://winterforeststump-thesis.hf.space/dataset/b70e2bd8-d2b6-40d0-9b9c-a24eea6bffd3/annotation-mode\n",
            "   fields=[RemoteTextField(id=UUID('a5cd7df7-418a-4b2f-b117-44b098af1603'), client=None, name='query', title=\"User's question (query)\", required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('d3704a8d-dad8-45ea-80f0-b205c1e7146c'), client=None, name='company_name', title='The name of the company', required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('6131943f-9174-4b8b-adde-cee1f9ae9ebc'), client=None, name='generated_response', title='Generated LLM response (answer) to the question', required=True, type='text', use_markdown=False)]\n",
            "   questions=[RemoteLabelQuestion(id=UUID('d2f62fb0-9e38-433c-9134-e843ffe884ff'), client=None, name='faithfulness', title='Does the generated answer contain truthful information?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), RemoteLabelQuestion(id=UUID('c55a589f-6be1-4412-95e9-2071c0ebc49c'), client=None, name='usefulness', title='Is the generated answer useful to resolve a question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None)]\n",
            "   guidelines=Grade the generated answers\n",
            "   metadata_properties=[]\n",
            "   vectors_settings=[]\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect responses NO RAG Generaal QA"
      ],
      "metadata": {
        "id": "VLqtjLnHoSM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = rg.FeedbackDataset.from_argilla(\"noRag-generalQA\", workspace=\"admin\")"
      ],
      "metadata": {
        "id": "0dez0ie6n-ZO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_values = []\n",
        "for record_ix,record in enumerate(feedback):\n",
        "  list_values.append({\"record\": str(record_ix+1),\n",
        "                      \"id\": feedback.records[record_ix].id,\n",
        "                      \"query\": feedback.records[record_ix].fields['query'],\n",
        "                      \"company_name\": feedback.records[record_ix].fields['company_name'],\n",
        "                      \"generated_response\": feedback.records[record_ix].fields['generated_response'],\n",
        "                      \"faithfulness_value\": feedback.records[record_ix].responses[0].values[\"faithfulness\"].value,\n",
        "                      \"usefulness_value\": feedback.records[record_ix].responses[0].values[\"usefulness\"].value,\n",
        "                      })"
      ],
      "metadata": {
        "id": "PnctJPhuoDA2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxnOEWHEoYvr",
        "outputId": "fa58f32a-f342-4aff-8090-c802b2d6cd49"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "350"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list_values)\n",
        "df.to_csv('eval_resuts_argilla_NoRagGeneralQA.csv')"
      ],
      "metadata": {
        "id": "owB_igofoalm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding datasets NoRag FinBench150"
      ],
      "metadata": {
        "id": "ZyZJJIGujF6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds=pd.read_csv('https://raw.githubusercontent.com/winterForestStump/thesis/main/evaluation/phi-3_x_no_rag/csv/financebench_eval_no_rag.json.csv')\n",
        "data_set = Dataset.from_pandas(ds)\n",
        "data_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7xNMAjcjMDS",
        "outputId": "1a2e4462-a6bf-4dc5-b3c0-6091084c664d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['Unnamed: 0', 'question', 'response', 'correct_answer'],\n",
              "    num_rows: 150\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feedback_dataset = rg.FeedbackDataset(\n",
        "    guidelines=\"Grade the retrieval and generation results of the RAG chain\",\n",
        "    fields=[\n",
        "        rg.TextField(name=\"query\", title=\"User's question (query)\"),\n",
        "        rg.TextField(name=\"generated_response\", title=\"Generated LLM response (answer) to the question\"),\n",
        "        rg.TextField(name=\"ground_truth\", title=\"Ground Truth (Correct Answer)\"),\n",
        "    ],\n",
        "    questions=[\n",
        "        rg.LabelQuestion(\n",
        "            name=\"correctness\",\n",
        "            title=\"Does the generated answer match the ground truth answer?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "        rg.LabelQuestion(\n",
        "            name=\"usefulness\",\n",
        "            title=\"Is generated answer useful to resolve a question?\",\n",
        "            labels=[\"YES\", \"NO\", \"UNSURE\"],\n",
        "            required=True,\n",
        "            visible_labels=None\n",
        "            ),\n",
        "    ]\n",
        ")\n",
        "feedback_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jytkpq97jVLc",
        "outputId": "49e34375-c7b7-46fa-e44e-8f21e7b65d67"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeedbackDataset(\n",
              "   fields=[TextField(name='query', title=\"User's question (query)\", required=True, type='text', use_markdown=False), TextField(name='generated_response', title='Generated LLM response (answer) to the question', required=True, type='text', use_markdown=False), TextField(name='ground_truth', title='Ground Truth (Correct Answer)', required=True, type='text', use_markdown=False)]\n",
              "   questions=[LabelQuestion(name='correctness', title='Does the generated answer match the ground truth answer?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), LabelQuestion(name='usefulness', title='Is generated answer useful to resolve a question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None)]\n",
              "   guidelines=Grade the retrieval and generation results of the RAG chain)\n",
              "   metadata_properties=[])\n",
              "   vectors_settings=[])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "for i, item in enumerate(data_set):\n",
        "    records.append(\n",
        "        rg.FeedbackRecord(\n",
        "            fields={\n",
        "                \"query\": item[\"question\"],\n",
        "                \"generated_response\": item[\"response\"],\n",
        "                \"ground_truth\": item[\"correct_answer\"],\n",
        "            },\n",
        "            external_id=f\"record-{i}\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "feedback_dataset.add_records(records)"
      ],
      "metadata": {
        "id": "cayuI1_wjpk4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remote_dataset = feedback_dataset.push_to_argilla(name=\"noRag-financebench150\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "428ae5f77847477eb3ff26ea9cc105c8",
            "d3410b6bc0a64ef89cf2fff708912ada"
          ]
        },
        "id": "4vig0kVJjueX",
        "outputId": "0af2b5f8-56e1-455b-c1da-2d7c337b27af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "428ae5f77847477eb3ff26ea9cc105c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:argilla.client.feedback.dataset.local.mixins:✓ Dataset succesfully pushed to Argilla\n",
            "INFO:argilla.client.feedback.dataset.local.mixins:RemoteFeedbackDataset(\n",
            "   id=4c74d7f6-2b4a-4303-86c9-e4bc1ef2f957\n",
            "   name=noRag-financebench150\n",
            "   workspace=Workspace(id=142a9c2d-f1dc-4704-8188-daf87f726164, name=admin, inserted_at=2024-07-03 09:01:38.246747, updated_at=2024-07-03 09:01:38.246747)\n",
            "   url=https://winterforeststump-thesis.hf.space/dataset/4c74d7f6-2b4a-4303-86c9-e4bc1ef2f957/annotation-mode\n",
            "   fields=[RemoteTextField(id=UUID('bba49e30-a1ab-4890-9031-5fd1059fe965'), client=None, name='query', title=\"User's question (query)\", required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('f82090d3-01b4-488e-bdb5-4d218de310d1'), client=None, name='generated_response', title='Generated LLM response (answer) to the question', required=True, type='text', use_markdown=False), RemoteTextField(id=UUID('c65e7fdd-4b36-4972-b118-e1350936813d'), client=None, name='ground_truth', title='Ground Truth (Correct Answer)', required=True, type='text', use_markdown=False)]\n",
            "   questions=[RemoteLabelQuestion(id=UUID('13cdb66c-d79a-4f50-87f5-1e10d3b2c425'), client=None, name='correctness', title='Does the generated answer match the ground truth answer?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None), RemoteLabelQuestion(id=UUID('d497c79a-639d-4f15-8ae6-48a487e74fd4'), client=None, name='usefulness', title='Is generated answer useful to resolve a question?', description=None, required=True, type='label_selection', labels=['YES', 'NO', 'UNSURE'], visible_labels=None)]\n",
            "   guidelines=Grade the retrieval and generation results of the RAG chain\n",
            "   metadata_properties=[]\n",
            "   vectors_settings=[]\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collect responses NO RAG Generaal QA"
      ],
      "metadata": {
        "id": "zoCdd4LOkO76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = rg.FeedbackDataset.from_argilla(\"noRag-financebench150\", workspace=\"admin\")"
      ],
      "metadata": {
        "id": "_ezyMKPLkRuL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_values = []\n",
        "for record_ix,record in enumerate(feedback):\n",
        "  list_values.append({\"record\": str(record_ix+1),\n",
        "                      \"id\": feedback.records[record_ix].id,\n",
        "                      \"query\": feedback.records[record_ix].fields['query'],\n",
        "                      \"generated_response\": feedback.records[record_ix].fields['generated_response'],\n",
        "                      \"usefulness_value\": feedback.records[record_ix].responses[0].values[\"usefulness\"].value,\n",
        "                      \"correctness_value\": feedback.records[record_ix].responses[0].values[\"correctness\"].value\n",
        "                      })"
      ],
      "metadata": {
        "id": "u9rKZZDOkWEk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6lt5zL8k5iR",
        "outputId": "9bdf6787-f2ce-4ad6-94b3-f8e72bbe3a43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list_values)\n",
        "df.to_csv('eval_resuts_argilla_NoRagFinanceBench150.csv')"
      ],
      "metadata": {
        "id": "uH1Gcv0Uk7wi"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}