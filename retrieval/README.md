LangChain's **RetrievalQA** chain has 4 different strategies, which can be used to retrieve documents:
* *Stuff*. Takes multiple small documents and combines them into a single prompt for the LLM. It is cost-efficient because it involves only one request to the LLM. The chain type stuff is the simplest method. It uses all related text from the documents as the context in the prompt to the LLM. **Pros**: You only make a single call to the LLM. And when it generates text, it has access to all the data simultaneously. Itâ€™s cost-efficient since you only make one call to the LLM. **Cons**: This can sometimes exceed the token limit for a LLM.
* *Refine*. Looks at each document individually and updates its answer with each new document. It is useful when there are too many documents, but it can be slow and confusing if they reference each other. Refine also separates texts into batches, but it feeds the first batch to LLM, the answer, and the second to LLM. It refines the answer by going through all the batches.
* *Map reduce*. Consists of a map step, where each document is individually summarized, and a reduce step where these mini-summaries are combined. An optional compression step can be added. This method runs an initial prompt on each chunk of data and then uses a different prompt to combine all the initial outputs. Map_reduce separates texts into batches, where you can define the batch size. It feeds each batch with the question to LLM separately and comes up with the final answer based on the answers from each batch. **Pros**: It can scale to more documents and documents of larger length. Since the calls to the LLM are on independent, individual documents they can be parallelized. **Cons**: This requires more calls to the LLM. You can also get some information during the final combined call.
* *Re-rank*. Tries to get an answer for each document and assigns it a confidence score, picking the highest confidence answer in the end. Rerank separates texts into batches, feeds each batch to LLM, returns a score of how fully it answers the question, and comes up with the final answer based on the high-scored answers from each batch.

Source: https://www.comet.com/site/blog/evaluating-rag-pipelines-with-ragas/