\section{Literature Review}
Retrieval-Augmented Generation (RAG) is a technique designed to enhance the performance of Large-Language Models (LLMs) by integrating external knowledge. This method has been extensively researched, exploring various configurations and applications. Key studies include a detailed analysis of RAG configurations \cite{Gao.18Dec2023}, highlighting their role in improving Natural Language Processing (NLP) tasks by reducing errors and enhancing factual accuracy. Several methods for context retrieval have been proposed to dynamically fetch documents, thereby improving the coherence of generated outputs \cite{Anantha.10Dec2023}. Other research has introduced advancements in RAG, such as reasoning chain storage and optimization strategies for retrieval, which broaden the scope and efficiency of RAG applications in LLMs \cite{Lewis.22May2020}. More recent work has compared RAG with LLM fine-tuning, finding that using both methods together improves their individual performances \cite{Balaguer.16Jan2024}.

Pre-trained LLMs store factual knowledge and achieve state-of-the-art results on NLP tasks but struggle with knowledge-intensive tasks and providing decision provenance. Combining parametric and non-parametric memory with generation improves knowledge-intensive task performance\cite{Lewis.22May2020}. The EDGAR-CORPUS dataset and edgar-crawler provide a financial NLP corpus excluding tables \cite{Loukas.2021}. Instruction Tuning explores task-specific tuning of LLMs in finance, highlighting diverse capabilities and future strategies for improvement \cite{Wang.31Dec2023}.

Recent frameworks like Self-Reflective Retrieval-Augmented Generation (SELF-RAG) enhance LLM quality and factuality through retrieval and self-reflection \cite{Asai.17Oct2023}. ITER-RETGEN synergizes retrieval and generation iteratively, improving relevance modeling for question answering tasks \cite{Shao.24May2023}.

In the financial domain, NLP applications include sentiment analysis, question answering, and stock market prediction. Advances in general-domain LLMs have motivated interest in Financial LLMs (FinLLMs), employing methods like mixed-domain LLMs with prompt engineering and instruction fine-tuned LLMs \cite{Lee.4Feb2024}.

Exploring the structure of financial reports is essential for establishing optimal principles for chunking. The complexity of document structures and content has led most research on financial reports to focus on identifying structural elements \cite{Yepes.5Feb2024}. Recent work includes DocLayNet \cite{Pfitzmann.2022}, which addresses document structures in U.S. SEC reports, and FinTabNet \cite{Zheng.}, which specifically focuses on table structures within these reports.

Most previous work has focused on understanding the layout of financial documents or specific snippets of reports. Recent research includes FinanceBench \cite{Islam.20Nov2023}, which proposes questions about financial report content and evidence snippets.

Corporate annual reports, especially SEC Form 10-K filings, are crucial for financial decision-making but are challenging due to their length and complexity. Machine Learning and Deep Learning techniques are being used to process these reports, improving accessibility for individual investors.

Financial domain-specific LMs are often built using general-domain LMs. Notable financial pre-trained LMs include FinBERT19 \cite{Araci.27Aug2019}, FinBERT-20 \cite{Shah.31Oct2022}, and FinBERT-21 \cite{ZhuangLiuDegenHuangKaiyuHuangZhuangLiJunZhao.2020}, all based on the BERT architecture. FinBERT-19, the first FinBERT model, was released for financial sentiment analysis and involved continual pre-training on a financial domain corpus after initial training on a general-domain BERT model. FinBERT-20 is a finance-specific BERT model pre-trained on a larger financial communication corpus. FinBERT-21, also BERT-based, was trained on both general and financial domain corpora and experimented with tasks such as Sentiment Analysis, Sentence Boundary Detection, and Question Answering \cite{Lee.4Feb2024}.

FLANG \cite{Shah.31Oct2022} is a domain-specific model trained using financial keywords and phrases for masking, following the training strategy of ELECTRA \cite{Clark.23Mar2020}. It introduces Financial Language Understanding Evaluation (FLUE), a collection of five financial NLP benchmark tasks: Sentiment Analysis, Headline Text Classification, Named Entity Recognition, Structure Boundary Detection, and Question Answering.

FinMA or PIXIU \cite{Xie.8Jun2023} consists of two fine-tuned LLaMA models with 7B and 30B parameters \cite{Touvron.27Feb2023}, using financial instruction datasets for various financial tasks. It is based on the Financial Instruction Tuning (FIT) dataset and includes tasks beyond FLUE, such as Stock Movement Prediction.

InvestLM \cite{Yang.15Sep2023} is a fine-tuned LLaMA model with 65B parameters, trained using a curated financial domain instruction dataset that includes Chartered Financial Analyst (CFA) exam questions, SEC filings, and financial NLP tasks. It extends beyond tasks covered by FinMA to include financial text summarization.

FinGPT \cite{Yang.9Jun2023} is an open-source, data-centric framework providing APIs for financial data sources, instruction datasets for financial tasks, and fine-tuned financial LLMs. It utilizes the Low-Rank Adaptation (LoRA) method for instruction fine-tuning.

BloombergGPT \cite{Wu.30Mar2023} is a closed-source FinLLM based on the BLOOM model \cite{Workshop.9Nov2022}, trained on both general and financial corpora, with financial data sourced from Bloombergâ€™s proprietary database. It covers both financial and general-purpose NLP tasks.

Recent advances in LLMs have opened new possibilities in finance. The paper 'Large Language Models in Finance: A Survey' \cite{Li.28Sep2023} reviews current approaches, including zero-shot and few-shot learning, domain-specific fine-tuning, and custom LLM training. It provides a decision framework for financial professionals to select appropriate LLM solutions based on their use case constraints.

Question Answering (QA) in finance involves retrieving or generating answers to questions from unstructured documents, requiring numerical reasoning across multiple formats. FiQA-QA \cite{Yang.23Aug2018} is an early Financial QA dataset for opinion-based QA. FinQA \cite{Chen.1Sep2021} and ConvFinQA \cite{Chen.8Oct2022} datasets involve hybrid QA, combining tabular and textual content for financial reasoning tasks.

Using 10-K forms, \cite{Ge.26Jan2023} measures regulatory barriers globally by fine-tuning BERT. \cite{Pasch.4Feb2022} fine-tuned BERT on various company-related text data for stock price prediction, resulting in StonkBERT, a transformer-based stock performance classifier. Analysts also use BERT for causal information classification or GPT-3.5 for creating quantitative datasets. Summarization and RAG improve accessibility for individual investors.

Major challenges in finance include utilizing internal data without privacy breaches and enhancing trust in FinLLMs' responses. Techniques like RAG \cite{Lewis.22May2020} help by providing LLMs with access to external knowledge, improving factual accuracy and reducing hallucination issues. RAG allows the use of internal data without retraining the entire model, ensuring privacy.