{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a560469e13744bfb855def1d3f169cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b5a62797c5448c28114e8895d86a49f",
              "IPY_MODEL_67723769e2ad4d03b05db94a72b54997",
              "IPY_MODEL_ab1a229329234ec8b1a38db7a5c63538"
            ],
            "layout": "IPY_MODEL_d3707b13a9ec4be29fc558e4642f0b92"
          }
        },
        "6b5a62797c5448c28114e8895d86a49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4caaefe8fb3845dc8d435f3a0c9ae9ba",
            "placeholder": "​",
            "style": "IPY_MODEL_8e67064bb9f5441699c6f86833cb2cec",
            "value": "modules.json: 100%"
          }
        },
        "67723769e2ad4d03b05db94a72b54997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed45d04cf29e4e1c999bef6a23a7c6e1",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_340fd53202ab458faf71b44b159b2980",
            "value": 349
          }
        },
        "ab1a229329234ec8b1a38db7a5c63538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aefd1848e577430d83cf68eaa7610681",
            "placeholder": "​",
            "style": "IPY_MODEL_b5cbb949e14a44ddb0ffb8b999f2f5da",
            "value": " 349/349 [00:00&lt;00:00, 23.8kB/s]"
          }
        },
        "d3707b13a9ec4be29fc558e4642f0b92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4caaefe8fb3845dc8d435f3a0c9ae9ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e67064bb9f5441699c6f86833cb2cec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed45d04cf29e4e1c999bef6a23a7c6e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "340fd53202ab458faf71b44b159b2980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aefd1848e577430d83cf68eaa7610681": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5cbb949e14a44ddb0ffb8b999f2f5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b81a17704ed4b3bbfa2f7e2b6f9965f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_900a13d1341d4209841bb78274ba1e3e",
              "IPY_MODEL_c0c266362e7549f7bf70a85e88dd8d3e",
              "IPY_MODEL_77b81fd7428243e5be6e31249deae73b"
            ],
            "layout": "IPY_MODEL_567124d02f6f4bd283141ab3b44a2dbf"
          }
        },
        "900a13d1341d4209841bb78274ba1e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ce66a04b55c407b9f50269c5328c0b3",
            "placeholder": "​",
            "style": "IPY_MODEL_59d676957ce04ab1b90c862329ab60a0",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "c0c266362e7549f7bf70a85e88dd8d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed370dd6e86a46c2b5ad1f8e6cf42913",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5be3eaa1edff4fa68eb710c699c2077e",
            "value": 124
          }
        },
        "77b81fd7428243e5be6e31249deae73b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0284e73f56b44280966849a08fce3ce1",
            "placeholder": "​",
            "style": "IPY_MODEL_9181e5682a1e4d54bbb9a01969e3b273",
            "value": " 124/124 [00:00&lt;00:00, 7.81kB/s]"
          }
        },
        "567124d02f6f4bd283141ab3b44a2dbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ce66a04b55c407b9f50269c5328c0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d676957ce04ab1b90c862329ab60a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed370dd6e86a46c2b5ad1f8e6cf42913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5be3eaa1edff4fa68eb710c699c2077e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0284e73f56b44280966849a08fce3ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9181e5682a1e4d54bbb9a01969e3b273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa6cb7abc1fe4d83b30711d68807eef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91dbb46e831a4f4a8bfe9bcd49164521",
              "IPY_MODEL_b46a6cace53f4c168a25ace536a9c6b7",
              "IPY_MODEL_3d59caf78f43405f8a28cb9b8987e405"
            ],
            "layout": "IPY_MODEL_5a206d9f216d451d8d8ad1970e9e3931"
          }
        },
        "91dbb46e831a4f4a8bfe9bcd49164521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cef32a9c3d64d388bad5b02390d4f45",
            "placeholder": "​",
            "style": "IPY_MODEL_ccd0558687984b97a2170af96eb7e45c",
            "value": "README.md: 100%"
          }
        },
        "b46a6cace53f4c168a25ace536a9c6b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_351f34f268814d43b277ea53e1d156ef",
            "max": 94783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5a013edb3994473b2007a17915422cb",
            "value": 94783
          }
        },
        "3d59caf78f43405f8a28cb9b8987e405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c928f09b165496b8a9486e2afe6e383",
            "placeholder": "​",
            "style": "IPY_MODEL_2d78edf6865d451b90d085b4bfd8bc67",
            "value": " 94.8k/94.8k [00:00&lt;00:00, 1.11MB/s]"
          }
        },
        "5a206d9f216d451d8d8ad1970e9e3931": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cef32a9c3d64d388bad5b02390d4f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccd0558687984b97a2170af96eb7e45c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "351f34f268814d43b277ea53e1d156ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5a013edb3994473b2007a17915422cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c928f09b165496b8a9486e2afe6e383": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d78edf6865d451b90d085b4bfd8bc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e54331fc50a7445faade1de1a3a2190c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_388d1a1e4ed14660bb714383a78ce1fb",
              "IPY_MODEL_0cc036b5a6444877abe22c2bce591f3f",
              "IPY_MODEL_71c850655d934681a9871b2c59672bb0"
            ],
            "layout": "IPY_MODEL_5fe221069ab44e8fa016049adb5c8c97"
          }
        },
        "388d1a1e4ed14660bb714383a78ce1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_737205100be24e84a674dcc773541a91",
            "placeholder": "​",
            "style": "IPY_MODEL_77f51ce2907d4b1d980ddda93eefa776",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "0cc036b5a6444877abe22c2bce591f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_576726e9ce4d4edd807b5f16c938f966",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42a1496fe4414d808cd9c365b814fa84",
            "value": 52
          }
        },
        "71c850655d934681a9871b2c59672bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ccf455f114748f5a8d8148528641224",
            "placeholder": "​",
            "style": "IPY_MODEL_5b9a99ea6d1845eaa45afc2815963646",
            "value": " 52.0/52.0 [00:00&lt;00:00, 3.03kB/s]"
          }
        },
        "5fe221069ab44e8fa016049adb5c8c97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "737205100be24e84a674dcc773541a91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f51ce2907d4b1d980ddda93eefa776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "576726e9ce4d4edd807b5f16c938f966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a1496fe4414d808cd9c365b814fa84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ccf455f114748f5a8d8148528641224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9a99ea6d1845eaa45afc2815963646": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0095b430c423494cb263ea0624ba3412": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9594e051c1ef41bdb8d7927130d9c4fd",
              "IPY_MODEL_14755c7c54ea401db08e9da61f217fcd",
              "IPY_MODEL_d93ffc106a4a48589c97bb76b8b3c26b"
            ],
            "layout": "IPY_MODEL_6921f14621704990acf8e194381b618c"
          }
        },
        "9594e051c1ef41bdb8d7927130d9c4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67b18d944a0d46088ea604fdfdb5aae0",
            "placeholder": "​",
            "style": "IPY_MODEL_829ba8be9e5743be8ef891b53bce658d",
            "value": "config.json: 100%"
          }
        },
        "14755c7c54ea401db08e9da61f217fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6db2ef54ab74518bab26e4adbaf261c",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42416af2fe264a80816aab1bfe159a46",
            "value": 743
          }
        },
        "d93ffc106a4a48589c97bb76b8b3c26b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d65c7d176674187a55443146e73c22b",
            "placeholder": "​",
            "style": "IPY_MODEL_e4b4af7fcebc4cae91430ebf48723d42",
            "value": " 743/743 [00:00&lt;00:00, 47.1kB/s]"
          }
        },
        "6921f14621704990acf8e194381b618c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67b18d944a0d46088ea604fdfdb5aae0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829ba8be9e5743be8ef891b53bce658d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6db2ef54ab74518bab26e4adbaf261c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42416af2fe264a80816aab1bfe159a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d65c7d176674187a55443146e73c22b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4b4af7fcebc4cae91430ebf48723d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8e22584677e464993ee5b12fea6ce3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43b2f433122947289ab38a7e0a40fffc",
              "IPY_MODEL_6d3d75f2daf14a4abbb43f81ccdb98d2",
              "IPY_MODEL_e358e1476d0c4bcea1b68ad943b0efdf"
            ],
            "layout": "IPY_MODEL_ae6b72ba07544e359e9cf3a5fec41e08"
          }
        },
        "43b2f433122947289ab38a7e0a40fffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d0ed0d06ebe4e07ad880f1f80db9558",
            "placeholder": "​",
            "style": "IPY_MODEL_a71b9c0e6b63414987d03aff67d7a76c",
            "value": "model.safetensors: 100%"
          }
        },
        "6d3d75f2daf14a4abbb43f81ccdb98d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a9859ef62284a19af1d31d35620884b",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acf4fe03afbd41c49649aab4e7861ffb",
            "value": 133466304
          }
        },
        "e358e1476d0c4bcea1b68ad943b0efdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf75e0c80bea426886042efb64dbf482",
            "placeholder": "​",
            "style": "IPY_MODEL_fa166296d43943b89132aec53094c8d9",
            "value": " 133M/133M [00:00&lt;00:00, 214MB/s]"
          }
        },
        "ae6b72ba07544e359e9cf3a5fec41e08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d0ed0d06ebe4e07ad880f1f80db9558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a71b9c0e6b63414987d03aff67d7a76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a9859ef62284a19af1d31d35620884b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acf4fe03afbd41c49649aab4e7861ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf75e0c80bea426886042efb64dbf482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa166296d43943b89132aec53094c8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b394ea5833834dce9d67fa4487a5b49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a1c4922b755482cb2ab5fc934859953",
              "IPY_MODEL_6991e6188a92445c991f6cde4064a319",
              "IPY_MODEL_b3892a5ce49b4c87bf40dd69e1856b56"
            ],
            "layout": "IPY_MODEL_b11fff98e0bc4069860925c96fac22a7"
          }
        },
        "3a1c4922b755482cb2ab5fc934859953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7552a8e5ecb143af8e56ae2564fa392a",
            "placeholder": "​",
            "style": "IPY_MODEL_7c5664a7cda1440d9cc50e801821232c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6991e6188a92445c991f6cde4064a319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_755248e9baea40d6b8e7261adfc50f12",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab7c43dc66c94e60a33d1842f12351f9",
            "value": 366
          }
        },
        "b3892a5ce49b4c87bf40dd69e1856b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e002c054f643feb8b814e1fb243e92",
            "placeholder": "​",
            "style": "IPY_MODEL_7f2d1d72385d4405891db714e8107cc4",
            "value": " 366/366 [00:00&lt;00:00, 19.8kB/s]"
          }
        },
        "b11fff98e0bc4069860925c96fac22a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7552a8e5ecb143af8e56ae2564fa392a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c5664a7cda1440d9cc50e801821232c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "755248e9baea40d6b8e7261adfc50f12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab7c43dc66c94e60a33d1842f12351f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6e002c054f643feb8b814e1fb243e92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f2d1d72385d4405891db714e8107cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "617c6703971b4001ba62c4b7b08fbb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d1073d31bfd4328bb6771767d875aed",
              "IPY_MODEL_a6db33919e2840cca3cd0dc9ec839e54",
              "IPY_MODEL_9435628da1f7446599d1763c1dd4cf48"
            ],
            "layout": "IPY_MODEL_0c0ac18b049848ac82df0b885410e9e7"
          }
        },
        "2d1073d31bfd4328bb6771767d875aed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fae51191844471ab83c26b2633fc12a",
            "placeholder": "​",
            "style": "IPY_MODEL_5f177b40b8ba4e91b7271ccfdf57baeb",
            "value": "vocab.txt: 100%"
          }
        },
        "a6db33919e2840cca3cd0dc9ec839e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed74cdfedae74e4db6e6e3c77bc80b7c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0888fd95c7c4833931ee42ddbfc2d66",
            "value": 231508
          }
        },
        "9435628da1f7446599d1763c1dd4cf48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002841c9cf26414082113c9ee481b429",
            "placeholder": "​",
            "style": "IPY_MODEL_1c4eaa621c2b4da08a77043d1ec7ae91",
            "value": " 232k/232k [00:00&lt;00:00, 1.38MB/s]"
          }
        },
        "0c0ac18b049848ac82df0b885410e9e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fae51191844471ab83c26b2633fc12a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f177b40b8ba4e91b7271ccfdf57baeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed74cdfedae74e4db6e6e3c77bc80b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0888fd95c7c4833931ee42ddbfc2d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "002841c9cf26414082113c9ee481b429": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c4eaa621c2b4da08a77043d1ec7ae91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7693bf1465d44f4cbc20ce085c250592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfbc2e2fb3b046a9b926874a4599d7cc",
              "IPY_MODEL_cbc6308db4f746eeb1916c28bbf25d05",
              "IPY_MODEL_e13faf21abfb4c17a8dd82b65a67a418"
            ],
            "layout": "IPY_MODEL_8da543c6754843cc8f9f23004d34b2ea"
          }
        },
        "bfbc2e2fb3b046a9b926874a4599d7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d472c59ccf5348e5a959fbc0a459bcbe",
            "placeholder": "​",
            "style": "IPY_MODEL_639f887f2b8449e18d8522e66a25bd94",
            "value": "tokenizer.json: 100%"
          }
        },
        "cbc6308db4f746eeb1916c28bbf25d05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caac38ca80c34e61a9c3d48958816564",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24976cbfaaee4956a64ff633c8973839",
            "value": 711396
          }
        },
        "e13faf21abfb4c17a8dd82b65a67a418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81267afa3ef84ff194b390cf2ff8bfeb",
            "placeholder": "​",
            "style": "IPY_MODEL_40ccd1ad1e3a4bbab41f3b0af309782a",
            "value": " 711k/711k [00:00&lt;00:00, 2.89MB/s]"
          }
        },
        "8da543c6754843cc8f9f23004d34b2ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d472c59ccf5348e5a959fbc0a459bcbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "639f887f2b8449e18d8522e66a25bd94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caac38ca80c34e61a9c3d48958816564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24976cbfaaee4956a64ff633c8973839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81267afa3ef84ff194b390cf2ff8bfeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ccd1ad1e3a4bbab41f3b0af309782a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ec2bcd5569d4da79eb650c54f858ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5f8692ca648441e81da9e752585cc81",
              "IPY_MODEL_1d8923bab8eb41408fc839589daedd13",
              "IPY_MODEL_e43b34c3b4534d2391f499ffc633f89e"
            ],
            "layout": "IPY_MODEL_2141255a52514f09baa7d9ed7cc4e63c"
          }
        },
        "a5f8692ca648441e81da9e752585cc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e6964351e0a416885078babb11c5b21",
            "placeholder": "​",
            "style": "IPY_MODEL_f52098bf42c14b71b15df9c61fae8258",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "1d8923bab8eb41408fc839589daedd13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d46d64c91b414b558b365173abdca29b",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba433949a984445391b7a0f7809cf4ff",
            "value": 125
          }
        },
        "e43b34c3b4534d2391f499ffc633f89e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbd0a0150bf84a74a2865dfdfb59da2c",
            "placeholder": "​",
            "style": "IPY_MODEL_edfe26c8e047454cbda778beb6c5a0fe",
            "value": " 125/125 [00:00&lt;00:00, 4.90kB/s]"
          }
        },
        "2141255a52514f09baa7d9ed7cc4e63c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e6964351e0a416885078babb11c5b21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f52098bf42c14b71b15df9c61fae8258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d46d64c91b414b558b365173abdca29b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba433949a984445391b7a0f7809cf4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbd0a0150bf84a74a2865dfdfb59da2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edfe26c8e047454cbda778beb6c5a0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "847284bd274e474c90a17d9b90df6a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_908daa75c9bf435fb9ea5e00f698ef29",
              "IPY_MODEL_65288e0b61e446e484f56843b5716342",
              "IPY_MODEL_8329de120ce64e76acc866b178215433"
            ],
            "layout": "IPY_MODEL_67c7752cc3914c4fbfc20542c8af636b"
          }
        },
        "908daa75c9bf435fb9ea5e00f698ef29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bcb4e54089544bd998d15faeae966b5",
            "placeholder": "​",
            "style": "IPY_MODEL_8c2de74d3f2943efb6c3ab415c63e02a",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "65288e0b61e446e484f56843b5716342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1899a435d5bc40a092811c629bc454b4",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc4365e4632c4264843bb1eead9cae2a",
            "value": 190
          }
        },
        "8329de120ce64e76acc866b178215433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3db326e1f0da4bb6a8210632f1929e72",
            "placeholder": "​",
            "style": "IPY_MODEL_240ad16d41584028b6149c15aff18404",
            "value": " 190/190 [00:00&lt;00:00, 11.1kB/s]"
          }
        },
        "67c7752cc3914c4fbfc20542c8af636b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bcb4e54089544bd998d15faeae966b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c2de74d3f2943efb6c3ab415c63e02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1899a435d5bc40a092811c629bc454b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc4365e4632c4264843bb1eead9cae2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3db326e1f0da4bb6a8210632f1929e72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "240ad16d41584028b6149c15aff18404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4b10e26782943a8adc07773c3ee5008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d02570632a0425ab623bb7e595687d1",
              "IPY_MODEL_4c5cd45a15464700ab52d108553320ee",
              "IPY_MODEL_e0ac23fe1446492099fdb3525f49975c"
            ],
            "layout": "IPY_MODEL_a354099d23aa458ead9435ee6ebf1e31"
          }
        },
        "0d02570632a0425ab623bb7e595687d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ffdcd2c778c4305bb410a4ec7e75261",
            "placeholder": "​",
            "style": "IPY_MODEL_927e44b62f4841118cb7229de1bf5e39",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4c5cd45a15464700ab52d108553320ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab0d608886b942f3ab7862e141c52d75",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9be38057018b4f50b8c289eacb0ccf2d",
            "value": 443
          }
        },
        "e0ac23fe1446492099fdb3525f49975c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2378935302f4e2b9af58a4f7e47a8ae",
            "placeholder": "​",
            "style": "IPY_MODEL_47fcbf09178144d19b32d198d6e53efe",
            "value": " 443/443 [00:00&lt;00:00, 32.7kB/s]"
          }
        },
        "a354099d23aa458ead9435ee6ebf1e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ffdcd2c778c4305bb410a4ec7e75261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927e44b62f4841118cb7229de1bf5e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab0d608886b942f3ab7862e141c52d75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9be38057018b4f50b8c289eacb0ccf2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2378935302f4e2b9af58a4f7e47a8ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47fcbf09178144d19b32d198d6e53efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b2e4650a1a14dd4bd2948f2c95b2546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7fcf232e77584b7fa99978e07dede84b",
              "IPY_MODEL_dff223f206654663bdd56a1652195598",
              "IPY_MODEL_820fe93ae5ee43aa8771577a7cb41007"
            ],
            "layout": "IPY_MODEL_515ea979adce451394da2dce47252065"
          }
        },
        "7fcf232e77584b7fa99978e07dede84b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30f5d39b8b684cfc993db4e197116f11",
            "placeholder": "​",
            "style": "IPY_MODEL_c9aa595f43274cac99cd901c5fb90817",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "dff223f206654663bdd56a1652195598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2193a41cc23746cc8ec1a3a828c3d816",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc3c96983b294c5387a0010300a44ccc",
            "value": 5069051
          }
        },
        "820fe93ae5ee43aa8771577a7cb41007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d0051cf23664e189d68cf71f39dde02",
            "placeholder": "​",
            "style": "IPY_MODEL_09632095bb184f4eb2b3a953c25775ff",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 119MB/s]"
          }
        },
        "515ea979adce451394da2dce47252065": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30f5d39b8b684cfc993db4e197116f11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9aa595f43274cac99cd901c5fb90817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2193a41cc23746cc8ec1a3a828c3d816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc3c96983b294c5387a0010300a44ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d0051cf23664e189d68cf71f39dde02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09632095bb184f4eb2b3a953c25775ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db72d262fe4c4aa39740c989d8267235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59f63875ad724c80a3e05868566d1229",
              "IPY_MODEL_17fa797a231e4703813aae8b2a0ce08b",
              "IPY_MODEL_c1ec11f69bd84d7a983afaacfe31a1e1"
            ],
            "layout": "IPY_MODEL_6ae82e4fb6cb4deca99c44f1ea2be9f6"
          }
        },
        "59f63875ad724c80a3e05868566d1229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66679c5e7d1d49b499508e706b7dea19",
            "placeholder": "​",
            "style": "IPY_MODEL_01329da608c049eca7eacc746270b09b",
            "value": "tokenizer.json: 100%"
          }
        },
        "17fa797a231e4703813aae8b2a0ce08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae810761ca844fa8a2aaeb5ca615b367",
            "max": 17098107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04bd4ca79fd94cf48aa8403e8c4cc006",
            "value": 17098107
          }
        },
        "c1ec11f69bd84d7a983afaacfe31a1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d08bd3da94f4f41bf462a2d8be3ff99",
            "placeholder": "​",
            "style": "IPY_MODEL_a046e67d74364681a330223e728b3368",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 164MB/s]"
          }
        },
        "6ae82e4fb6cb4deca99c44f1ea2be9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66679c5e7d1d49b499508e706b7dea19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01329da608c049eca7eacc746270b09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae810761ca844fa8a2aaeb5ca615b367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04bd4ca79fd94cf48aa8403e8c4cc006": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d08bd3da94f4f41bf462a2d8be3ff99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a046e67d74364681a330223e728b3368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab18937c16e245cd98d407ef602129b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_040eabfbc2244727a9f4a726f1944bd5",
              "IPY_MODEL_1b9564a6c1c8489eb993535262ef6fe1",
              "IPY_MODEL_8cdf002ef1b8459181f59f8c55c6eabf"
            ],
            "layout": "IPY_MODEL_dda18ecb0f0a48eeb4b0cb2666ddcc76"
          }
        },
        "040eabfbc2244727a9f4a726f1944bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3db0314a3e0428a927e6826bcc42f7a",
            "placeholder": "​",
            "style": "IPY_MODEL_de2b9d61ff5f42eda0f378df13d1383b",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "1b9564a6c1c8489eb993535262ef6fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80fbb76a117d4a9bad1a9fc1ab94f7a6",
            "max": 279,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a4ad3f0120b400fa8f4fe6453553d11",
            "value": 279
          }
        },
        "8cdf002ef1b8459181f59f8c55c6eabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f55c543050d9414bbceafe7ac4daf812",
            "placeholder": "​",
            "style": "IPY_MODEL_368ebddbe3ec4c34ae6875987076a42c",
            "value": " 279/279 [00:00&lt;00:00, 20.7kB/s]"
          }
        },
        "dda18ecb0f0a48eeb4b0cb2666ddcc76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3db0314a3e0428a927e6826bcc42f7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de2b9d61ff5f42eda0f378df13d1383b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80fbb76a117d4a9bad1a9fc1ab94f7a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a4ad3f0120b400fa8f4fe6453553d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f55c543050d9414bbceafe7ac4daf812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "368ebddbe3ec4c34ae6875987076a42c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b433235a07924d7c97f521e92099500f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee47c9079c664959bb60c73ac9e06772",
              "IPY_MODEL_a62bd656b53b41639a6a1528869ed64d",
              "IPY_MODEL_81f392375151487292938f2adb92788b"
            ],
            "layout": "IPY_MODEL_fa0fd35da8364c29969fdcd0d242b8df"
          }
        },
        "ee47c9079c664959bb60c73ac9e06772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_794fa2406e8d481993bf434dce76bb3a",
            "placeholder": "​",
            "style": "IPY_MODEL_d874de2255ab432dbdbd2560e4e6b66a",
            "value": "config.json: 100%"
          }
        },
        "a62bd656b53b41639a6a1528869ed64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_752ad177cdd04a2d84be0bab4b5c0331",
            "max": 801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_724b9c5f4f304bd9a38ce24241023d03",
            "value": 801
          }
        },
        "81f392375151487292938f2adb92788b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3ffb08c23e94a21b623130ad64beb20",
            "placeholder": "​",
            "style": "IPY_MODEL_878dc30ede9845d68ba7765e11dabda2",
            "value": " 801/801 [00:00&lt;00:00, 59.4kB/s]"
          }
        },
        "fa0fd35da8364c29969fdcd0d242b8df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794fa2406e8d481993bf434dce76bb3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d874de2255ab432dbdbd2560e4e6b66a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "752ad177cdd04a2d84be0bab4b5c0331": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "724b9c5f4f304bd9a38ce24241023d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3ffb08c23e94a21b623130ad64beb20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878dc30ede9845d68ba7765e11dabda2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a658f674a10845569e57ed7b6743ba65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35424399b36f40d4803bba75bdafff3a",
              "IPY_MODEL_ad7fe4211a244521abb9839a6c98370d",
              "IPY_MODEL_bfb8459c981d46919b114a22aed92979"
            ],
            "layout": "IPY_MODEL_75c81daf39fc46208bcea3b45593e210"
          }
        },
        "35424399b36f40d4803bba75bdafff3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1a02d633e2f4c359377ee94ae7532d2",
            "placeholder": "​",
            "style": "IPY_MODEL_39cd307b9ad54fecbb9a477a2ff18aa8",
            "value": "model.safetensors: 100%"
          }
        },
        "ad7fe4211a244521abb9839a6c98370d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71a2a82ed3864ecb9b0537bc6745c8da",
            "max": 2239618772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88e6ccd4685b4075b3c4ceeecc937d7f",
            "value": 2239618772
          }
        },
        "bfb8459c981d46919b114a22aed92979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bac8b0717db4a49bf54a5ff0488fcfc",
            "placeholder": "​",
            "style": "IPY_MODEL_0d00b4e36ab947e5a6012232f849eb15",
            "value": " 2.24G/2.24G [00:14&lt;00:00, 210MB/s]"
          }
        },
        "75c81daf39fc46208bcea3b45593e210": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1a02d633e2f4c359377ee94ae7532d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39cd307b9ad54fecbb9a477a2ff18aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71a2a82ed3864ecb9b0537bc6745c8da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88e6ccd4685b4075b3c4ceeecc937d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bac8b0717db4a49bf54a5ff0488fcfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d00b4e36ab947e5a6012232f849eb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winterForestStump/thesis/blob/main/notebooks/rag_x_phi3_generalQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-nomic langchain langchain-core langchain-community --quiet\n",
        "%pip install -U tiktoken langchainhub chromadb langgraph tavily-python langchain-text-splitters\n",
        "%pip install sentence_transformers FlagEmbedding --quiet"
      ],
      "metadata": {
        "id": "FvVmzL2j9VE2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LlamaCpp x GPU usage\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUlkK-AQ9VE_",
        "outputId": "f9ec933a-be1b-4aef-8334-3e4e3fc8d0b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.79.tar.gz (50.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.79-cp310-cp310-linux_x86_64.whl size=172348863 sha256=7f2aa475809f1f6a81b855fc8265cfaa71895be232746084097b5040e32b9383\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/2e/11/8b10c6b698e6abc1289e9919e098ac4bcf6b16ebd46153e8ba\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IfHIpuz9VFB",
        "outputId": "8add702e-7a1f-4dee-cae0-e18fcad57123"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "import chromadb\n",
        "from langchain.storage.file_system import LocalFileStore\n",
        "from langchain.storage._lc_store import create_kv_docstore\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "4chIcfH79VFC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-fp16.gguf --local-dir ./models --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFSdx4MY9VFD",
        "outputId": "54ddbc4a-7842-4515-8792-6d9b731376a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Downloading 'Phi-3-mini-4k-instruct-fp16.gguf' to 'models/.huggingface/download/Phi-3-mini-4k-instruct-fp16.gguf.5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3.incomplete'\n",
            "Phi-3-mini-4k-instruct-fp16.gguf: 100% 7.64G/7.64G [00:46<00:00, 165MB/s]\n",
            "Download complete. Moving file to models/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "models/Phi-3-mini-4k-instruct-fp16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEMP = 0\n",
        "N_CTX = 4096\n",
        "N_GPU_L = -1\n",
        "\n",
        "llm_phi3 = LlamaCpp(\n",
        "    model_path=\"/content/models/Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    temperature=TEMP,\n",
        "    n_ctx=N_CTX,\n",
        "    n_gpu_layers = N_GPU_L,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UWQXosB9VFE",
        "outputId": "78fcd74e-d7c7-459b-ce3e-4939893c6769"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /content/models/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  130 tensors\n",
            "llm_load_vocab: special tokens cache size = 323\n",
            "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = phi3\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = Phi3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   187.88 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  7100.64 MiB\n",
            "....................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1536.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =    18.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.88 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = pd.read_fwf(\"https://raw.githubusercontent.com/winterForestStump/thesis/main/questions/questions_ver2.txt\", names=['question'])\n",
        "questions.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA87mjiJ9VFE",
        "outputId": "6c9de67e-c2ab-49df-f25c-2b782b94b27f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 35 entries, 0 to 34\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   question  35 non-null     object\n",
            "dtypes: object(1)\n",
            "memory usage: 408.0+ bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'}, #gpu\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "9a560469e13744bfb855def1d3f169cb",
            "6b5a62797c5448c28114e8895d86a49f",
            "67723769e2ad4d03b05db94a72b54997",
            "ab1a229329234ec8b1a38db7a5c63538",
            "d3707b13a9ec4be29fc558e4642f0b92",
            "4caaefe8fb3845dc8d435f3a0c9ae9ba",
            "8e67064bb9f5441699c6f86833cb2cec",
            "ed45d04cf29e4e1c999bef6a23a7c6e1",
            "340fd53202ab458faf71b44b159b2980",
            "aefd1848e577430d83cf68eaa7610681",
            "b5cbb949e14a44ddb0ffb8b999f2f5da",
            "5b81a17704ed4b3bbfa2f7e2b6f9965f",
            "900a13d1341d4209841bb78274ba1e3e",
            "c0c266362e7549f7bf70a85e88dd8d3e",
            "77b81fd7428243e5be6e31249deae73b",
            "567124d02f6f4bd283141ab3b44a2dbf",
            "1ce66a04b55c407b9f50269c5328c0b3",
            "59d676957ce04ab1b90c862329ab60a0",
            "ed370dd6e86a46c2b5ad1f8e6cf42913",
            "5be3eaa1edff4fa68eb710c699c2077e",
            "0284e73f56b44280966849a08fce3ce1",
            "9181e5682a1e4d54bbb9a01969e3b273",
            "aa6cb7abc1fe4d83b30711d68807eef7",
            "91dbb46e831a4f4a8bfe9bcd49164521",
            "b46a6cace53f4c168a25ace536a9c6b7",
            "3d59caf78f43405f8a28cb9b8987e405",
            "5a206d9f216d451d8d8ad1970e9e3931",
            "5cef32a9c3d64d388bad5b02390d4f45",
            "ccd0558687984b97a2170af96eb7e45c",
            "351f34f268814d43b277ea53e1d156ef",
            "b5a013edb3994473b2007a17915422cb",
            "3c928f09b165496b8a9486e2afe6e383",
            "2d78edf6865d451b90d085b4bfd8bc67",
            "e54331fc50a7445faade1de1a3a2190c",
            "388d1a1e4ed14660bb714383a78ce1fb",
            "0cc036b5a6444877abe22c2bce591f3f",
            "71c850655d934681a9871b2c59672bb0",
            "5fe221069ab44e8fa016049adb5c8c97",
            "737205100be24e84a674dcc773541a91",
            "77f51ce2907d4b1d980ddda93eefa776",
            "576726e9ce4d4edd807b5f16c938f966",
            "42a1496fe4414d808cd9c365b814fa84",
            "6ccf455f114748f5a8d8148528641224",
            "5b9a99ea6d1845eaa45afc2815963646",
            "0095b430c423494cb263ea0624ba3412",
            "9594e051c1ef41bdb8d7927130d9c4fd",
            "14755c7c54ea401db08e9da61f217fcd",
            "d93ffc106a4a48589c97bb76b8b3c26b",
            "6921f14621704990acf8e194381b618c",
            "67b18d944a0d46088ea604fdfdb5aae0",
            "829ba8be9e5743be8ef891b53bce658d",
            "d6db2ef54ab74518bab26e4adbaf261c",
            "42416af2fe264a80816aab1bfe159a46",
            "6d65c7d176674187a55443146e73c22b",
            "e4b4af7fcebc4cae91430ebf48723d42",
            "d8e22584677e464993ee5b12fea6ce3a",
            "43b2f433122947289ab38a7e0a40fffc",
            "6d3d75f2daf14a4abbb43f81ccdb98d2",
            "e358e1476d0c4bcea1b68ad943b0efdf",
            "ae6b72ba07544e359e9cf3a5fec41e08",
            "6d0ed0d06ebe4e07ad880f1f80db9558",
            "a71b9c0e6b63414987d03aff67d7a76c",
            "7a9859ef62284a19af1d31d35620884b",
            "acf4fe03afbd41c49649aab4e7861ffb",
            "cf75e0c80bea426886042efb64dbf482",
            "fa166296d43943b89132aec53094c8d9",
            "b394ea5833834dce9d67fa4487a5b49c",
            "3a1c4922b755482cb2ab5fc934859953",
            "6991e6188a92445c991f6cde4064a319",
            "b3892a5ce49b4c87bf40dd69e1856b56",
            "b11fff98e0bc4069860925c96fac22a7",
            "7552a8e5ecb143af8e56ae2564fa392a",
            "7c5664a7cda1440d9cc50e801821232c",
            "755248e9baea40d6b8e7261adfc50f12",
            "ab7c43dc66c94e60a33d1842f12351f9",
            "b6e002c054f643feb8b814e1fb243e92",
            "7f2d1d72385d4405891db714e8107cc4",
            "617c6703971b4001ba62c4b7b08fbb61",
            "2d1073d31bfd4328bb6771767d875aed",
            "a6db33919e2840cca3cd0dc9ec839e54",
            "9435628da1f7446599d1763c1dd4cf48",
            "0c0ac18b049848ac82df0b885410e9e7",
            "0fae51191844471ab83c26b2633fc12a",
            "5f177b40b8ba4e91b7271ccfdf57baeb",
            "ed74cdfedae74e4db6e6e3c77bc80b7c",
            "f0888fd95c7c4833931ee42ddbfc2d66",
            "002841c9cf26414082113c9ee481b429",
            "1c4eaa621c2b4da08a77043d1ec7ae91",
            "7693bf1465d44f4cbc20ce085c250592",
            "bfbc2e2fb3b046a9b926874a4599d7cc",
            "cbc6308db4f746eeb1916c28bbf25d05",
            "e13faf21abfb4c17a8dd82b65a67a418",
            "8da543c6754843cc8f9f23004d34b2ea",
            "d472c59ccf5348e5a959fbc0a459bcbe",
            "639f887f2b8449e18d8522e66a25bd94",
            "caac38ca80c34e61a9c3d48958816564",
            "24976cbfaaee4956a64ff633c8973839",
            "81267afa3ef84ff194b390cf2ff8bfeb",
            "40ccd1ad1e3a4bbab41f3b0af309782a",
            "3ec2bcd5569d4da79eb650c54f858ac3",
            "a5f8692ca648441e81da9e752585cc81",
            "1d8923bab8eb41408fc839589daedd13",
            "e43b34c3b4534d2391f499ffc633f89e",
            "2141255a52514f09baa7d9ed7cc4e63c",
            "1e6964351e0a416885078babb11c5b21",
            "f52098bf42c14b71b15df9c61fae8258",
            "d46d64c91b414b558b365173abdca29b",
            "ba433949a984445391b7a0f7809cf4ff",
            "dbd0a0150bf84a74a2865dfdfb59da2c",
            "edfe26c8e047454cbda778beb6c5a0fe",
            "847284bd274e474c90a17d9b90df6a9e",
            "908daa75c9bf435fb9ea5e00f698ef29",
            "65288e0b61e446e484f56843b5716342",
            "8329de120ce64e76acc866b178215433",
            "67c7752cc3914c4fbfc20542c8af636b",
            "0bcb4e54089544bd998d15faeae966b5",
            "8c2de74d3f2943efb6c3ab415c63e02a",
            "1899a435d5bc40a092811c629bc454b4",
            "fc4365e4632c4264843bb1eead9cae2a",
            "3db326e1f0da4bb6a8210632f1929e72",
            "240ad16d41584028b6149c15aff18404"
          ]
        },
        "id": "QCpr-Zyg9VFG",
        "outputId": "ab5348f2-df2a-40bb-9156-f1aad86bfc34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a560469e13744bfb855def1d3f169cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b81a17704ed4b3bbfa2f7e2b6f9965f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa6cb7abc1fe4d83b30711d68807eef7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e54331fc50a7445faade1de1a3a2190c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0095b430c423494cb263ea0624ba3412"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8e22584677e464993ee5b12fea6ce3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b394ea5833834dce9d67fa4487a5b49c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "617c6703971b4001ba62c4b7b08fbb61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7693bf1465d44f4cbc20ce085c250592"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ec2bcd5569d4da79eb650c54f858ac3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "847284bd274e474c90a17d9b90df6a9e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation"
      ],
      "metadata": {
        "id": "Dt9-e49TRv0n",
        "outputId": "3ddaf23d-09d2-44a4-e7db-d98f6ef11278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "a4b10e26782943a8adc07773c3ee5008",
            "0d02570632a0425ab623bb7e595687d1",
            "4c5cd45a15464700ab52d108553320ee",
            "e0ac23fe1446492099fdb3525f49975c",
            "a354099d23aa458ead9435ee6ebf1e31",
            "3ffdcd2c778c4305bb410a4ec7e75261",
            "927e44b62f4841118cb7229de1bf5e39",
            "ab0d608886b942f3ab7862e141c52d75",
            "9be38057018b4f50b8c289eacb0ccf2d",
            "d2378935302f4e2b9af58a4f7e47a8ae",
            "47fcbf09178144d19b32d198d6e53efe",
            "2b2e4650a1a14dd4bd2948f2c95b2546",
            "7fcf232e77584b7fa99978e07dede84b",
            "dff223f206654663bdd56a1652195598",
            "820fe93ae5ee43aa8771577a7cb41007",
            "515ea979adce451394da2dce47252065",
            "30f5d39b8b684cfc993db4e197116f11",
            "c9aa595f43274cac99cd901c5fb90817",
            "2193a41cc23746cc8ec1a3a828c3d816",
            "cc3c96983b294c5387a0010300a44ccc",
            "7d0051cf23664e189d68cf71f39dde02",
            "09632095bb184f4eb2b3a953c25775ff",
            "db72d262fe4c4aa39740c989d8267235",
            "59f63875ad724c80a3e05868566d1229",
            "17fa797a231e4703813aae8b2a0ce08b",
            "c1ec11f69bd84d7a983afaacfe31a1e1",
            "6ae82e4fb6cb4deca99c44f1ea2be9f6",
            "66679c5e7d1d49b499508e706b7dea19",
            "01329da608c049eca7eacc746270b09b",
            "ae810761ca844fa8a2aaeb5ca615b367",
            "04bd4ca79fd94cf48aa8403e8c4cc006",
            "0d08bd3da94f4f41bf462a2d8be3ff99",
            "a046e67d74364681a330223e728b3368",
            "ab18937c16e245cd98d407ef602129b3",
            "040eabfbc2244727a9f4a726f1944bd5",
            "1b9564a6c1c8489eb993535262ef6fe1",
            "8cdf002ef1b8459181f59f8c55c6eabf",
            "dda18ecb0f0a48eeb4b0cb2666ddcc76",
            "c3db0314a3e0428a927e6826bcc42f7a",
            "de2b9d61ff5f42eda0f378df13d1383b",
            "80fbb76a117d4a9bad1a9fc1ab94f7a6",
            "3a4ad3f0120b400fa8f4fe6453553d11",
            "f55c543050d9414bbceafe7ac4daf812",
            "368ebddbe3ec4c34ae6875987076a42c",
            "b433235a07924d7c97f521e92099500f",
            "ee47c9079c664959bb60c73ac9e06772",
            "a62bd656b53b41639a6a1528869ed64d",
            "81f392375151487292938f2adb92788b",
            "fa0fd35da8364c29969fdcd0d242b8df",
            "794fa2406e8d481993bf434dce76bb3a",
            "d874de2255ab432dbdbd2560e4e6b66a",
            "752ad177cdd04a2d84be0bab4b5c0331",
            "724b9c5f4f304bd9a38ce24241023d03",
            "b3ffb08c23e94a21b623130ad64beb20",
            "878dc30ede9845d68ba7765e11dabda2",
            "a658f674a10845569e57ed7b6743ba65",
            "35424399b36f40d4803bba75bdafff3a",
            "ad7fe4211a244521abb9839a6c98370d",
            "bfb8459c981d46919b114a22aed92979",
            "75c81daf39fc46208bcea3b45593e210",
            "e1a02d633e2f4c359377ee94ae7532d2",
            "39cd307b9ad54fecbb9a477a2ff18aa8",
            "71a2a82ed3864ecb9b0537bc6745c8da",
            "88e6ccd4685b4075b3c4ceeecc937d7f",
            "5bac8b0717db4a49bf54a5ff0488fcfc",
            "0d00b4e36ab947e5a6012232f849eb15"
          ]
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4b10e26782943a8adc07773c3ee5008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b2e4650a1a14dd4bd2948f2c95b2546"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db72d262fe4c4aa39740c989d8267235"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab18937c16e245cd98d407ef602129b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b433235a07924d7c97f521e92099500f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a658f674a10845569e57ed7b6743ba65"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Metadata company name\n",
        "prompt_metadata = PromptTemplate(\n",
        "template=\"\"\"\n",
        "  <|assistant|> You are tasked with identifying the correct spelling of the company name mentioned in the user's input by searching through the list of fixed company names in the database metadata.\n",
        "  This precise spelling will be crucial for SQL filtering purposes. \\n\n",
        "  Provide a concise response containing only the correct company name. \\n\n",
        "  Please format your response as a JSON object with only a single key 'company', WITHOUT any additional commentary. <|end|>\n",
        "  <|user|> Database metadata with company names: \\n\\n {metadata_list} \\n\\n User question: {name_of_the_company} <|end|>\n",
        "  <|assistant|>\n",
        "\"\"\",\n",
        "input_variables=[\"name_of_the_company\", \"metadata_list\"])\n",
        "\n",
        "retrieval_metadata = prompt_metadata | llm_phi3 | JsonOutputParser()"
      ],
      "metadata": {
        "id": "kJanQu9w9VFH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persistent_client = chromadb.PersistentClient('/content/drive/MyDrive/Thesis/chromadb')\n",
        "collection = persistent_client.get_or_create_collection(\"reports_l2\")\n",
        "fs = LocalFileStore('/content/drive/MyDrive/Thesis/reports_store_location')\n",
        "store = create_kv_docstore(fs)\n",
        "vectorstore = Chroma(client = persistent_client,\n",
        "                     collection_name=\"reports_l2\",\n",
        "                     embedding_function=bge_embeddings,\n",
        "                     persist_directory='/content/drive/MyDrive/Thesis/chromadb')\n",
        "vectorstore.persist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncJiWt939VFI",
        "outputId": "81668497-6577-485b-cf2e-3a240a52ebf5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = vectorstore.get()['metadatas']\n",
        "metadata_list = []\n",
        "for i in range(len(metadata)):\n",
        "  metadata_list.append(metadata[i]['company'])\n",
        "metadata_list = list(set(metadata_list))"
      ],
      "metadata": {
        "id": "Xcq5E9K19VFI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Retrieval Grader\n",
        "llm_retrieval = llm_phi3\n",
        "\n",
        "prompt_retrieval_grader = PromptTemplate(\n",
        "    template=\"\"\"<|assistant|> You are a grader assessing relevance of a retrieved document to a user question.\n",
        "    If the document contains information related to the user question, grade it as relevant. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.<|end|>\n",
        "    <|user|> Here is the retrieved document: {document}\\n Here is the user question: {question} <|end|>\n",
        "    <|assistant|>\n",
        "    \"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n",
        "\n",
        "retrieval_grader = prompt_retrieval_grader | llm_retrieval | StrOutputParser()"
      ],
      "metadata": {
        "id": "aEFPpe0M9VFL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "llm_generate = llm_phi3\n",
        "\n",
        "prompt_generate = PromptTemplate(\n",
        "    template=\"\"\"<|assistant|> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know. Keep the answer concise <|end|>\n",
        "    <|user|> Question: {question}. \\n Context: {documents} \\n Answer: <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"question\", \"documents\"],\n",
        ")\n",
        "\n",
        "rag_chain = prompt_generate | llm_generate | StrOutputParser()"
      ],
      "metadata": {
        "id": "hdn828f89VFM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Hallucination Grader\n",
        "llm_hallucination_grader = llm_phi3\n",
        "\n",
        "# Prompt\n",
        "prompt_hallucination_grader = PromptTemplate(\n",
        "    template=\"\"\" <|assistant|> You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
        "    Give a binary 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts.<|end|>\n",
        "    <|user|> Here are the facts: {documents} \\n Here is the answer: {generation}  <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"generation\", \"documents\"],\n",
        ")\n",
        "\n",
        "hallucination_grader = prompt_hallucination_grader | llm_hallucination_grader | StrOutputParser()"
      ],
      "metadata": {
        "id": "ZIub6HTG9VFM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Answer Grader\n",
        "llm_answer_grader = llm_phi3\n",
        "\n",
        "# Prompt\n",
        "prompt_answer_grader = PromptTemplate(\n",
        "    template=\"\"\"<|assistant|> You are a grader assessing whether an answer is useful to resolve a question.\n",
        "    Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question.<|end|>\n",
        "    <|user|> Here is the answer: {generation} \\n Here is the question: {question} <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"generation\", \"question\"],\n",
        ")\n",
        "\n",
        "answer_grader = prompt_answer_grader | llm_answer_grader | StrOutputParser()"
      ],
      "metadata": {
        "id": "vWM7EZxJ9VFN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "company_names = ['coca cola', 'nike', '3M', 'amazon', 'jpmorgan', 'locheed martin', 'microsoft', 'paypal', 'verizon', 'walmart']"
      ],
      "metadata": {
        "id": "7xZMFSSNRCjl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PAR_CHUNKS = 20\n",
        "N_DOCS_RETURN = 2\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=256)\n",
        "\n",
        "for company_name in company_names:\n",
        "\n",
        "  results_list = []\n",
        "\n",
        "  for i in tqdm(range(len(questions))):\n",
        "    company = retrieval_metadata.invoke({\"name_of_the_company\": company_name, \"metadata_list\": metadata_list})\n",
        "\n",
        "    big_chunks_retriever = ParentDocumentRetriever(\n",
        "      vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter,\n",
        "      search_kwargs={'filter': {'company': company['company']}, 'k': NUM_PAR_CHUNKS})\n",
        "\n",
        "    query = questions['question'][i]\n",
        "    passage = big_chunks_retriever.invoke(query)\n",
        "    texts = []\n",
        "    for i in range(len(passage)):\n",
        "      texts.append([query, passage[i].page_content])\n",
        "\n",
        "    scores = reranker.compute_score(texts)\n",
        "    combined = list(zip(texts, scores))\n",
        "    sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
        "    top_texts = [item[0] for item in sorted_combined[:N_DOCS_RETURN]]\n",
        "    docs = [inner_list[1] for inner_list in top_texts if len(inner_list)>1]\n",
        "\n",
        "    retrieval_grade = retrieval_grader.invoke({\"question\": query, \"document\": docs})\n",
        "    generation = rag_chain.invoke({\"documents\": docs, \"question\": query})\n",
        "    hallucination_grade = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
        "    answer_grade = answer_grader.invoke({\"question\": query, \"generation\": generation})\n",
        "\n",
        "    results_list.append(pd.DataFrame({\n",
        "          'question': [query],\n",
        "          'company': [company['company']],\n",
        "          'response': [generation],\n",
        "          'context': [docs],\n",
        "          'retrieval_grade': [retrieval_grade],\n",
        "          'hallucination_grade': [hallucination_grade],\n",
        "          'answer_grade': [answer_grade]\n",
        "      }))\n",
        "\n",
        "  results = pd.concat(results_list, ignore_index=True)\n",
        "  results.to_json(f'/content/drive/MyDrive/Thesis/rag_evaluation/bge-reranker_x_phi3-4k/eval_{company[\"company\"]}.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro7GaCz69VFO",
        "outputId": "2b1715f8-f3bf-4eb7-eef6-d62e6d3954e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "llama_print_timings: prompt eval time =    4514.66 ms /   856 tokens (    5.27 ms per token,   189.60 tokens per second)\n",
            "llama_print_timings:        eval time =      39.67 ms /     1 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    4567.85 ms /   857 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      56.02 ms /   104 runs   (    0.54 ms per token,  1856.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4347.47 ms /   823 tokens (    5.28 ms per token,   189.31 tokens per second)\n",
            "llama_print_timings:        eval time =    4082.68 ms /   103 runs   (   39.64 ms per token,    25.23 tokens per second)\n",
            "llama_print_timings:       total time =    8561.03 ms /   926 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.37 ms /    61 runs   (    0.71 ms per token,  1406.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4847.81 ms /   910 tokens (    5.33 ms per token,   187.71 tokens per second)\n",
            "llama_print_timings:        eval time =    2426.20 ms /    60 runs   (   40.44 ms per token,    24.73 tokens per second)\n",
            "llama_print_timings:       total time =    7371.30 ms /   970 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1886.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =     962.47 ms /   190 tokens (    5.07 ms per token,   197.41 tokens per second)\n",
            "llama_print_timings:        eval time =      37.85 ms /     1 runs   (   37.85 ms per token,    26.42 tokens per second)\n",
            "llama_print_timings:       total time =    1005.30 ms /   191 tokens\n",
            " 94%|█████████▍| 33/35 [25:23<01:23, 41.68s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      59.47 ms /   106 runs   (    0.56 ms per token,  1782.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2242.67 ms /   437 tokens (    5.13 ms per token,   194.86 tokens per second)\n",
            "llama_print_timings:        eval time =    4075.58 ms /   105 runs   (   38.82 ms per token,    25.76 tokens per second)\n",
            "llama_print_timings:       total time =    6437.83 ms /   542 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.47 ms /     2 runs   (    0.73 ms per token,  1364.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4645.75 ms /   871 tokens (    5.33 ms per token,   187.48 tokens per second)\n",
            "llama_print_timings:        eval time =      39.67 ms /     1 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    4702.64 ms /   872 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      88.63 ms /   156 runs   (    0.57 ms per token,  1760.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4418.35 ms /   838 tokens (    5.27 ms per token,   189.66 tokens per second)\n",
            "llama_print_timings:        eval time =    6158.46 ms /   155 runs   (   39.73 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =   10772.64 ms /   993 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      79.51 ms /   140 runs   (    0.57 ms per token,  1760.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5288.82 ms /   982 tokens (    5.39 ms per token,   185.67 tokens per second)\n",
            "llama_print_timings:        eval time =    5584.99 ms /   139 runs   (   40.18 ms per token,    24.89 tokens per second)\n",
            "llama_print_timings:       total time =   11048.18 ms /  1121 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.23 ms /     2 runs   (    0.61 ms per token,  1628.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1196.98 ms /   237 tokens (    5.05 ms per token,   198.00 tokens per second)\n",
            "llama_print_timings:        eval time =      37.59 ms /     1 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    1239.02 ms /   238 tokens\n",
            " 97%|█████████▋| 34/35 [26:00<00:40, 40.23s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      71.46 ms /   106 runs   (    0.67 ms per token,  1483.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2241.70 ms /   437 tokens (    5.13 ms per token,   194.94 tokens per second)\n",
            "llama_print_timings:        eval time =    4086.72 ms /   105 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    6478.71 ms /   542 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.39 ms /     2 runs   (    0.70 ms per token,  1435.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4283.88 ms /   810 tokens (    5.29 ms per token,   189.08 tokens per second)\n",
            "llama_print_timings:        eval time =      39.07 ms /     1 runs   (   39.07 ms per token,    25.60 tokens per second)\n",
            "llama_print_timings:       total time =    4336.36 ms /   811 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      94.14 ms /   166 runs   (    0.57 ms per token,  1763.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4075.65 ms /   776 tokens (    5.25 ms per token,   190.40 tokens per second)\n",
            "llama_print_timings:        eval time =    6557.87 ms /   166 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =   10841.66 ms /   942 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      89.35 ms /   155 runs   (    0.58 ms per token,  1734.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4969.96 ms /   922 tokens (    5.39 ms per token,   185.51 tokens per second)\n",
            "llama_print_timings:        eval time =    6212.99 ms /   154 runs   (   40.34 ms per token,    24.79 tokens per second)\n",
            "llama_print_timings:       total time =   11376.63 ms /  1076 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1813.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1285.33 ms /   256 tokens (    5.02 ms per token,   199.17 tokens per second)\n",
            "llama_print_timings:        eval time =      37.60 ms /     1 runs   (   37.60 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    1327.90 ms /   257 tokens\n",
            "100%|██████████| 35/35 [26:42<00:00, 45.78s/it]\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.84 ms /    18 runs   (    0.60 ms per token,  1661.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2245.49 ms /   434 tokens (    5.17 ms per token,   193.28 tokens per second)\n",
            "llama_print_timings:        eval time =     659.35 ms /    17 runs   (   38.79 ms per token,    25.78 tokens per second)\n",
            "llama_print_timings:       total time =    2930.10 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     138.78 ms /   231 runs   (    0.60 ms per token,  1664.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10793.38 ms /  1886 tokens (    5.72 ms per token,   174.74 tokens per second)\n",
            "llama_print_timings:        eval time =    9992.20 ms /   230 runs   (   43.44 ms per token,    23.02 tokens per second)\n",
            "llama_print_timings:       total time =   21153.87 ms /  2116 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      87.31 ms /   147 runs   (    0.59 ms per token,  1683.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10660.66 ms /  1853 tokens (    5.75 ms per token,   173.82 tokens per second)\n",
            "llama_print_timings:        eval time =    6325.91 ms /   146 runs   (   43.33 ms per token,    23.08 tokens per second)\n",
            "llama_print_timings:       total time =   17212.68 ms /  1999 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     105.56 ms /   217 runs   (    0.49 ms per token,  2055.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11521.59 ms /  1986 tokens (    5.80 ms per token,   172.37 tokens per second)\n",
            "llama_print_timings:        eval time =    9382.51 ms /   216 runs   (   43.44 ms per token,    23.02 tokens per second)\n",
            "llama_print_timings:       total time =   21182.92 ms /  2202 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1514.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1175.74 ms /   230 tokens (    5.11 ms per token,   195.62 tokens per second)\n",
            "llama_print_timings:        eval time =      39.19 ms /     1 runs   (   39.19 ms per token,    25.52 tokens per second)\n",
            "llama_print_timings:       total time =    1221.58 ms /   231 tokens\n",
            "  3%|▎         | 1/35 [01:14<42:21, 74.74s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      12.67 ms /    18 runs   (    0.70 ms per token,  1421.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2259.47 ms /   434 tokens (    5.21 ms per token,   192.08 tokens per second)\n",
            "llama_print_timings:        eval time =     662.40 ms /    17 runs   (   38.96 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    2953.44 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     130.38 ms /   233 runs   (    0.56 ms per token,  1787.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9505.58 ms /  1688 tokens (    5.63 ms per token,   177.58 tokens per second)\n",
            "llama_print_timings:        eval time =    9871.76 ms /   232 runs   (   42.55 ms per token,    23.50 tokens per second)\n",
            "llama_print_timings:       total time =   19698.12 ms /  1920 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      54.45 ms /   107 runs   (    0.51 ms per token,  1964.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9390.67 ms /  1655 tokens (    5.67 ms per token,   176.24 tokens per second)\n",
            "llama_print_timings:        eval time =    4518.32 ms /   106 runs   (   42.63 ms per token,    23.46 tokens per second)\n",
            "llama_print_timings:       total time =   14051.38 ms /  1761 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      68.86 ms /   123 runs   (    0.56 ms per token,  1786.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9938.51 ms /  1744 tokens (    5.70 ms per token,   175.48 tokens per second)\n",
            "llama_print_timings:        eval time =    5217.92 ms /   122 runs   (   42.77 ms per token,    23.38 tokens per second)\n",
            "llama_print_timings:       total time =   15333.62 ms /  1866 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.33 ms /     2 runs   (    0.67 ms per token,  1503.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1007.34 ms /   194 tokens (    5.19 ms per token,   192.59 tokens per second)\n",
            "llama_print_timings:        eval time =      37.70 ms /     1 runs   (   37.70 ms per token,    26.52 tokens per second)\n",
            "llama_print_timings:       total time =    1050.60 ms /   195 tokens\n",
            "  6%|▌         | 2/35 [02:12<35:46, 65.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.94 ms /    18 runs   (    0.55 ms per token,  1811.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2269.10 ms /   434 tokens (    5.23 ms per token,   191.27 tokens per second)\n",
            "llama_print_timings:        eval time =     655.03 ms /    17 runs   (   38.53 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    2949.12 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.35 ms /     2 runs   (    0.68 ms per token,  1479.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6080.42 ms /  1127 tokens (    5.40 ms per token,   185.35 tokens per second)\n",
            "llama_print_timings:        eval time =      40.21 ms /     1 runs   (   40.21 ms per token,    24.87 tokens per second)\n",
            "llama_print_timings:       total time =    6137.45 ms /  1128 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      52.05 ms /    93 runs   (    0.56 ms per token,  1786.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5910.75 ms /  1094 tokens (    5.40 ms per token,   185.09 tokens per second)\n",
            "llama_print_timings:        eval time =    3723.78 ms /    92 runs   (   40.48 ms per token,    24.71 tokens per second)\n",
            "llama_print_timings:       total time =    9757.41 ms /  1186 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.50 ms /     2 runs   (    0.75 ms per token,  1335.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6449.40 ms /  1179 tokens (    5.47 ms per token,   182.81 tokens per second)\n",
            "llama_print_timings:        eval time =      40.95 ms /     1 runs   (   40.95 ms per token,    24.42 tokens per second)\n",
            "llama_print_timings:       total time =    6509.34 ms /  1180 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1661.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     872.18 ms /   170 tokens (    5.13 ms per token,   194.92 tokens per second)\n",
            "llama_print_timings:        eval time =      38.77 ms /     1 runs   (   38.77 ms per token,    25.79 tokens per second)\n",
            "llama_print_timings:       total time =     916.37 ms /   171 tokens\n",
            "  9%|▊         | 3/35 [02:44<26:30, 49.71s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.46 ms /    18 runs   (    0.53 ms per token,  1903.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2228.45 ms /   434 tokens (    5.13 ms per token,   194.75 tokens per second)\n",
            "llama_print_timings:        eval time =     655.37 ms /    17 runs   (   38.55 ms per token,    25.94 tokens per second)\n",
            "llama_print_timings:       total time =    2907.40 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     111.66 ms /   203 runs   (    0.55 ms per token,  1818.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5745.79 ms /  1064 tokens (    5.40 ms per token,   185.18 tokens per second)\n",
            "llama_print_timings:        eval time =    8245.49 ms /   203 runs   (   40.62 ms per token,    24.62 tokens per second)\n",
            "llama_print_timings:       total time =   14247.41 ms /  1267 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      81.55 ms /   155 runs   (    0.53 ms per token,  1900.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5581.17 ms /  1032 tokens (    5.41 ms per token,   184.91 tokens per second)\n",
            "llama_print_timings:        eval time =    6255.25 ms /   154 runs   (   40.62 ms per token,    24.62 tokens per second)\n",
            "llama_print_timings:       total time =   12024.65 ms /  1186 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      38.79 ms /    65 runs   (    0.60 ms per token,  1675.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6423.48 ms /  1176 tokens (    5.46 ms per token,   183.08 tokens per second)\n",
            "llama_print_timings:        eval time =    2612.77 ms /    64 runs   (   40.82 ms per token,    24.50 tokens per second)\n",
            "llama_print_timings:       total time =    9126.74 ms /  1240 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.55 ms per token,  1831.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1198.48 ms /   235 tokens (    5.10 ms per token,   196.08 tokens per second)\n",
            "llama_print_timings:        eval time =      37.50 ms /     1 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =    1240.72 ms /   236 tokens\n",
            " 11%|█▏        | 4/35 [03:30<24:50, 48.07s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.93 ms /    18 runs   (    0.55 ms per token,  1812.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2230.06 ms /   434 tokens (    5.14 ms per token,   194.61 tokens per second)\n",
            "llama_print_timings:        eval time =     655.35 ms /    17 runs   (   38.55 ms per token,    25.94 tokens per second)\n",
            "llama_print_timings:       total time =    2909.23 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      48.22 ms /    92 runs   (    0.52 ms per token,  1908.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7799.06 ms /  1415 tokens (    5.51 ms per token,   181.43 tokens per second)\n",
            "llama_print_timings:        eval time =    3771.31 ms /    91 runs   (   41.44 ms per token,    24.13 tokens per second)\n",
            "llama_print_timings:       total time =   11686.92 ms /  1506 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      51.07 ms /    97 runs   (    0.53 ms per token,  1899.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7656.12 ms /  1382 tokens (    5.54 ms per token,   180.51 tokens per second)\n",
            "llama_print_timings:        eval time =    3980.57 ms /    96 runs   (   41.46 ms per token,    24.12 tokens per second)\n",
            "llama_print_timings:       total time =   11761.62 ms /  1478 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      42.90 ms /    76 runs   (    0.56 ms per token,  1771.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8208.02 ms /  1470 tokens (    5.58 ms per token,   179.09 tokens per second)\n",
            "llama_print_timings:        eval time =    3132.58 ms /    75 runs   (   41.77 ms per token,    23.94 tokens per second)\n",
            "llama_print_timings:       total time =   11443.60 ms /  1545 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1845.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     873.86 ms /   175 tokens (    4.99 ms per token,   200.26 tokens per second)\n",
            "llama_print_timings:        eval time =      38.17 ms /     1 runs   (   38.17 ms per token,    26.20 tokens per second)\n",
            "llama_print_timings:       total time =     915.84 ms /   176 tokens\n",
            " 14%|█▍        | 5/35 [04:13<23:17, 46.59s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.62 ms /    18 runs   (    0.53 ms per token,  1870.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.00 ms /   434 tokens (    5.15 ms per token,   194.27 tokens per second)\n",
            "llama_print_timings:        eval time =     654.32 ms /    17 runs   (   38.49 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2911.57 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     125.75 ms /   215 runs   (    0.58 ms per token,  1709.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10417.04 ms /  1832 tokens (    5.69 ms per token,   175.87 tokens per second)\n",
            "llama_print_timings:        eval time =    9279.17 ms /   215 runs   (   43.16 ms per token,    23.17 tokens per second)\n",
            "llama_print_timings:       total time =   20006.71 ms /  2047 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      46.32 ms /    87 runs   (    0.53 ms per token,  1878.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10270.72 ms /  1800 tokens (    5.71 ms per token,   175.26 tokens per second)\n",
            "llama_print_timings:        eval time =    3678.54 ms /    86 runs   (   42.77 ms per token,    23.38 tokens per second)\n",
            "llama_print_timings:       total time =   14075.52 ms /  1886 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      93.54 ms /   171 runs   (    0.55 ms per token,  1828.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10728.01 ms /  1869 tokens (    5.74 ms per token,   174.22 tokens per second)\n",
            "llama_print_timings:        eval time =    7318.71 ms /   170 runs   (   43.05 ms per token,    23.23 tokens per second)\n",
            "llama_print_timings:       total time =   18279.03 ms /  2039 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.52 ms /     2 runs   (    0.76 ms per token,  1313.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     886.99 ms /   174 tokens (    5.10 ms per token,   196.17 tokens per second)\n",
            "llama_print_timings:        eval time =      38.43 ms /     1 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =     932.17 ms /   175 tokens\n",
            " 17%|█▋        | 6/35 [05:14<24:51, 51.42s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.97 ms /    18 runs   (    0.67 ms per token,  1503.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2257.50 ms /   434 tokens (    5.20 ms per token,   192.25 tokens per second)\n",
            "llama_print_timings:        eval time =     659.16 ms /    17 runs   (   38.77 ms per token,    25.79 tokens per second)\n",
            "llama_print_timings:       total time =    2946.52 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      29.26 ms /    51 runs   (    0.57 ms per token,  1743.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12562.25 ms /  2152 tokens (    5.84 ms per token,   171.31 tokens per second)\n",
            "llama_print_timings:        eval time =    2188.26 ms /    50 runs   (   43.77 ms per token,    22.85 tokens per second)\n",
            "llama_print_timings:       total time =   14838.54 ms /  2202 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      24.13 ms /    46 runs   (    0.52 ms per token,  1906.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12372.16 ms /  2119 tokens (    5.84 ms per token,   171.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1967.39 ms /    45 runs   (   43.72 ms per token,    22.87 tokens per second)\n",
            "llama_print_timings:       total time =   14418.41 ms /  2164 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     151.32 ms /   256 runs   (    0.59 ms per token,  1691.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12627.00 ms /  2154 tokens (    5.86 ms per token,   170.59 tokens per second)\n",
            "llama_print_timings:        eval time =   11253.55 ms /   255 runs   (   44.13 ms per token,    22.66 tokens per second)\n",
            "llama_print_timings:       total time =   24288.65 ms /  2409 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1953.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     629.82 ms /   126 tokens (    5.00 ms per token,   200.06 tokens per second)\n",
            "llama_print_timings:        eval time =      37.49 ms /     1 runs   (   37.49 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =     671.12 ms /   127 tokens\n",
            " 20%|██        | 7/35 [06:15<25:21, 54.36s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.65 ms /    18 runs   (    0.54 ms per token,  1865.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2229.04 ms /   434 tokens (    5.14 ms per token,   194.70 tokens per second)\n",
            "llama_print_timings:        eval time =     653.59 ms /    17 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2905.23 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       0.90 ms /     2 runs   (    0.45 ms per token,  2232.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5673.43 ms /  1052 tokens (    5.39 ms per token,   185.43 tokens per second)\n",
            "llama_print_timings:        eval time =      39.51 ms /     1 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    5730.05 ms /  1053 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      93.54 ms /   151 runs   (    0.62 ms per token,  1614.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5451.68 ms /  1019 tokens (    5.35 ms per token,   186.91 tokens per second)\n",
            "llama_print_timings:        eval time =    6099.40 ms /   150 runs   (   40.66 ms per token,    24.59 tokens per second)\n",
            "llama_print_timings:       total time =   11774.02 ms /  1169 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1782.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6282.36 ms /  1154 tokens (    5.44 ms per token,   183.69 tokens per second)\n",
            "llama_print_timings:        eval time =      40.33 ms /     1 runs   (   40.33 ms per token,    24.79 tokens per second)\n",
            "llama_print_timings:       total time =    6338.31 ms /  1155 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1754.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1197.94 ms /   236 tokens (    5.08 ms per token,   197.01 tokens per second)\n",
            "llama_print_timings:        eval time =      37.24 ms /     1 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
            "llama_print_timings:       total time =    1240.44 ms /   237 tokens\n",
            " 23%|██▎       | 8/35 [06:50<21:41, 48.19s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.88 ms /    18 runs   (    0.66 ms per token,  1515.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2241.10 ms /   434 tokens (    5.16 ms per token,   193.65 tokens per second)\n",
            "llama_print_timings:        eval time =     670.80 ms /    17 runs   (   39.46 ms per token,    25.34 tokens per second)\n",
            "llama_print_timings:       total time =    2944.05 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1719.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4494.09 ms /   856 tokens (    5.25 ms per token,   190.47 tokens per second)\n",
            "llama_print_timings:        eval time =      78.78 ms /     2 runs   (   39.39 ms per token,    25.39 tokens per second)\n",
            "llama_print_timings:       total time =    4585.98 ms /   858 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      58.10 ms /    96 runs   (    0.61 ms per token,  1652.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4358.33 ms /   824 tokens (    5.29 ms per token,   189.06 tokens per second)\n",
            "llama_print_timings:        eval time =    3771.76 ms /    95 runs   (   39.70 ms per token,    25.19 tokens per second)\n",
            "llama_print_timings:       total time =    8257.22 ms /   919 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      58.57 ms /    91 runs   (    0.64 ms per token,  1553.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4831.62 ms /   907 tokens (    5.33 ms per token,   187.72 tokens per second)\n",
            "llama_print_timings:        eval time =    3619.10 ms /    90 runs   (   40.21 ms per token,    24.87 tokens per second)\n",
            "llama_print_timings:       total time =    8582.91 ms /   997 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.43 ms /     2 runs   (    0.72 ms per token,  1395.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =     927.55 ms /   178 tokens (    5.21 ms per token,   191.90 tokens per second)\n",
            "llama_print_timings:        eval time =      38.11 ms /     1 runs   (   38.11 ms per token,    26.24 tokens per second)\n",
            "llama_print_timings:       total time =     973.96 ms /   179 tokens\n",
            " 26%|██▌       | 9/35 [07:23<18:50, 43.49s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.58 ms /    18 runs   (    0.53 ms per token,  1878.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2264.53 ms /   434 tokens (    5.22 ms per token,   191.65 tokens per second)\n",
            "llama_print_timings:        eval time =     656.39 ms /    17 runs   (   38.61 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    2947.93 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     148.89 ms /   256 runs   (    0.58 ms per token,  1719.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10382.19 ms /  1822 tokens (    5.70 ms per token,   175.49 tokens per second)\n",
            "llama_print_timings:        eval time =   11034.78 ms /   255 runs   (   43.27 ms per token,    23.11 tokens per second)\n",
            "llama_print_timings:       total time =   21815.69 ms /  2077 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     127.49 ms /   240 runs   (    0.53 ms per token,  1882.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10257.27 ms /  1789 tokens (    5.73 ms per token,   174.41 tokens per second)\n",
            "llama_print_timings:        eval time =   10282.73 ms /   239 runs   (   43.02 ms per token,    23.24 tokens per second)\n",
            "llama_print_timings:       total time =   20854.04 ms /  2028 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     113.59 ms /   193 runs   (    0.59 ms per token,  1699.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11672.14 ms /  2011 tokens (    5.80 ms per token,   172.29 tokens per second)\n",
            "llama_print_timings:        eval time =    8376.81 ms /   192 runs   (   43.63 ms per token,    22.92 tokens per second)\n",
            "llama_print_timings:       total time =   20348.87 ms /  2203 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.51 ms per token,  1945.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1648.55 ms /   327 tokens (    5.04 ms per token,   198.36 tokens per second)\n",
            "llama_print_timings:        eval time =      37.55 ms /     1 runs   (   37.55 ms per token,    26.63 tokens per second)\n",
            "llama_print_timings:       total time =    1691.58 ms /   328 tokens\n",
            " 29%|██▊       | 10/35 [08:39<22:19, 53.60s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.58 ms /    18 runs   (    0.53 ms per token,  1878.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2230.46 ms /   434 tokens (    5.14 ms per token,   194.58 tokens per second)\n",
            "llama_print_timings:        eval time =     653.22 ms /    17 runs   (   38.42 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    2906.86 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      55.44 ms /    89 runs   (    0.62 ms per token,  1605.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12504.46 ms /  2152 tokens (    5.81 ms per token,   172.10 tokens per second)\n",
            "llama_print_timings:        eval time =    3920.38 ms /    89 runs   (   44.05 ms per token,    22.70 tokens per second)\n",
            "llama_print_timings:       total time =   16568.19 ms /  2241 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      41.75 ms /    74 runs   (    0.56 ms per token,  1772.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12428.55 ms /  2120 tokens (    5.86 ms per token,   170.57 tokens per second)\n",
            "llama_print_timings:        eval time =    3209.30 ms /    73 runs   (   43.96 ms per token,    22.75 tokens per second)\n",
            "llama_print_timings:       total time =   15748.46 ms /  2193 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      83.12 ms /   155 runs   (    0.54 ms per token,  1864.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12809.81 ms /  2179 tokens (    5.88 ms per token,   170.10 tokens per second)\n",
            "llama_print_timings:        eval time =    6768.13 ms /   154 runs   (   43.95 ms per token,    22.75 tokens per second)\n",
            "llama_print_timings:       total time =   19786.02 ms /  2333 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.35 ms /     2 runs   (    0.67 ms per token,  1482.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     794.72 ms /   158 tokens (    5.03 ms per token,   198.81 tokens per second)\n",
            "llama_print_timings:        eval time =      37.62 ms /     1 runs   (   37.62 ms per token,    26.58 tokens per second)\n",
            "llama_print_timings:       total time =     837.41 ms /   159 tokens\n",
            " 31%|███▏      | 11/35 [09:41<22:27, 56.16s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.93 ms /    18 runs   (    0.66 ms per token,  1508.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2257.25 ms /   434 tokens (    5.20 ms per token,   192.27 tokens per second)\n",
            "llama_print_timings:        eval time =     660.67 ms /    17 runs   (   38.86 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    2947.38 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     148.35 ms /   256 runs   (    0.58 ms per token,  1725.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14551.88 ms /  2448 tokens (    5.94 ms per token,   168.23 tokens per second)\n",
            "llama_print_timings:        eval time =   12552.50 ms /   256 runs   (   49.03 ms per token,    20.39 tokens per second)\n",
            "llama_print_timings:       total time =   27505.67 ms /  2704 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     137.77 ms /   256 runs   (    0.54 ms per token,  1858.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14434.87 ms /  2416 tokens (    5.97 ms per token,   167.37 tokens per second)\n",
            "llama_print_timings:        eval time =   12322.38 ms /   255 runs   (   48.32 ms per token,    20.69 tokens per second)\n",
            "llama_print_timings:       total time =   27146.30 ms /  2671 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     138.31 ms /   256 runs   (    0.54 ms per token,  1850.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16105.59 ms /  2663 tokens (    6.05 ms per token,   165.35 tokens per second)\n",
            "llama_print_timings:        eval time =   12746.83 ms /   255 runs   (   49.99 ms per token,    20.00 tokens per second)\n",
            "llama_print_timings:       total time =   29242.38 ms /  2918 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      34.33 ms /    63 runs   (    0.54 ms per token,  1835.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1691.92 ms /   336 tokens (    5.04 ms per token,   198.59 tokens per second)\n",
            "llama_print_timings:        eval time =    2372.57 ms /    62 runs   (   38.27 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =    4135.44 ms /   398 tokens\n",
            " 34%|███▍      | 12/35 [11:18<26:19, 68.65s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      12.56 ms /    18 runs   (    0.70 ms per token,  1432.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2233.66 ms /   434 tokens (    5.15 ms per token,   194.30 tokens per second)\n",
            "llama_print_timings:        eval time =     658.01 ms /    17 runs   (   38.71 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    2922.03 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     138.74 ms /   256 runs   (    0.54 ms per token,  1845.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14630.60 ms /  2451 tokens (    5.97 ms per token,   167.53 tokens per second)\n",
            "llama_print_timings:        eval time =   12599.69 ms /   255 runs   (   49.41 ms per token,    20.24 tokens per second)\n",
            "llama_print_timings:       total time =   27620.36 ms /  2706 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     138.97 ms /   256 runs   (    0.54 ms per token,  1842.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14488.40 ms /  2418 tokens (    5.99 ms per token,   166.89 tokens per second)\n",
            "llama_print_timings:        eval time =   12317.85 ms /   255 runs   (   48.31 ms per token,    20.70 tokens per second)\n",
            "llama_print_timings:       total time =   27189.14 ms /  2673 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     140.49 ms /   256 runs   (    0.55 ms per token,  1822.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16029.96 ms /  2656 tokens (    6.04 ms per token,   165.69 tokens per second)\n",
            "llama_print_timings:        eval time =   12778.30 ms /   256 runs   (   49.92 ms per token,    20.03 tokens per second)\n",
            "llama_print_timings:       total time =   29204.39 ms /  2912 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      27.20 ms /    52 runs   (    0.52 ms per token,  1911.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1730.37 ms /   338 tokens (    5.12 ms per token,   195.33 tokens per second)\n",
            "llama_print_timings:        eval time =    1951.50 ms /    51 runs   (   38.26 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =    3737.13 ms /   389 tokens\n",
            " 37%|███▋      | 13/35 [12:57<28:29, 77.72s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.00 ms /    18 runs   (    0.56 ms per token,  1800.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2228.07 ms /   434 tokens (    5.13 ms per token,   194.79 tokens per second)\n",
            "llama_print_timings:        eval time =     657.33 ms /    17 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    2910.19 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1514.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5405.08 ms /  1011 tokens (    5.35 ms per token,   187.05 tokens per second)\n",
            "llama_print_timings:        eval time =      39.75 ms /     1 runs   (   39.75 ms per token,    25.16 tokens per second)\n",
            "llama_print_timings:       total time =    5459.58 ms /  1012 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      31.08 ms /    55 runs   (    0.57 ms per token,  1769.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5261.06 ms /   978 tokens (    5.38 ms per token,   185.89 tokens per second)\n",
            "llama_print_timings:        eval time =    2164.47 ms /    54 runs   (   40.08 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    7498.21 ms /  1032 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      49.60 ms /    68 runs   (    0.73 ms per token,  1371.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5484.57 ms /  1022 tokens (    5.37 ms per token,   186.34 tokens per second)\n",
            "llama_print_timings:        eval time =    2741.32 ms /    67 runs   (   40.92 ms per token,    24.44 tokens per second)\n",
            "llama_print_timings:       total time =    8336.55 ms /  1089 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.66 ms per token,  1523.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     688.39 ms /   135 tokens (    5.10 ms per token,   196.11 tokens per second)\n",
            "llama_print_timings:        eval time =      37.81 ms /     1 runs   (   37.81 ms per token,    26.45 tokens per second)\n",
            "llama_print_timings:       total time =     730.79 ms /   136 tokens\n",
            " 40%|████      | 14/35 [13:31<22:35, 64.55s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.85 ms /    18 runs   (    0.55 ms per token,  1827.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2252.87 ms /   434 tokens (    5.19 ms per token,   192.64 tokens per second)\n",
            "llama_print_timings:        eval time =     661.22 ms /    17 runs   (   38.90 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    2937.59 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.54 ms per token,  1838.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4794.50 ms /   904 tokens (    5.30 ms per token,   188.55 tokens per second)\n",
            "llama_print_timings:        eval time =      79.77 ms /     2 runs   (   39.89 ms per token,    25.07 tokens per second)\n",
            "llama_print_timings:       total time =    4887.77 ms /   906 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     108.37 ms /   168 runs   (    0.65 ms per token,  1550.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4591.61 ms /   872 tokens (    5.27 ms per token,   189.91 tokens per second)\n",
            "llama_print_timings:        eval time =    6704.37 ms /   167 runs   (   40.15 ms per token,    24.91 tokens per second)\n",
            "llama_print_timings:       total time =   11548.92 ms /  1039 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      66.66 ms /   112 runs   (    0.60 ms per token,  1680.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5533.24 ms /  1031 tokens (    5.37 ms per token,   186.33 tokens per second)\n",
            "llama_print_timings:        eval time =    4514.31 ms /   111 runs   (   40.67 ms per token,    24.59 tokens per second)\n",
            "llama_print_timings:       total time =   10195.17 ms /  1142 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      94.79 ms /   160 runs   (    0.59 ms per token,  1687.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1260.31 ms /   246 tokens (    5.12 ms per token,   195.19 tokens per second)\n",
            "llama_print_timings:        eval time =    6105.73 ms /   159 runs   (   38.40 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    7560.51 ms /   405 tokens\n",
            " 43%|████▎     | 15/35 [14:18<19:43, 59.17s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.72 ms /    18 runs   (    0.54 ms per token,  1852.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2228.33 ms /   434 tokens (    5.13 ms per token,   194.77 tokens per second)\n",
            "llama_print_timings:        eval time =     654.18 ms /    17 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2906.56 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      53.44 ms /    81 runs   (    0.66 ms per token,  1515.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4663.92 ms /   886 tokens (    5.26 ms per token,   189.97 tokens per second)\n",
            "llama_print_timings:        eval time =    3192.81 ms /    80 runs   (   39.91 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =    7972.88 ms /   966 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     154.75 ms /   256 runs   (    0.60 ms per token,  1654.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4530.50 ms /   853 tokens (    5.31 ms per token,   188.28 tokens per second)\n",
            "llama_print_timings:        eval time =   10239.19 ms /   255 runs   (   40.15 ms per token,    24.90 tokens per second)\n",
            "llama_print_timings:       total time =   15142.64 ms /  1108 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     160.92 ms /   256 runs   (    0.63 ms per token,  1590.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5893.32 ms /  1088 tokens (    5.42 ms per token,   184.62 tokens per second)\n",
            "llama_print_timings:        eval time =   10462.68 ms /   255 runs   (   41.03 ms per token,    24.37 tokens per second)\n",
            "llama_print_timings:       total time =   16741.53 ms /  1343 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1885.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1774.30 ms /   348 tokens (    5.10 ms per token,   196.13 tokens per second)\n",
            "llama_print_timings:        eval time =      37.67 ms /     1 runs   (   37.67 ms per token,    26.54 tokens per second)\n",
            "llama_print_timings:       total time =    1819.23 ms /   349 tokens\n",
            " 46%|████▌     | 16/35 [15:11<18:11, 57.47s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.31 ms /    18 runs   (    0.52 ms per token,  1933.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.70 ms /   434 tokens (    5.14 ms per token,   194.47 tokens per second)\n",
            "llama_print_timings:        eval time =     654.97 ms /    17 runs   (   38.53 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    2909.55 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1718.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6575.71 ms /  1208 tokens (    5.44 ms per token,   183.71 tokens per second)\n",
            "llama_print_timings:        eval time =      40.05 ms /     1 runs   (   40.05 ms per token,    24.97 tokens per second)\n",
            "llama_print_timings:       total time =    6634.01 ms /  1209 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     161.56 ms /   256 runs   (    0.63 ms per token,  1584.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6340.13 ms /  1175 tokens (    5.40 ms per token,   185.33 tokens per second)\n",
            "llama_print_timings:        eval time =   10486.64 ms /   255 runs   (   41.12 ms per token,    24.32 tokens per second)\n",
            "llama_print_timings:       total time =   17198.24 ms /  1430 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     146.76 ms /   256 runs   (    0.57 ms per token,  1744.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7844.21 ms /  1416 tokens (    5.54 ms per token,   180.52 tokens per second)\n",
            "llama_print_timings:        eval time =   10676.30 ms /   255 runs   (   41.87 ms per token,    23.88 tokens per second)\n",
            "llama_print_timings:       total time =   18868.25 ms /  1671 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.38 ms /     2 runs   (    0.69 ms per token,  1446.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1750.32 ms /   342 tokens (    5.12 ms per token,   195.39 tokens per second)\n",
            "llama_print_timings:        eval time =      38.16 ms /     1 runs   (   38.16 ms per token,    26.21 tokens per second)\n",
            "llama_print_timings:       total time =    1795.60 ms /   343 tokens\n",
            " 49%|████▊     | 17/35 [16:03<16:44, 55.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    18 runs   (    0.52 ms per token,  1930.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2256.33 ms /   434 tokens (    5.20 ms per token,   192.35 tokens per second)\n",
            "llama_print_timings:        eval time =     654.33 ms /    17 runs   (   38.49 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2934.33 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1509.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4605.61 ms /   866 tokens (    5.32 ms per token,   188.03 tokens per second)\n",
            "llama_print_timings:        eval time =      39.57 ms /     1 runs   (   39.57 ms per token,    25.27 tokens per second)\n",
            "llama_print_timings:       total time =    4658.90 ms /   867 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      56.58 ms /   101 runs   (    0.56 ms per token,  1785.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4393.69 ms /   832 tokens (    5.28 ms per token,   189.36 tokens per second)\n",
            "llama_print_timings:        eval time =    4002.02 ms /   101 runs   (   39.62 ms per token,    25.24 tokens per second)\n",
            "llama_print_timings:       total time =    8517.30 ms /   933 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      39.44 ms /    52 runs   (    0.76 ms per token,  1318.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4983.19 ms /   931 tokens (    5.35 ms per token,   186.83 tokens per second)\n",
            "llama_print_timings:        eval time =    2062.02 ms /    51 runs   (   40.43 ms per token,    24.73 tokens per second)\n",
            "llama_print_timings:       total time =    7132.20 ms /   982 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1670.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =     880.13 ms /   173 tokens (    5.09 ms per token,   196.56 tokens per second)\n",
            "llama_print_timings:        eval time =      38.94 ms /     1 runs   (   38.94 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =     923.52 ms /   174 tokens\n",
            " 51%|█████▏    | 18/35 [16:34<13:40, 48.27s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.74 ms /    18 runs   (    0.54 ms per token,  1848.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2240.05 ms /   434 tokens (    5.16 ms per token,   193.75 tokens per second)\n",
            "llama_print_timings:        eval time =     657.52 ms /    17 runs   (   38.68 ms per token,    25.85 tokens per second)\n",
            "llama_print_timings:       total time =    2920.66 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.38 ms /     2 runs   (    0.69 ms per token,  1452.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3511.45 ms /   666 tokens (    5.27 ms per token,   189.67 tokens per second)\n",
            "llama_print_timings:        eval time =      38.64 ms /     1 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    3562.07 ms /   667 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      36.50 ms /    68 runs   (    0.54 ms per token,  1862.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3267.51 ms /   632 tokens (    5.17 ms per token,   193.42 tokens per second)\n",
            "llama_print_timings:        eval time =    2678.76 ms /    68 runs   (   39.39 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    6027.35 ms /   700 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      55.25 ms /    89 runs   (    0.62 ms per token,  1610.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3650.57 ms /   698 tokens (    5.23 ms per token,   191.20 tokens per second)\n",
            "llama_print_timings:        eval time =    3489.63 ms /    88 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    7269.39 ms /   786 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.23 ms /     2 runs   (    0.62 ms per token,  1623.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     725.03 ms /   140 tokens (    5.18 ms per token,   193.09 tokens per second)\n",
            "llama_print_timings:        eval time =      38.11 ms /     1 runs   (   38.11 ms per token,    26.24 tokens per second)\n",
            "llama_print_timings:       total time =     768.69 ms /   141 tokens\n",
            " 54%|█████▍    | 19/35 [17:01<11:11, 41.95s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.48 ms /    18 runs   (    0.53 ms per token,  1899.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2253.41 ms /   434 tokens (    5.19 ms per token,   192.60 tokens per second)\n",
            "llama_print_timings:        eval time =     656.28 ms /    17 runs   (   38.60 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    2937.22 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.42 ms /     2 runs   (    0.71 ms per token,  1410.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4524.03 ms /   852 tokens (    5.31 ms per token,   188.33 tokens per second)\n",
            "llama_print_timings:        eval time =      39.12 ms /     1 runs   (   39.12 ms per token,    25.56 tokens per second)\n",
            "llama_print_timings:       total time =    4576.70 ms /   853 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      72.42 ms /   126 runs   (    0.57 ms per token,  1739.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4357.44 ms /   819 tokens (    5.32 ms per token,   187.95 tokens per second)\n",
            "llama_print_timings:        eval time =    4944.58 ms /   125 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    9456.09 ms /   944 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      64.34 ms /    99 runs   (    0.65 ms per token,  1538.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5037.08 ms /   940 tokens (    5.36 ms per token,   186.62 tokens per second)\n",
            "llama_print_timings:        eval time =    3959.68 ms /    98 runs   (   40.40 ms per token,    24.75 tokens per second)\n",
            "llama_print_timings:       total time =    9138.23 ms /  1038 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.57 ms per token,  1742.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     994.58 ms /   200 tokens (    4.97 ms per token,   201.09 tokens per second)\n",
            "llama_print_timings:        eval time =      37.23 ms /     1 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
            "llama_print_timings:       total time =    1036.30 ms /   201 tokens\n",
            " 57%|█████▋    | 20/35 [17:34<09:49, 39.27s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.58 ms /    18 runs   (    0.53 ms per token,  1879.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2237.44 ms /   434 tokens (    5.16 ms per token,   193.97 tokens per second)\n",
            "llama_print_timings:        eval time =     657.25 ms /    17 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    2918.49 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.50 ms per token,  1986.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7694.88 ms /  1394 tokens (    5.52 ms per token,   181.16 tokens per second)\n",
            "llama_print_timings:        eval time =      40.93 ms /     1 runs   (   40.93 ms per token,    24.43 tokens per second)\n",
            "llama_print_timings:       total time =    7754.35 ms /  1395 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      19.09 ms /    38 runs   (    0.50 ms per token,  1990.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7525.39 ms /  1360 tokens (    5.53 ms per token,   180.72 tokens per second)\n",
            "llama_print_timings:        eval time =    1574.65 ms /    38 runs   (   41.44 ms per token,    24.13 tokens per second)\n",
            "llama_print_timings:       total time =    9160.44 ms /  1398 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     134.16 ms /   236 runs   (    0.57 ms per token,  1759.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7726.10 ms /  1398 tokens (    5.53 ms per token,   180.95 tokens per second)\n",
            "llama_print_timings:        eval time =    9870.93 ms /   235 runs   (   42.00 ms per token,    23.81 tokens per second)\n",
            "llama_print_timings:       total time =   17922.45 ms /  1633 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1953.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     551.79 ms /   108 tokens (    5.11 ms per token,   195.73 tokens per second)\n",
            "llama_print_timings:        eval time =      36.86 ms /     1 runs   (   36.86 ms per token,    27.13 tokens per second)\n",
            "llama_print_timings:       total time =     592.24 ms /   109 tokens\n",
            " 60%|██████    | 21/35 [18:19<09:31, 40.83s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.53 ms /    18 runs   (    0.53 ms per token,  1888.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2233.29 ms /   434 tokens (    5.15 ms per token,   194.33 tokens per second)\n",
            "llama_print_timings:        eval time =     654.81 ms /    17 runs   (   38.52 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    2911.35 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1777.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4135.47 ms /   790 tokens (    5.23 ms per token,   191.03 tokens per second)\n",
            "llama_print_timings:        eval time =      39.13 ms /     1 runs   (   39.13 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    4185.38 ms /   791 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      68.93 ms /   101 runs   (    0.68 ms per token,  1465.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3963.45 ms /   757 tokens (    5.24 ms per token,   191.00 tokens per second)\n",
            "llama_print_timings:        eval time =    3974.45 ms /   100 runs   (   39.74 ms per token,    25.16 tokens per second)\n",
            "llama_print_timings:       total time =    8088.38 ms /   857 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      40.80 ms /    74 runs   (    0.55 ms per token,  1813.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4521.67 ms /   852 tokens (    5.31 ms per token,   188.43 tokens per second)\n",
            "llama_print_timings:        eval time =    2885.80 ms /    73 runs   (   39.53 ms per token,    25.30 tokens per second)\n",
            "llama_print_timings:       total time =    7495.89 ms /   925 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1768.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     871.06 ms /   176 tokens (    4.95 ms per token,   202.05 tokens per second)\n",
            "llama_print_timings:        eval time =      38.21 ms /     1 runs   (   38.21 ms per token,    26.17 tokens per second)\n",
            "llama_print_timings:       total time =     918.42 ms /   177 tokens\n",
            " 63%|██████▎   | 22/35 [18:47<08:01, 37.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.15 ms /    18 runs   (    0.56 ms per token,  1773.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.51 ms /   434 tokens (    5.15 ms per token,   194.23 tokens per second)\n",
            "llama_print_timings:        eval time =     658.95 ms /    17 runs   (   38.76 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =    2917.72 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1669.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5272.38 ms /   988 tokens (    5.34 ms per token,   187.39 tokens per second)\n",
            "llama_print_timings:        eval time =      39.52 ms /     1 runs   (   39.52 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    5326.52 ms /   989 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      15.36 ms /    22 runs   (    0.70 ms per token,  1432.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5113.80 ms /   955 tokens (    5.35 ms per token,   186.75 tokens per second)\n",
            "llama_print_timings:        eval time =     849.46 ms /    21 runs   (   40.45 ms per token,    24.72 tokens per second)\n",
            "llama_print_timings:       total time =    6007.99 ms /   976 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1846.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5219.76 ms /   974 tokens (    5.36 ms per token,   186.60 tokens per second)\n",
            "llama_print_timings:        eval time =      39.29 ms /     1 runs   (   39.29 ms per token,    25.45 tokens per second)\n",
            "llama_print_timings:       total time =    5275.98 ms /   975 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.00 ms /     2 runs   (    0.50 ms per token,  2008.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     470.22 ms /    94 tokens (    5.00 ms per token,   199.90 tokens per second)\n",
            "llama_print_timings:        eval time =      37.55 ms /     1 runs   (   37.55 ms per token,    26.63 tokens per second)\n",
            "llama_print_timings:       total time =     511.79 ms /    95 tokens\n",
            " 66%|██████▌   | 23/35 [19:12<06:40, 33.41s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.25 ms /    18 runs   (    0.57 ms per token,  1756.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2233.15 ms /   434 tokens (    5.15 ms per token,   194.34 tokens per second)\n",
            "llama_print_timings:        eval time =     654.25 ms /    17 runs   (   38.49 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2911.09 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1855.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4320.10 ms /   810 tokens (    5.33 ms per token,   187.50 tokens per second)\n",
            "llama_print_timings:        eval time =      38.99 ms /     1 runs   (   38.99 ms per token,    25.65 tokens per second)\n",
            "llama_print_timings:       total time =    4376.82 ms /   811 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      84.41 ms /   148 runs   (    0.57 ms per token,  1753.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4058.43 ms /   776 tokens (    5.23 ms per token,   191.21 tokens per second)\n",
            "llama_print_timings:        eval time =    5872.45 ms /   148 runs   (   39.68 ms per token,    25.20 tokens per second)\n",
            "llama_print_timings:       total time =   10118.00 ms /   924 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      53.38 ms /    93 runs   (    0.57 ms per token,  1742.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4911.19 ms /   920 tokens (    5.34 ms per token,   187.33 tokens per second)\n",
            "llama_print_timings:        eval time =    3716.52 ms /    93 runs   (   39.96 ms per token,    25.02 tokens per second)\n",
            "llama_print_timings:       total time =    8743.36 ms /  1013 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.19 ms /     2 runs   (    0.59 ms per token,  1683.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1115.52 ms /   221 tokens (    5.05 ms per token,   198.11 tokens per second)\n",
            "llama_print_timings:        eval time =      37.24 ms /     1 runs   (   37.24 ms per token,    26.86 tokens per second)\n",
            "llama_print_timings:       total time =    1157.89 ms /   222 tokens\n",
            " 69%|██████▊   | 24/35 [19:43<05:59, 32.70s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.42 ms /    18 runs   (    0.52 ms per token,  1911.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.63 ms /   434 tokens (    5.14 ms per token,   194.48 tokens per second)\n",
            "llama_print_timings:        eval time =     653.98 ms /    17 runs   (   38.47 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2907.84 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      68.47 ms /   103 runs   (    0.66 ms per token,  1504.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7917.53 ms /  1434 tokens (    5.52 ms per token,   181.12 tokens per second)\n",
            "llama_print_timings:        eval time =    4272.10 ms /   102 runs   (   41.88 ms per token,    23.88 tokens per second)\n",
            "llama_print_timings:       total time =   12357.56 ms /  1536 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     102.56 ms /   169 runs   (    0.61 ms per token,  1647.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7710.38 ms /  1400 tokens (    5.51 ms per token,   181.57 tokens per second)\n",
            "llama_print_timings:        eval time =    7099.82 ms /   169 runs   (   42.01 ms per token,    23.80 tokens per second)\n",
            "llama_print_timings:       total time =   15061.27 ms /  1569 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      75.20 ms /   108 runs   (    0.70 ms per token,  1436.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8712.44 ms /  1557 tokens (    5.60 ms per token,   178.71 tokens per second)\n",
            "llama_print_timings:        eval time =    4543.43 ms /   107 runs   (   42.46 ms per token,    23.55 tokens per second)\n",
            "llama_print_timings:       total time =   13433.53 ms /  1664 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.55 ms per token,  1831.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1276.30 ms /   251 tokens (    5.08 ms per token,   196.66 tokens per second)\n",
            "llama_print_timings:        eval time =      37.34 ms /     1 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
            "llama_print_timings:       total time =    1318.82 ms /   252 tokens\n",
            " 71%|███████▏  | 25/35 [20:34<06:21, 38.20s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.49 ms /    18 runs   (    0.53 ms per token,  1896.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2235.09 ms /   434 tokens (    5.15 ms per token,   194.18 tokens per second)\n",
            "llama_print_timings:        eval time =     657.23 ms /    17 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    2916.27 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     135.12 ms /   238 runs   (    0.57 ms per token,  1761.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7655.04 ms /  1383 tokens (    5.54 ms per token,   180.67 tokens per second)\n",
            "llama_print_timings:        eval time =    9867.01 ms /   237 runs   (   41.63 ms per token,    24.02 tokens per second)\n",
            "llama_print_timings:       total time =   17840.95 ms /  1620 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.33 ms /    86 runs   (    0.50 ms per token,  1984.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7457.70 ms /  1350 tokens (    5.52 ms per token,   181.02 tokens per second)\n",
            "llama_print_timings:        eval time =    3509.80 ms /    85 runs   (   41.29 ms per token,    24.22 tokens per second)\n",
            "llama_print_timings:       total time =   11073.57 ms /  1435 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1784.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7894.35 ms /  1423 tokens (    5.55 ms per token,   180.26 tokens per second)\n",
            "llama_print_timings:        eval time =      41.03 ms /     1 runs   (   41.03 ms per token,    24.37 tokens per second)\n",
            "llama_print_timings:       total time =    7955.88 ms /  1424 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1816.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     830.78 ms /   168 tokens (    4.95 ms per token,   202.22 tokens per second)\n",
            "llama_print_timings:        eval time =      37.49 ms /     1 runs   (   37.49 ms per token,    26.68 tokens per second)\n",
            "llama_print_timings:       total time =     871.59 ms /   169 tokens\n",
            " 74%|███████▍  | 26/35 [21:16<05:55, 39.53s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.55 ms /    18 runs   (    0.53 ms per token,  1884.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.97 ms /   434 tokens (    5.14 ms per token,   194.45 tokens per second)\n",
            "llama_print_timings:        eval time =     654.08 ms /    17 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2909.71 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1703.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5307.09 ms /   992 tokens (    5.35 ms per token,   186.92 tokens per second)\n",
            "llama_print_timings:        eval time =      39.71 ms /     1 runs   (   39.71 ms per token,    25.18 tokens per second)\n",
            "llama_print_timings:       total time =    5363.81 ms /   993 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      48.58 ms /    86 runs   (    0.56 ms per token,  1770.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5091.60 ms /   959 tokens (    5.31 ms per token,   188.35 tokens per second)\n",
            "llama_print_timings:        eval time =    3403.07 ms /    85 runs   (   40.04 ms per token,    24.98 tokens per second)\n",
            "llama_print_timings:       total time =    8600.26 ms /  1044 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      30.24 ms /    54 runs   (    0.56 ms per token,  1785.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5675.82 ms /  1046 tokens (    5.43 ms per token,   184.29 tokens per second)\n",
            "llama_print_timings:        eval time =    2145.82 ms /    53 runs   (   40.49 ms per token,    24.70 tokens per second)\n",
            "llama_print_timings:       total time =    7896.96 ms /  1099 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1813.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     793.16 ms /   154 tokens (    5.15 ms per token,   194.16 tokens per second)\n",
            "llama_print_timings:        eval time =      37.35 ms /     1 runs   (   37.35 ms per token,    26.78 tokens per second)\n",
            "llama_print_timings:       total time =     835.21 ms /   155 tokens\n",
            " 77%|███████▋  | 27/35 [21:48<04:56, 37.08s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.45 ms /    18 runs   (    0.52 ms per token,  1905.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2238.23 ms /   434 tokens (    5.16 ms per token,   193.90 tokens per second)\n",
            "llama_print_timings:        eval time =     655.84 ms /    17 runs   (   38.58 ms per token,    25.92 tokens per second)\n",
            "llama_print_timings:       total time =    2916.63 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1814.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4234.45 ms /   796 tokens (    5.32 ms per token,   187.98 tokens per second)\n",
            "llama_print_timings:        eval time =      38.64 ms /     1 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    4287.35 ms /   797 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      74.92 ms /   132 runs   (    0.57 ms per token,  1761.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4013.32 ms /   763 tokens (    5.26 ms per token,   190.12 tokens per second)\n",
            "llama_print_timings:        eval time =    5181.89 ms /   131 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    9356.11 ms /   894 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      47.23 ms /    82 runs   (    0.58 ms per token,  1736.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4784.89 ms /   893 tokens (    5.36 ms per token,   186.63 tokens per second)\n",
            "llama_print_timings:        eval time =    3223.56 ms /    81 runs   (   39.80 ms per token,    25.13 tokens per second)\n",
            "llama_print_timings:       total time =    8116.27 ms /   974 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.58 ms per token,  1712.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1034.21 ms /   203 tokens (    5.09 ms per token,   196.28 tokens per second)\n",
            "llama_print_timings:        eval time =      37.28 ms /     1 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
            "llama_print_timings:       total time =    1076.04 ms /   204 tokens\n",
            " 80%|████████  | 28/35 [22:17<04:02, 34.67s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.79 ms /    18 runs   (    0.54 ms per token,  1839.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2229.91 ms /   434 tokens (    5.14 ms per token,   194.63 tokens per second)\n",
            "llama_print_timings:        eval time =     655.42 ms /    17 runs   (   38.55 ms per token,    25.94 tokens per second)\n",
            "llama_print_timings:       total time =    2907.85 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1798.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5409.36 ms /  1016 tokens (    5.32 ms per token,   187.82 tokens per second)\n",
            "llama_print_timings:        eval time =      39.50 ms /     1 runs   (   39.50 ms per token,    25.32 tokens per second)\n",
            "llama_print_timings:       total time =    5462.65 ms /  1017 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     105.04 ms /   165 runs   (    0.64 ms per token,  1570.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5236.87 ms /   983 tokens (    5.33 ms per token,   187.71 tokens per second)\n",
            "llama_print_timings:        eval time =    6639.94 ms /   164 runs   (   40.49 ms per token,    24.70 tokens per second)\n",
            "llama_print_timings:       total time =   12108.20 ms /  1147 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      52.42 ms /    75 runs   (    0.70 ms per token,  1430.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6082.38 ms /  1127 tokens (    5.40 ms per token,   185.29 tokens per second)\n",
            "llama_print_timings:        eval time =    3035.75 ms /    74 runs   (   41.02 ms per token,    24.38 tokens per second)\n",
            "llama_print_timings:       total time =    9240.38 ms /  1201 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.43 ms /     2 runs   (    0.71 ms per token,  1400.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1298.74 ms /   255 tokens (    5.09 ms per token,   196.34 tokens per second)\n",
            "llama_print_timings:        eval time =      37.69 ms /     1 runs   (   37.69 ms per token,    26.53 tokens per second)\n",
            "llama_print_timings:       total time =    1343.57 ms /   256 tokens\n",
            " 83%|████████▎ | 29/35 [22:52<03:29, 34.96s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.70 ms /    18 runs   (    0.54 ms per token,  1855.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2236.63 ms /   434 tokens (    5.15 ms per token,   194.04 tokens per second)\n",
            "llama_print_timings:        eval time =     656.21 ms /    17 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2915.03 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      47.49 ms /    84 runs   (    0.57 ms per token,  1768.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7935.36 ms /  1426 tokens (    5.56 ms per token,   179.70 tokens per second)\n",
            "llama_print_timings:        eval time =    3446.15 ms /    83 runs   (   41.52 ms per token,    24.08 tokens per second)\n",
            "llama_print_timings:       total time =   11495.05 ms /  1509 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     133.14 ms /   249 runs   (    0.53 ms per token,  1870.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7706.74 ms /  1392 tokens (    5.54 ms per token,   180.62 tokens per second)\n",
            "llama_print_timings:        eval time =   10409.29 ms /   249 runs   (   41.80 ms per token,    23.92 tokens per second)\n",
            "llama_print_timings:       total time =   18442.92 ms /  1641 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      49.21 ms /    89 runs   (    0.55 ms per token,  1808.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9199.74 ms /  1627 tokens (    5.65 ms per token,   176.85 tokens per second)\n",
            "llama_print_timings:        eval time =    3716.71 ms /    88 runs   (   42.24 ms per token,    23.68 tokens per second)\n",
            "llama_print_timings:       total time =   13037.19 ms /  1715 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.40 ms /     2 runs   (    0.70 ms per token,  1431.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1701.29 ms /   333 tokens (    5.11 ms per token,   195.73 tokens per second)\n",
            "llama_print_timings:        eval time =      38.49 ms /     1 runs   (   38.49 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    1746.05 ms /   334 tokens\n",
            " 86%|████████▌ | 30/35 [23:46<03:22, 40.54s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      12.26 ms /    18 runs   (    0.68 ms per token,  1468.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2260.72 ms /   434 tokens (    5.21 ms per token,   191.97 tokens per second)\n",
            "llama_print_timings:        eval time =     662.56 ms /    17 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    2956.45 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.55 ms per token,  1831.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5178.72 ms /   976 tokens (    5.31 ms per token,   188.46 tokens per second)\n",
            "llama_print_timings:        eval time =      39.61 ms /     1 runs   (   39.61 ms per token,    25.24 tokens per second)\n",
            "llama_print_timings:       total time =    5231.41 ms /   977 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      40.94 ms /    73 runs   (    0.56 ms per token,  1783.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5050.63 ms /   943 tokens (    5.36 ms per token,   186.71 tokens per second)\n",
            "llama_print_timings:        eval time =    2876.61 ms /    72 runs   (   39.95 ms per token,    25.03 tokens per second)\n",
            "llama_print_timings:       total time =    8022.77 ms /  1015 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.57 ms per token,  1740.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5369.52 ms /  1002 tokens (    5.36 ms per token,   186.61 tokens per second)\n",
            "llama_print_timings:        eval time =      39.53 ms /     1 runs   (   39.53 ms per token,    25.30 tokens per second)\n",
            "llama_print_timings:       total time =    5422.94 ms /  1003 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1869.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     790.77 ms /   156 tokens (    5.07 ms per token,   197.27 tokens per second)\n",
            "llama_print_timings:        eval time =      37.06 ms /     1 runs   (   37.06 ms per token,    26.98 tokens per second)\n",
            "llama_print_timings:       total time =     832.45 ms /   157 tokens\n",
            " 89%|████████▊ | 31/35 [24:13<02:25, 36.44s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.83 ms /    18 runs   (    0.66 ms per token,  1521.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2252.18 ms /   434 tokens (    5.19 ms per token,   192.70 tokens per second)\n",
            "llama_print_timings:        eval time =     662.14 ms /    17 runs   (   38.95 ms per token,    25.67 tokens per second)\n",
            "llama_print_timings:       total time =    2945.51 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       0.93 ms /     2 runs   (    0.47 ms per token,  2141.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5495.29 ms /  1026 tokens (    5.36 ms per token,   186.71 tokens per second)\n",
            "llama_print_timings:        eval time =      39.60 ms /     1 runs   (   39.60 ms per token,    25.25 tokens per second)\n",
            "llama_print_timings:       total time =    5548.92 ms /  1027 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      62.25 ms /   109 runs   (    0.57 ms per token,  1750.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5327.19 ms /   992 tokens (    5.37 ms per token,   186.21 tokens per second)\n",
            "llama_print_timings:        eval time =    4393.48 ms /   109 runs   (   40.31 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =    9857.63 ms /  1101 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.71 ms per token,  1417.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5867.43 ms /  1088 tokens (    5.39 ms per token,   185.43 tokens per second)\n",
            "llama_print_timings:        eval time =      40.59 ms /     1 runs   (   40.59 ms per token,    24.64 tokens per second)\n",
            "llama_print_timings:       total time =    5923.34 ms /  1089 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.44 ms /     2 runs   (    0.72 ms per token,  1391.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =     966.14 ms /   192 tokens (    5.03 ms per token,   198.73 tokens per second)\n",
            "llama_print_timings:        eval time =      38.09 ms /     1 runs   (   38.09 ms per token,    26.25 tokens per second)\n",
            "llama_print_timings:       total time =    1011.68 ms /   193 tokens\n",
            " 91%|█████████▏| 32/35 [24:44<01:44, 34.84s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.59 ms /    18 runs   (    0.64 ms per token,  1552.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2274.55 ms /   434 tokens (    5.24 ms per token,   190.81 tokens per second)\n",
            "llama_print_timings:        eval time =     658.93 ms /    17 runs   (   38.76 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =    2962.14 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     138.28 ms /   256 runs   (    0.54 ms per token,  1851.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12756.53 ms /  2175 tokens (    5.87 ms per token,   170.50 tokens per second)\n",
            "llama_print_timings:        eval time =   11265.14 ms /   255 runs   (   44.18 ms per token,    22.64 tokens per second)\n",
            "llama_print_timings:       total time =   24397.92 ms /  2430 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      57.15 ms /    85 runs   (    0.67 ms per token,  1487.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12502.13 ms /  2142 tokens (    5.84 ms per token,   171.33 tokens per second)\n",
            "llama_print_timings:        eval time =    3702.61 ms /    84 runs   (   44.08 ms per token,    22.69 tokens per second)\n",
            "llama_print_timings:       total time =   16357.29 ms /  2226 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     140.13 ms /   256 runs   (    0.55 ms per token,  1826.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12989.25 ms /  2210 tokens (    5.88 ms per token,   170.14 tokens per second)\n",
            "llama_print_timings:        eval time =   11274.30 ms /   255 runs   (   44.21 ms per token,    22.62 tokens per second)\n",
            "llama_print_timings:       total time =   24630.29 ms /  2465 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.19 ms /     2 runs   (    0.60 ms per token,  1676.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     887.56 ms /   171 tokens (    5.19 ms per token,   192.66 tokens per second)\n",
            "llama_print_timings:        eval time =      38.74 ms /     1 runs   (   38.74 ms per token,    25.81 tokens per second)\n",
            "llama_print_timings:       total time =     931.40 ms /   172 tokens\n",
            " 94%|█████████▍| 33/35 [25:56<01:31, 45.97s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.28 ms /    18 runs   (    0.57 ms per token,  1751.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2256.18 ms /   434 tokens (    5.20 ms per token,   192.36 tokens per second)\n",
            "llama_print_timings:        eval time =     659.20 ms /    17 runs   (   38.78 ms per token,    25.79 tokens per second)\n",
            "llama_print_timings:       total time =    2940.89 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1962.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4896.01 ms /   922 tokens (    5.31 ms per token,   188.32 tokens per second)\n",
            "llama_print_timings:        eval time =      39.30 ms /     1 runs   (   39.30 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    4947.97 ms /   923 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     109.55 ms /   198 runs   (    0.55 ms per token,  1807.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4731.73 ms /   888 tokens (    5.33 ms per token,   187.67 tokens per second)\n",
            "llama_print_timings:        eval time =    7916.93 ms /   198 runs   (   39.98 ms per token,    25.01 tokens per second)\n",
            "llama_print_timings:       total time =   12892.15 ms /  1086 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1762.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5847.85 ms /  1075 tokens (    5.44 ms per token,   183.83 tokens per second)\n",
            "llama_print_timings:        eval time =      39.60 ms /     1 runs   (   39.60 ms per token,    25.25 tokens per second)\n",
            "llama_print_timings:       total time =    5904.37 ms /  1076 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      24.44 ms /    44 runs   (    0.56 ms per token,  1800.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1401.75 ms /   279 tokens (    5.02 ms per token,   199.04 tokens per second)\n",
            "llama_print_timings:        eval time =    1644.48 ms /    43 runs   (   38.24 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    3094.09 ms /   322 tokens\n",
            " 97%|█████████▋| 34/35 [26:30<00:42, 42.49s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.85 ms /    18 runs   (    0.55 ms per token,  1827.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.42 ms /   434 tokens (    5.15 ms per token,   194.23 tokens per second)\n",
            "llama_print_timings:        eval time =     660.13 ms /    17 runs   (   38.83 ms per token,    25.75 tokens per second)\n",
            "llama_print_timings:       total time =    2917.35 ms /   451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1846.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5473.59 ms /  1013 tokens (    5.40 ms per token,   185.07 tokens per second)\n",
            "llama_print_timings:        eval time =      39.71 ms /     1 runs   (   39.71 ms per token,    25.18 tokens per second)\n",
            "llama_print_timings:       total time =    5529.38 ms /  1014 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     119.49 ms /   185 runs   (    0.65 ms per token,  1548.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5234.19 ms /   980 tokens (    5.34 ms per token,   187.23 tokens per second)\n",
            "llama_print_timings:        eval time =    7457.19 ms /   184 runs   (   40.53 ms per token,    24.67 tokens per second)\n",
            "llama_print_timings:       total time =   12964.30 ms /  1164 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     163.94 ms /   256 runs   (    0.64 ms per token,  1561.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6181.69 ms /  1144 tokens (    5.40 ms per token,   185.06 tokens per second)\n",
            "llama_print_timings:        eval time =   10470.49 ms /   255 runs   (   41.06 ms per token,    24.35 tokens per second)\n",
            "llama_print_timings:       total time =   17035.61 ms /  1399 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1727.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1401.55 ms /   275 tokens (    5.10 ms per token,   196.21 tokens per second)\n",
            "llama_print_timings:        eval time =      37.46 ms /     1 runs   (   37.46 ms per token,    26.69 tokens per second)\n",
            "llama_print_timings:       total time =    1444.51 ms /   276 tokens\n",
            "100%|██████████| 35/35 [27:13<00:00, 46.68s/it]\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    17 runs   (    0.55 ms per token,  1821.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.89 ms /   434 tokens (    5.14 ms per token,   194.45 tokens per second)\n",
            "llama_print_timings:        eval time =     614.83 ms /    16 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    2871.07 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.26 ms /     2 runs   (    0.63 ms per token,  1587.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6190.81 ms /  1152 tokens (    5.37 ms per token,   186.08 tokens per second)\n",
            "llama_print_timings:        eval time =      80.33 ms /     2 runs   (   40.17 ms per token,    24.90 tokens per second)\n",
            "llama_print_timings:       total time =    6286.91 ms /  1154 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      74.91 ms /   150 runs   (    0.50 ms per token,  2002.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6068.47 ms /  1120 tokens (    5.42 ms per token,   184.56 tokens per second)\n",
            "llama_print_timings:        eval time =    6081.79 ms /   149 runs   (   40.82 ms per token,    24.50 tokens per second)\n",
            "llama_print_timings:       total time =   12324.61 ms /  1269 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      65.03 ms /   126 runs   (    0.52 ms per token,  1937.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6930.91 ms /  1256 tokens (    5.52 ms per token,   181.22 tokens per second)\n",
            "llama_print_timings:        eval time =    5182.02 ms /   125 runs   (   41.46 ms per token,    24.12 tokens per second)\n",
            "llama_print_timings:       total time =   12277.64 ms /  1381 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       0.98 ms /     2 runs   (    0.49 ms per token,  2047.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1165.68 ms /   232 tokens (    5.02 ms per token,   199.02 tokens per second)\n",
            "llama_print_timings:        eval time =      75.60 ms /     2 runs   (   37.80 ms per token,    26.45 tokens per second)\n",
            "llama_print_timings:       total time =    1246.18 ms /   234 tokens\n",
            "  3%|▎         | 1/35 [00:44<25:03, 44.22s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.03 ms /    17 runs   (    0.59 ms per token,  1694.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2238.99 ms /   434 tokens (    5.16 ms per token,   193.84 tokens per second)\n",
            "llama_print_timings:        eval time =     621.23 ms /    16 runs   (   38.83 ms per token,    25.76 tokens per second)\n",
            "llama_print_timings:       total time =    2888.27 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1728.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5584.96 ms /  1044 tokens (    5.35 ms per token,   186.93 tokens per second)\n",
            "llama_print_timings:        eval time =      39.57 ms /     1 runs   (   39.57 ms per token,    25.27 tokens per second)\n",
            "llama_print_timings:       total time =    5638.95 ms /  1045 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      21.51 ms /    45 runs   (    0.48 ms per token,  2092.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5462.28 ms /  1011 tokens (    5.40 ms per token,   185.09 tokens per second)\n",
            "llama_print_timings:        eval time =    1764.72 ms /    44 runs   (   40.11 ms per token,    24.93 tokens per second)\n",
            "llama_print_timings:       total time =    7284.70 ms /  1055 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1736.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5560.23 ms /  1038 tokens (    5.36 ms per token,   186.68 tokens per second)\n",
            "llama_print_timings:        eval time =      39.40 ms /     1 runs   (   39.40 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    5613.56 ms /  1039 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.52 ms per token,  1912.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     671.37 ms /   132 tokens (    5.09 ms per token,   196.61 tokens per second)\n",
            "llama_print_timings:        eval time =      37.04 ms /     1 runs   (   37.04 ms per token,    27.00 tokens per second)\n",
            "llama_print_timings:       total time =     712.30 ms /   133 tokens\n",
            "  6%|▌         | 2/35 [01:13<19:38, 35.70s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.44 ms /    17 runs   (    0.67 ms per token,  1485.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2249.33 ms /   434 tokens (    5.18 ms per token,   192.95 tokens per second)\n",
            "llama_print_timings:        eval time =     619.59 ms /    16 runs   (   38.72 ms per token,    25.82 tokens per second)\n",
            "llama_print_timings:       total time =    2897.60 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.26 ms /     2 runs   (    0.63 ms per token,  1583.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6737.10 ms /  1236 tokens (    5.45 ms per token,   183.46 tokens per second)\n",
            "llama_print_timings:        eval time =      40.73 ms /     1 runs   (   40.73 ms per token,    24.55 tokens per second)\n",
            "llama_print_timings:       total time =    6796.61 ms /  1237 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      63.33 ms /   122 runs   (    0.52 ms per token,  1926.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6571.36 ms /  1203 tokens (    5.46 ms per token,   183.07 tokens per second)\n",
            "llama_print_timings:        eval time =    4972.90 ms /   121 runs   (   41.10 ms per token,    24.33 tokens per second)\n",
            "llama_print_timings:       total time =   11692.39 ms /  1324 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      41.21 ms /    75 runs   (    0.55 ms per token,  1820.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7323.84 ms /  1317 tokens (    5.56 ms per token,   179.82 tokens per second)\n",
            "llama_print_timings:        eval time =    3074.28 ms /    74 runs   (   41.54 ms per token,    24.07 tokens per second)\n",
            "llama_print_timings:       total time =   10497.13 ms /  1391 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1771.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     998.95 ms /   199 tokens (    5.02 ms per token,   199.21 tokens per second)\n",
            "llama_print_timings:        eval time =      37.50 ms /     1 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =    1041.54 ms /   200 tokens\n",
            "  9%|▊         | 3/35 [01:54<20:15, 37.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.11 ms /    17 runs   (    0.54 ms per token,  1865.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2241.50 ms /   434 tokens (    5.16 ms per token,   193.62 tokens per second)\n",
            "llama_print_timings:        eval time =     618.00 ms /    16 runs   (   38.63 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    2881.54 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1816.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4863.32 ms /   920 tokens (    5.29 ms per token,   189.17 tokens per second)\n",
            "llama_print_timings:        eval time =      39.23 ms /     1 runs   (   39.23 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    4915.16 ms /   921 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     156.27 ms /   256 runs   (    0.61 ms per token,  1638.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4696.17 ms /   887 tokens (    5.29 ms per token,   188.88 tokens per second)\n",
            "llama_print_timings:        eval time =   10238.53 ms /   255 runs   (   40.15 ms per token,    24.91 tokens per second)\n",
            "llama_print_timings:       total time =   15284.06 ms /  1142 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     148.04 ms /   256 runs   (    0.58 ms per token,  1729.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6170.90 ms /  1133 tokens (    5.45 ms per token,   183.60 tokens per second)\n",
            "llama_print_timings:        eval time =   10441.15 ms /   255 runs   (   40.95 ms per token,    24.42 tokens per second)\n",
            "llama_print_timings:       total time =   16959.41 ms /  1388 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      28.14 ms /    45 runs   (    0.63 ms per token,  1598.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1713.12 ms /   336 tokens (    5.10 ms per token,   196.13 tokens per second)\n",
            "llama_print_timings:        eval time =    1747.85 ms /    45 runs   (   38.84 ms per token,    25.75 tokens per second)\n",
            "llama_print_timings:       total time =    3521.49 ms /   381 tokens\n",
            " 11%|█▏        | 4/35 [02:43<21:53, 42.37s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.98 ms /    17 runs   (    0.53 ms per token,  1892.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.64 ms /   434 tokens (    5.15 ms per token,   194.21 tokens per second)\n",
            "llama_print_timings:        eval time =     617.68 ms /    16 runs   (   38.60 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    2875.77 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      65.24 ms /   133 runs   (    0.49 ms per token,  2038.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9581.18 ms /  1692 tokens (    5.66 ms per token,   176.60 tokens per second)\n",
            "llama_print_timings:        eval time =    5609.91 ms /   132 runs   (   42.50 ms per token,    23.53 tokens per second)\n",
            "llama_print_timings:       total time =   15357.71 ms /  1824 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      55.57 ms /   112 runs   (    0.50 ms per token,  2015.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9393.92 ms /  1659 tokens (    5.66 ms per token,   176.60 tokens per second)\n",
            "llama_print_timings:        eval time =    4707.44 ms /   111 runs   (   42.41 ms per token,    23.58 tokens per second)\n",
            "llama_print_timings:       total time =   14242.47 ms /  1770 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      80.80 ms /   154 runs   (    0.52 ms per token,  1905.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10061.08 ms /  1762 tokens (    5.71 ms per token,   175.13 tokens per second)\n",
            "llama_print_timings:        eval time =    6547.29 ms /   153 runs   (   42.79 ms per token,    23.37 tokens per second)\n",
            "llama_print_timings:       total time =   16815.71 ms /  1915 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.40 ms /     2 runs   (    0.70 ms per token,  1431.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =     962.09 ms /   190 tokens (    5.06 ms per token,   197.49 tokens per second)\n",
            "llama_print_timings:        eval time =      37.82 ms /     1 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
            "llama_print_timings:       total time =    1005.69 ms /   191 tokens\n",
            " 14%|█▍        | 5/35 [03:37<23:17, 46.57s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.75 ms /    17 runs   (    0.57 ms per token,  1743.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2255.12 ms /   434 tokens (    5.20 ms per token,   192.45 tokens per second)\n",
            "llama_print_timings:        eval time =     617.08 ms /    16 runs   (   38.57 ms per token,    25.93 tokens per second)\n",
            "llama_print_timings:       total time =    2896.84 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     137.02 ms /   256 runs   (    0.54 ms per token,  1868.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9356.21 ms /  1660 tokens (    5.64 ms per token,   177.42 tokens per second)\n",
            "llama_print_timings:        eval time =   10864.05 ms /   255 runs   (   42.60 ms per token,    23.47 tokens per second)\n",
            "llama_print_timings:       total time =   20584.52 ms /  1915 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      62.64 ms /   114 runs   (    0.55 ms per token,  1819.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9178.71 ms /  1627 tokens (    5.64 ms per token,   177.26 tokens per second)\n",
            "llama_print_timings:        eval time =    4809.16 ms /   113 runs   (   42.56 ms per token,    23.50 tokens per second)\n",
            "llama_print_timings:       total time =   14157.95 ms /  1740 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      67.36 ms /   112 runs   (    0.60 ms per token,  1662.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9790.18 ms /  1723 tokens (    5.68 ms per token,   175.99 tokens per second)\n",
            "llama_print_timings:        eval time =    4743.37 ms /   111 runs   (   42.73 ms per token,    23.40 tokens per second)\n",
            "llama_print_timings:       total time =   14711.44 ms /  1834 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.66 ms per token,  1524.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1013.33 ms /   200 tokens (    5.07 ms per token,   197.37 tokens per second)\n",
            "llama_print_timings:        eval time =      77.71 ms /     2 runs   (   38.86 ms per token,    25.74 tokens per second)\n",
            "llama_print_timings:       total time =    1097.48 ms /   202 tokens\n",
            " 17%|█▋        | 6/35 [04:36<24:30, 50.70s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.22 ms /    17 runs   (    0.54 ms per token,  1843.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.86 ms /   434 tokens (    5.15 ms per token,   194.20 tokens per second)\n",
            "llama_print_timings:        eval time =     614.49 ms /    16 runs   (   38.41 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    2870.88 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      35.56 ms /    67 runs   (    0.53 ms per token,  1884.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9370.44 ms /  1659 tokens (    5.65 ms per token,   177.05 tokens per second)\n",
            "llama_print_timings:        eval time =    2787.46 ms /    66 runs   (   42.23 ms per token,    23.68 tokens per second)\n",
            "llama_print_timings:       total time =   12251.08 ms /  1725 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      48.15 ms /    85 runs   (    0.57 ms per token,  1765.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9165.47 ms /  1626 tokens (    5.64 ms per token,   177.41 tokens per second)\n",
            "llama_print_timings:        eval time =    3560.36 ms /    84 runs   (   42.39 ms per token,    23.59 tokens per second)\n",
            "llama_print_timings:       total time =   12851.81 ms /  1710 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      87.76 ms /   145 runs   (    0.61 ms per token,  1652.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9619.48 ms /  1700 tokens (    5.66 ms per token,   176.72 tokens per second)\n",
            "llama_print_timings:        eval time =    6154.33 ms /   144 runs   (   42.74 ms per token,    23.40 tokens per second)\n",
            "llama_print_timings:       total time =   15988.42 ms /  1844 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1895.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     832.18 ms /   165 tokens (    5.04 ms per token,   198.27 tokens per second)\n",
            "llama_print_timings:        eval time =      37.47 ms /     1 runs   (   37.47 ms per token,    26.69 tokens per second)\n",
            "llama_print_timings:       total time =     873.77 ms /   166 tokens\n",
            " 20%|██        | 7/35 [05:24<23:19, 49.98s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.11 ms /    17 runs   (    0.54 ms per token,  1866.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2232.33 ms /   434 tokens (    5.14 ms per token,   194.42 tokens per second)\n",
            "llama_print_timings:        eval time =     616.87 ms /    16 runs   (   38.55 ms per token,    25.94 tokens per second)\n",
            "llama_print_timings:       total time =    2870.68 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.57 ms per token,  1742.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4905.21 ms /   924 tokens (    5.31 ms per token,   188.37 tokens per second)\n",
            "llama_print_timings:        eval time =      39.11 ms /     1 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    4957.40 ms /   925 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      92.10 ms /   135 runs   (    0.68 ms per token,  1465.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4726.66 ms /   891 tokens (    5.30 ms per token,   188.51 tokens per second)\n",
            "llama_print_timings:        eval time =    5396.95 ms /   134 runs   (   40.28 ms per token,    24.83 tokens per second)\n",
            "llama_print_timings:       total time =   10328.95 ms /  1025 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.51 ms per token,  1945.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5422.53 ms /  1010 tokens (    5.37 ms per token,   186.26 tokens per second)\n",
            "llama_print_timings:        eval time =      39.16 ms /     1 runs   (   39.16 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    5475.78 ms /  1011 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1769.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1116.69 ms /   220 tokens (    5.08 ms per token,   197.01 tokens per second)\n",
            "llama_print_timings:        eval time =      37.29 ms /     1 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
            "llama_print_timings:       total time =    1159.17 ms /   221 tokens\n",
            " 23%|██▎       | 8/35 [05:54<19:33, 43.45s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.33 ms /    17 runs   (    0.67 ms per token,  1500.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2238.54 ms /   434 tokens (    5.16 ms per token,   193.88 tokens per second)\n",
            "llama_print_timings:        eval time =     628.72 ms /    16 runs   (   39.29 ms per token,    25.45 tokens per second)\n",
            "llama_print_timings:       total time =    2893.56 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1776.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5634.48 ms /  1054 tokens (    5.35 ms per token,   187.06 tokens per second)\n",
            "llama_print_timings:        eval time =      39.89 ms /     1 runs   (   39.89 ms per token,    25.07 tokens per second)\n",
            "llama_print_timings:       total time =    5688.46 ms /  1055 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      56.72 ms /   103 runs   (    0.55 ms per token,  1816.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5510.29 ms /  1021 tokens (    5.40 ms per token,   185.29 tokens per second)\n",
            "llama_print_timings:        eval time =    4115.56 ms /   102 runs   (   40.35 ms per token,    24.78 tokens per second)\n",
            "llama_print_timings:       total time =    9751.96 ms /  1123 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      94.23 ms /   152 runs   (    0.62 ms per token,  1613.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6004.15 ms /  1111 tokens (    5.40 ms per token,   185.04 tokens per second)\n",
            "llama_print_timings:        eval time =    6188.96 ms /   151 runs   (   40.99 ms per token,    24.40 tokens per second)\n",
            "llama_print_timings:       total time =   12407.72 ms /  1262 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1748.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     916.28 ms /   184 tokens (    4.98 ms per token,   200.81 tokens per second)\n",
            "llama_print_timings:        eval time =      76.17 ms /     2 runs   (   38.09 ms per token,    26.26 tokens per second)\n",
            "llama_print_timings:       total time =     997.33 ms /   186 tokens\n",
            " 26%|██▌       | 9/35 [06:33<18:10, 41.95s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.29 ms /    17 runs   (    0.55 ms per token,  1829.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2236.70 ms /   434 tokens (    5.15 ms per token,   194.04 tokens per second)\n",
            "llama_print_timings:        eval time =     618.16 ms /    16 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2877.22 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      45.72 ms /    67 runs   (    0.68 ms per token,  1465.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7488.35 ms /  1365 tokens (    5.49 ms per token,   182.28 tokens per second)\n",
            "llama_print_timings:        eval time =    2749.03 ms /    66 runs   (   41.65 ms per token,    24.01 tokens per second)\n",
            "llama_print_timings:       total time =   10350.12 ms /  1431 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      52.95 ms /    97 runs   (    0.55 ms per token,  1831.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7343.84 ms /  1332 tokens (    5.51 ms per token,   181.38 tokens per second)\n",
            "llama_print_timings:        eval time =    3980.42 ms /    96 runs   (   41.46 ms per token,    24.12 tokens per second)\n",
            "llama_print_timings:       total time =   11448.17 ms /  1428 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      31.00 ms /    59 runs   (    0.53 ms per token,  1903.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7886.14 ms /  1411 tokens (    5.59 ms per token,   178.92 tokens per second)\n",
            "llama_print_timings:        eval time =    2412.71 ms /    58 runs   (   41.60 ms per token,    24.04 tokens per second)\n",
            "llama_print_timings:       total time =   10379.29 ms /  1469 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.54 ms per token,  1838.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     915.83 ms /   184 tokens (    4.98 ms per token,   200.91 tokens per second)\n",
            "llama_print_timings:        eval time =      38.09 ms /     1 runs   (   38.09 ms per token,    26.25 tokens per second)\n",
            "llama_print_timings:       total time =     957.66 ms /   185 tokens\n",
            " 29%|██▊       | 10/35 [07:16<17:41, 42.45s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.93 ms /    17 runs   (    0.53 ms per token,  1904.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2235.27 ms /   434 tokens (    5.15 ms per token,   194.16 tokens per second)\n",
            "llama_print_timings:        eval time =     617.62 ms /    16 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2875.09 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      53.89 ms /    96 runs   (    0.56 ms per token,  1781.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10830.18 ms /  1890 tokens (    5.73 ms per token,   174.51 tokens per second)\n",
            "llama_print_timings:        eval time =    4079.49 ms /    95 runs   (   42.94 ms per token,    23.29 tokens per second)\n",
            "llama_print_timings:       total time =   15042.91 ms /  1985 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     139.09 ms /   256 runs   (    0.54 ms per token,  1840.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10645.98 ms /  1856 tokens (    5.74 ms per token,   174.34 tokens per second)\n",
            "llama_print_timings:        eval time =   11096.70 ms /   256 runs   (   43.35 ms per token,    23.07 tokens per second)\n",
            "llama_print_timings:       total time =   22105.09 ms /  2112 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     150.47 ms /   256 runs   (    0.59 ms per token,  1701.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12241.45 ms /  2099 tokens (    5.83 ms per token,   171.47 tokens per second)\n",
            "llama_print_timings:        eval time =   11211.27 ms /   255 runs   (   43.97 ms per token,    22.74 tokens per second)\n",
            "llama_print_timings:       total time =   23830.95 ms /  2354 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1737.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1728.58 ms /   341 tokens (    5.07 ms per token,   197.27 tokens per second)\n",
            "llama_print_timings:        eval time =      37.53 ms /     1 runs   (   37.53 ms per token,    26.65 tokens per second)\n",
            "llama_print_timings:       total time =    1773.57 ms /   342 tokens\n",
            " 31%|███▏      | 11/35 [08:29<20:43, 51.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.29 ms /    17 runs   (    0.66 ms per token,  1505.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2257.68 ms /   434 tokens (    5.20 ms per token,   192.23 tokens per second)\n",
            "llama_print_timings:        eval time =     623.59 ms /    16 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    2907.75 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      31.39 ms /    59 runs   (    0.53 ms per token,  1879.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8643.97 ms /  1547 tokens (    5.59 ms per token,   178.97 tokens per second)\n",
            "llama_print_timings:        eval time =    2424.31 ms /    58 runs   (   41.80 ms per token,    23.92 tokens per second)\n",
            "llama_print_timings:       total time =   11150.39 ms /  1605 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      42.03 ms /    64 runs   (    0.66 ms per token,  1522.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8460.08 ms /  1514 tokens (    5.59 ms per token,   178.96 tokens per second)\n",
            "llama_print_timings:        eval time =    2675.10 ms /    63 runs   (   42.46 ms per token,    23.55 tokens per second)\n",
            "llama_print_timings:       total time =   11245.74 ms /  1577 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      79.82 ms /   114 runs   (    0.70 ms per token,  1428.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8809.83 ms /  1568 tokens (    5.62 ms per token,   177.98 tokens per second)\n",
            "llama_print_timings:        eval time =    4812.92 ms /   113 runs   (   42.59 ms per token,    23.48 tokens per second)\n",
            "llama_print_timings:       total time =   13810.17 ms /  1681 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1869.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     716.46 ms /   143 tokens (    5.01 ms per token,   199.59 tokens per second)\n",
            "llama_print_timings:        eval time =      37.70 ms /     1 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
            "llama_print_timings:       total time =     758.60 ms /   144 tokens\n",
            " 34%|███▍      | 12/35 [09:16<19:18, 50.35s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.82 ms /    17 runs   (    0.52 ms per token,  1926.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.98 ms /   434 tokens (    5.14 ms per token,   194.45 tokens per second)\n",
            "llama_print_timings:        eval time =     615.50 ms /    16 runs   (   38.47 ms per token,    26.00 tokens per second)\n",
            "llama_print_timings:       total time =    2869.37 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.48 ms /     2 runs   (    0.74 ms per token,  1354.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5402.06 ms /  1007 tokens (    5.36 ms per token,   186.41 tokens per second)\n",
            "llama_print_timings:        eval time =      39.84 ms /     1 runs   (   39.84 ms per token,    25.10 tokens per second)\n",
            "llama_print_timings:       total time =    5458.52 ms /  1008 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      27.43 ms /    46 runs   (    0.60 ms per token,  1677.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5185.04 ms /   974 tokens (    5.32 ms per token,   187.85 tokens per second)\n",
            "llama_print_timings:        eval time =    1796.02 ms /    45 runs   (   39.91 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =    7043.69 ms /  1019 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      59.25 ms /    88 runs   (    0.67 ms per token,  1485.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5386.59 ms /  1005 tokens (    5.36 ms per token,   186.57 tokens per second)\n",
            "llama_print_timings:        eval time =    3516.49 ms /    87 runs   (   40.42 ms per token,    24.74 tokens per second)\n",
            "llama_print_timings:       total time =    9031.58 ms /  1092 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1876.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     668.51 ms /   130 tokens (    5.14 ms per token,   194.46 tokens per second)\n",
            "llama_print_timings:        eval time =      37.04 ms /     1 runs   (   37.04 ms per token,    27.00 tokens per second)\n",
            "llama_print_timings:       total time =     709.66 ms /   131 tokens\n",
            " 37%|███▋      | 13/35 [09:46<16:11, 44.15s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.03 ms /    17 runs   (    0.53 ms per token,  1883.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2227.18 ms /   434 tokens (    5.13 ms per token,   194.87 tokens per second)\n",
            "llama_print_timings:        eval time =     614.80 ms /    16 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    2863.86 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.45 ms /     2 runs   (    0.72 ms per token,  1384.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3134.27 ms /   603 tokens (    5.20 ms per token,   192.39 tokens per second)\n",
            "llama_print_timings:        eval time =      38.42 ms /     1 runs   (   38.42 ms per token,    26.03 tokens per second)\n",
            "llama_print_timings:       total time =    3183.01 ms /   604 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      31.00 ms /    52 runs   (    0.60 ms per token,  1677.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2991.76 ms /   570 tokens (    5.25 ms per token,   190.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1993.12 ms /    51 runs   (   39.08 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    5052.16 ms /   621 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1850.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3175.86 ms /   611 tokens (    5.20 ms per token,   192.39 tokens per second)\n",
            "llama_print_timings:        eval time =      38.00 ms /     1 runs   (   38.00 ms per token,    26.31 tokens per second)\n",
            "llama_print_timings:       total time =    3222.92 ms /   612 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.49 ms per token,  2028.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     668.75 ms /   132 tokens (    5.07 ms per token,   197.38 tokens per second)\n",
            "llama_print_timings:        eval time =      37.74 ms /     1 runs   (   37.74 ms per token,    26.49 tokens per second)\n",
            "llama_print_timings:       total time =     712.11 ms /   133 tokens\n",
            " 40%|████      | 14/35 [10:04<12:40, 36.23s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.35 ms /    17 runs   (    0.55 ms per token,  1818.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2237.62 ms /   434 tokens (    5.16 ms per token,   193.96 tokens per second)\n",
            "llama_print_timings:        eval time =     614.83 ms /    16 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    2874.01 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.57 ms per token,  1742.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3648.58 ms /   696 tokens (    5.24 ms per token,   190.76 tokens per second)\n",
            "llama_print_timings:        eval time =      38.40 ms /     1 runs   (   38.40 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    3698.17 ms /   697 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      72.04 ms /   119 runs   (    0.61 ms per token,  1651.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3438.88 ms /   663 tokens (    5.19 ms per token,   192.80 tokens per second)\n",
            "llama_print_timings:        eval time =    4661.17 ms /   118 runs   (   39.50 ms per token,    25.32 tokens per second)\n",
            "llama_print_timings:       total time =    8249.79 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      37.87 ms /    67 runs   (    0.57 ms per token,  1769.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4111.27 ms /   773 tokens (    5.32 ms per token,   188.02 tokens per second)\n",
            "llama_print_timings:        eval time =    2609.62 ms /    66 runs   (   39.54 ms per token,    25.29 tokens per second)\n",
            "llama_print_timings:       total time =    6801.91 ms /   839 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1663.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     998.67 ms /   197 tokens (    5.07 ms per token,   197.26 tokens per second)\n",
            "llama_print_timings:        eval time =      37.58 ms /     1 runs   (   37.58 ms per token,    26.61 tokens per second)\n",
            "llama_print_timings:       total time =    1041.13 ms /   198 tokens\n",
            " 43%|████▎     | 15/35 [10:30<11:03, 33.17s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.26 ms /    17 runs   (    0.54 ms per token,  1835.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.12 ms /   434 tokens (    5.14 ms per token,   194.52 tokens per second)\n",
            "llama_print_timings:        eval time =     617.55 ms /    16 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2873.02 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1751.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4053.67 ms /   776 tokens (    5.22 ms per token,   191.43 tokens per second)\n",
            "llama_print_timings:        eval time =      39.12 ms /     1 runs   (   39.12 ms per token,    25.56 tokens per second)\n",
            "llama_print_timings:       total time =    4105.01 ms /   777 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      82.02 ms /   121 runs   (    0.68 ms per token,  1475.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3878.49 ms /   743 tokens (    5.22 ms per token,   191.57 tokens per second)\n",
            "llama_print_timings:        eval time =    4763.58 ms /   120 runs   (   39.70 ms per token,    25.19 tokens per second)\n",
            "llama_print_timings:       total time =    8818.25 ms /   863 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      74.04 ms /   130 runs   (    0.57 ms per token,  1755.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4468.91 ms /   842 tokens (    5.31 ms per token,   188.41 tokens per second)\n",
            "llama_print_timings:        eval time =    5143.54 ms /   129 runs   (   39.87 ms per token,    25.08 tokens per second)\n",
            "llama_print_timings:       total time =    9770.92 ms /   971 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.58 ms /     2 runs   (    0.79 ms per token,  1262.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1094.40 ms /   212 tokens (    5.16 ms per token,   193.71 tokens per second)\n",
            "llama_print_timings:        eval time =      38.01 ms /     1 runs   (   38.01 ms per token,    26.31 tokens per second)\n",
            "llama_print_timings:       total time =    1140.36 ms /   213 tokens\n",
            " 46%|████▌     | 16/35 [11:04<10:35, 33.44s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.55 ms /    17 runs   (    0.62 ms per token,  1610.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2269.40 ms /   434 tokens (    5.23 ms per token,   191.24 tokens per second)\n",
            "llama_print_timings:        eval time =     627.71 ms /    16 runs   (   39.23 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    2924.51 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.21 ms /     2 runs   (    0.60 ms per token,  1655.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4873.76 ms /   920 tokens (    5.30 ms per token,   188.77 tokens per second)\n",
            "llama_print_timings:        eval time =      79.81 ms /     2 runs   (   39.91 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =    4966.27 ms /   922 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     124.17 ms /   209 runs   (    0.59 ms per token,  1683.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4728.67 ms /   888 tokens (    5.33 ms per token,   187.79 tokens per second)\n",
            "llama_print_timings:        eval time =    8346.84 ms /   208 runs   (   40.13 ms per token,    24.92 tokens per second)\n",
            "llama_print_timings:       total time =   13347.30 ms /  1096 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      42.62 ms /    67 runs   (    0.64 ms per token,  1572.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5833.70 ms /  1080 tokens (    5.40 ms per token,   185.13 tokens per second)\n",
            "llama_print_timings:        eval time =    2720.21 ms /    67 runs   (   40.60 ms per token,    24.63 tokens per second)\n",
            "llama_print_timings:       total time =    8649.57 ms /  1147 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1706.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1483.07 ms /   294 tokens (    5.04 ms per token,   198.24 tokens per second)\n",
            "llama_print_timings:        eval time =      37.37 ms /     1 runs   (   37.37 ms per token,    26.76 tokens per second)\n",
            "llama_print_timings:       total time =    1526.98 ms /   295 tokens\n",
            " 49%|████▊     | 17/35 [11:39<10:09, 33.85s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.06 ms /    17 runs   (    0.53 ms per token,  1876.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2230.47 ms /   434 tokens (    5.14 ms per token,   194.58 tokens per second)\n",
            "llama_print_timings:        eval time =     617.44 ms /    16 runs   (   38.59 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2869.38 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.49 ms /     2 runs   (    0.75 ms per token,  1341.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4234.87 ms /   794 tokens (    5.33 ms per token,   187.49 tokens per second)\n",
            "llama_print_timings:        eval time =      39.61 ms /     1 runs   (   39.61 ms per token,    25.24 tokens per second)\n",
            "llama_print_timings:       total time =    4288.52 ms /   795 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      50.34 ms /    88 runs   (    0.57 ms per token,  1747.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3967.93 ms /   760 tokens (    5.22 ms per token,   191.54 tokens per second)\n",
            "llama_print_timings:        eval time =    3480.22 ms /    88 runs   (   39.55 ms per token,    25.29 tokens per second)\n",
            "llama_print_timings:       total time =    7553.06 ms /   848 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      45.81 ms /    64 runs   (    0.72 ms per token,  1396.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4478.27 ms /   846 tokens (    5.29 ms per token,   188.91 tokens per second)\n",
            "llama_print_timings:        eval time =    2519.84 ms /    63 runs   (   40.00 ms per token,    25.00 tokens per second)\n",
            "llama_print_timings:       total time =    7101.00 ms /   909 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1721.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     788.88 ms /   160 tokens (    4.93 ms per token,   202.82 tokens per second)\n",
            "llama_print_timings:        eval time =      37.24 ms /     1 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
            "llama_print_timings:       total time =     832.23 ms /   161 tokens\n",
            " 51%|█████▏    | 18/35 [12:05<08:55, 31.48s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.27 ms /    17 runs   (    0.55 ms per token,  1834.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.75 ms /   434 tokens (    5.14 ms per token,   194.47 tokens per second)\n",
            "llama_print_timings:        eval time =     615.75 ms /    16 runs   (   38.48 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2869.72 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.42 ms /     2 runs   (    0.71 ms per token,  1406.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2893.08 ms /   557 tokens (    5.19 ms per token,   192.53 tokens per second)\n",
            "llama_print_timings:        eval time =      39.01 ms /     1 runs   (   39.01 ms per token,    25.63 tokens per second)\n",
            "llama_print_timings:       total time =    2943.04 ms /   558 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      37.39 ms /    67 runs   (    0.56 ms per token,  1791.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2697.54 ms /   524 tokens (    5.15 ms per token,   194.25 tokens per second)\n",
            "llama_print_timings:        eval time =    2565.47 ms /    66 runs   (   38.87 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    5340.03 ms /   590 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     160.62 ms /   246 runs   (    0.65 ms per token,  1531.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3041.78 ms /   588 tokens (    5.17 ms per token,   193.31 tokens per second)\n",
            "llama_print_timings:        eval time =    9661.57 ms /   245 runs   (   39.43 ms per token,    25.36 tokens per second)\n",
            "llama_print_timings:       total time =   13055.79 ms /   833 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1870.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     713.94 ms /   139 tokens (    5.14 ms per token,   194.70 tokens per second)\n",
            "llama_print_timings:        eval time =      37.03 ms /     1 runs   (   37.03 ms per token,    27.01 tokens per second)\n",
            "llama_print_timings:       total time =     755.01 ms /   140 tokens\n",
            " 54%|█████▍    | 19/35 [12:36<08:24, 31.50s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.05 ms /    17 runs   (    0.53 ms per token,  1877.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2240.66 ms /   434 tokens (    5.16 ms per token,   193.69 tokens per second)\n",
            "llama_print_timings:        eval time =     617.63 ms /    16 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2886.10 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1863.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5271.21 ms /   988 tokens (    5.34 ms per token,   187.43 tokens per second)\n",
            "llama_print_timings:        eval time =      39.26 ms /     1 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    5324.47 ms /   989 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      69.80 ms /   110 runs   (    0.63 ms per token,  1575.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5121.86 ms /   955 tokens (    5.36 ms per token,   186.46 tokens per second)\n",
            "llama_print_timings:        eval time =    4388.60 ms /   109 runs   (   40.26 ms per token,    24.84 tokens per second)\n",
            "llama_print_timings:       total time =    9665.40 ms /  1064 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      50.89 ms /    77 runs   (    0.66 ms per token,  1513.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5714.53 ms /  1060 tokens (    5.39 ms per token,   185.49 tokens per second)\n",
            "llama_print_timings:        eval time =    3100.24 ms /    76 runs   (   40.79 ms per token,    24.51 tokens per second)\n",
            "llama_print_timings:       total time =    8936.47 ms /  1136 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.37 ms /     2 runs   (    0.69 ms per token,  1458.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =     925.49 ms /   184 tokens (    5.03 ms per token,   198.81 tokens per second)\n",
            "llama_print_timings:        eval time =      38.02 ms /     1 runs   (   38.02 ms per token,    26.30 tokens per second)\n",
            "llama_print_timings:       total time =     968.85 ms /   185 tokens\n",
            " 57%|█████▋    | 20/35 [13:12<08:08, 32.55s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.94 ms /    17 runs   (    0.53 ms per token,  1901.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2261.91 ms /   434 tokens (    5.21 ms per token,   191.87 tokens per second)\n",
            "llama_print_timings:        eval time =     615.11 ms /    16 runs   (   38.44 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2899.88 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.66 ms per token,  1523.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6990.53 ms /  1279 tokens (    5.47 ms per token,   182.96 tokens per second)\n",
            "llama_print_timings:        eval time =      40.69 ms /     1 runs   (   40.69 ms per token,    24.57 tokens per second)\n",
            "llama_print_timings:       total time =    7049.46 ms /  1280 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.43 ms /    13 runs   (    0.49 ms per token,  2022.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6804.09 ms /  1246 tokens (    5.46 ms per token,   183.13 tokens per second)\n",
            "llama_print_timings:        eval time =     489.73 ms /    12 runs   (   40.81 ms per token,    24.50 tokens per second)\n",
            "llama_print_timings:       total time =    7325.33 ms /  1258 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      26.45 ms /    38 runs   (    0.70 ms per token,  1436.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6891.75 ms /  1258 tokens (    5.48 ms per token,   182.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1530.13 ms /    37 runs   (   41.35 ms per token,    24.18 tokens per second)\n",
            "llama_print_timings:       total time =    8490.50 ms /  1295 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.23 ms /     2 runs   (    0.61 ms per token,  1626.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     436.60 ms /    83 tokens (    5.26 ms per token,   190.11 tokens per second)\n",
            "llama_print_timings:        eval time =      37.61 ms /     1 runs   (   37.61 ms per token,    26.59 tokens per second)\n",
            "llama_print_timings:       total time =     479.01 ms /    84 tokens\n",
            " 60%|██████    | 21/35 [13:41<07:24, 31.72s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.97 ms /    17 runs   (    0.53 ms per token,  1894.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2235.73 ms /   434 tokens (    5.15 ms per token,   194.12 tokens per second)\n",
            "llama_print_timings:        eval time =     615.75 ms /    16 runs   (   38.48 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2873.24 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      58.88 ms /   106 runs   (    0.56 ms per token,  1800.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10715.54 ms /  1870 tokens (    5.73 ms per token,   174.51 tokens per second)\n",
            "llama_print_timings:        eval time =    4510.41 ms /   105 runs   (   42.96 ms per token,    23.28 tokens per second)\n",
            "llama_print_timings:       total time =   15371.05 ms /  1975 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      95.71 ms /   176 runs   (    0.54 ms per token,  1838.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10533.42 ms /  1837 tokens (    5.73 ms per token,   174.40 tokens per second)\n",
            "llama_print_timings:        eval time =    7529.75 ms /   175 runs   (   43.03 ms per token,    23.24 tokens per second)\n",
            "llama_print_timings:       total time =   18292.43 ms /  2012 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      75.17 ms /   113 runs   (    0.67 ms per token,  1503.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11638.56 ms /  2007 tokens (    5.80 ms per token,   172.44 tokens per second)\n",
            "llama_print_timings:        eval time =    4879.59 ms /   112 runs   (   43.57 ms per token,    22.95 tokens per second)\n",
            "llama_print_timings:       total time =   16701.61 ms /  2119 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.34 ms /     2 runs   (    0.67 ms per token,  1488.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1293.90 ms /   251 tokens (    5.15 ms per token,   193.99 tokens per second)\n",
            "llama_print_timings:        eval time =      37.53 ms /     1 runs   (   37.53 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =    1337.71 ms /   252 tokens\n",
            " 63%|██████▎   | 22/35 [14:38<08:31, 39.35s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.58 ms /    17 runs   (    0.56 ms per token,  1774.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2232.82 ms /   434 tokens (    5.14 ms per token,   194.37 tokens per second)\n",
            "llama_print_timings:        eval time =     615.65 ms /    16 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2873.35 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.71 ms per token,  1415.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6131.73 ms /  1130 tokens (    5.43 ms per token,   184.29 tokens per second)\n",
            "llama_print_timings:        eval time =      40.51 ms /     1 runs   (   40.51 ms per token,    24.69 tokens per second)\n",
            "llama_print_timings:       total time =    6189.39 ms /  1131 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      15.20 ms /    28 runs   (    0.54 ms per token,  1842.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5896.27 ms /  1096 tokens (    5.38 ms per token,   185.88 tokens per second)\n",
            "llama_print_timings:        eval time =    1131.42 ms /    28 runs   (   40.41 ms per token,    24.75 tokens per second)\n",
            "llama_print_timings:       total time =    7070.76 ms /  1124 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      85.77 ms /   140 runs   (    0.61 ms per token,  1632.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6092.84 ms /  1122 tokens (    5.43 ms per token,   184.15 tokens per second)\n",
            "llama_print_timings:        eval time =    5680.49 ms /   139 runs   (   40.87 ms per token,    24.47 tokens per second)\n",
            "llama_print_timings:       total time =   11962.86 ms /  1261 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.00 ms /     2 runs   (    0.50 ms per token,  1998.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     511.03 ms /   100 tokens (    5.11 ms per token,   195.68 tokens per second)\n",
            "llama_print_timings:        eval time =      37.17 ms /     1 runs   (   37.17 ms per token,    26.91 tokens per second)\n",
            "llama_print_timings:       total time =     551.79 ms /   101 tokens\n",
            " 66%|██████▌   | 23/35 [15:11<07:29, 37.44s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.12 ms /    17 runs   (    0.54 ms per token,  1864.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2232.65 ms /   434 tokens (    5.14 ms per token,   194.39 tokens per second)\n",
            "llama_print_timings:        eval time =     618.16 ms /    16 runs   (   38.63 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2872.13 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1787.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4980.81 ms /   936 tokens (    5.32 ms per token,   187.92 tokens per second)\n",
            "llama_print_timings:        eval time =      39.21 ms /     1 runs   (   39.21 ms per token,    25.51 tokens per second)\n",
            "llama_print_timings:       total time =    5034.25 ms /   937 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      35.41 ms /    60 runs   (    0.59 ms per token,  1694.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4780.60 ms /   903 tokens (    5.29 ms per token,   188.89 tokens per second)\n",
            "llama_print_timings:        eval time =    2355.06 ms /    59 runs   (   39.92 ms per token,    25.05 tokens per second)\n",
            "llama_print_timings:       total time =    7212.39 ms /   962 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      49.37 ms /    87 runs   (    0.57 ms per token,  1762.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5150.88 ms /   959 tokens (    5.37 ms per token,   186.18 tokens per second)\n",
            "llama_print_timings:        eval time =    3447.74 ms /    86 runs   (   40.09 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =    8712.15 ms /  1045 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1919.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     671.15 ms /   133 tokens (    5.05 ms per token,   198.17 tokens per second)\n",
            "llama_print_timings:        eval time =      37.62 ms /     1 runs   (   37.62 ms per token,    26.58 tokens per second)\n",
            "llama_print_timings:       total time =     712.70 ms /   134 tokens\n",
            " 69%|██████▊   | 24/35 [15:40<06:23, 34.86s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.13 ms /    17 runs   (    0.54 ms per token,  1862.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2232.44 ms /   434 tokens (    5.14 ms per token,   194.41 tokens per second)\n",
            "llama_print_timings:        eval time =     614.92 ms /    16 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    2872.27 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1766.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5546.54 ms /  1040 tokens (    5.33 ms per token,   187.50 tokens per second)\n",
            "llama_print_timings:        eval time =      80.01 ms /     2 runs   (   40.01 ms per token,    25.00 tokens per second)\n",
            "llama_print_timings:       total time =    5640.82 ms /  1042 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      70.80 ms /   113 runs   (    0.63 ms per token,  1596.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5392.47 ms /  1008 tokens (    5.35 ms per token,   186.93 tokens per second)\n",
            "llama_print_timings:        eval time =    4528.72 ms /   112 runs   (   40.44 ms per token,    24.73 tokens per second)\n",
            "llama_print_timings:       total time =   10076.19 ms /  1120 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1863.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5984.31 ms /  1108 tokens (    5.40 ms per token,   185.15 tokens per second)\n",
            "llama_print_timings:        eval time =      40.15 ms /     1 runs   (   40.15 ms per token,    24.91 tokens per second)\n",
            "llama_print_timings:       total time =    6039.71 ms /  1109 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      28.66 ms /    40 runs   (    0.72 ms per token,  1395.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     992.27 ms /   195 tokens (    5.09 ms per token,   196.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1491.17 ms /    39 runs   (   38.24 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    2538.64 ms /   234 tokens\n",
            " 71%|███████▏  | 25/35 [16:12<05:40, 34.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.63 ms /    17 runs   (    0.68 ms per token,  1461.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2259.26 ms /   434 tokens (    5.21 ms per token,   192.10 tokens per second)\n",
            "llama_print_timings:        eval time =     630.76 ms /    16 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    2917.71 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.00 ms /     2 runs   (    0.50 ms per token,  2006.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5710.36 ms /  1061 tokens (    5.38 ms per token,   185.80 tokens per second)\n",
            "llama_print_timings:        eval time =      40.30 ms /     1 runs   (   40.30 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =    5764.93 ms /  1062 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      35.96 ms /    60 runs   (    0.60 ms per token,  1668.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5549.20 ms /  1028 tokens (    5.40 ms per token,   185.25 tokens per second)\n",
            "llama_print_timings:        eval time =    2388.58 ms /    59 runs   (   40.48 ms per token,    24.70 tokens per second)\n",
            "llama_print_timings:       total time =    8026.24 ms /  1087 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.57 ms per token,  1743.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5788.53 ms /  1075 tokens (    5.38 ms per token,   185.71 tokens per second)\n",
            "llama_print_timings:        eval time =      40.11 ms /     1 runs   (   40.11 ms per token,    24.93 tokens per second)\n",
            "llama_print_timings:       total time =    5843.91 ms /  1076 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1739.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =     710.22 ms /   142 tokens (    5.00 ms per token,   199.94 tokens per second)\n",
            "llama_print_timings:        eval time =      37.26 ms /     1 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
            "llama_print_timings:       total time =     751.63 ms /   143 tokens\n",
            " 74%|███████▍  | 26/35 [16:38<04:42, 31.38s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.14 ms /    17 runs   (    0.66 ms per token,  1526.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2237.72 ms /   434 tokens (    5.16 ms per token,   193.95 tokens per second)\n",
            "llama_print_timings:        eval time =     626.10 ms /    16 runs   (   39.13 ms per token,    25.56 tokens per second)\n",
            "llama_print_timings:       total time =    2890.20 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       0.90 ms /     2 runs   (    0.45 ms per token,  2217.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4132.19 ms /   791 tokens (    5.22 ms per token,   191.42 tokens per second)\n",
            "llama_print_timings:        eval time =      38.60 ms /     1 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    4181.74 ms /   792 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.09 ms /    65 runs   (    0.66 ms per token,  1508.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3970.91 ms /   758 tokens (    5.24 ms per token,   190.89 tokens per second)\n",
            "llama_print_timings:        eval time =    2542.95 ms /    64 runs   (   39.73 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =    6612.91 ms /   822 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     149.00 ms /   241 runs   (    0.62 ms per token,  1617.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4332.89 ms /   824 tokens (    5.26 ms per token,   190.17 tokens per second)\n",
            "llama_print_timings:        eval time =    9609.48 ms /   240 runs   (   40.04 ms per token,    24.98 tokens per second)\n",
            "llama_print_timings:       total time =   14297.86 ms /  1064 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.19 ms /     2 runs   (    0.59 ms per token,  1683.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     677.24 ms /   133 tokens (    5.09 ms per token,   196.39 tokens per second)\n",
            "llama_print_timings:        eval time =      37.50 ms /     1 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =     718.84 ms /   134 tokens\n",
            " 77%|███████▋  | 27/35 [17:12<04:18, 32.36s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.16 ms /    17 runs   (    0.54 ms per token,  1856.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2239.92 ms /   434 tokens (    5.16 ms per token,   193.76 tokens per second)\n",
            "llama_print_timings:        eval time =     618.29 ms /    16 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2880.88 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.54 ms /     2 runs   (    0.77 ms per token,  1300.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4646.27 ms /   880 tokens (    5.28 ms per token,   189.40 tokens per second)\n",
            "llama_print_timings:        eval time =      39.41 ms /     1 runs   (   39.41 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    4698.18 ms /   881 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      41.23 ms /    72 runs   (    0.57 ms per token,  1746.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4518.57 ms /   847 tokens (    5.33 ms per token,   187.45 tokens per second)\n",
            "llama_print_timings:        eval time =    2810.68 ms /    71 runs   (   39.59 ms per token,    25.26 tokens per second)\n",
            "llama_print_timings:       total time =    7425.32 ms /   918 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      37.62 ms /    57 runs   (    0.66 ms per token,  1515.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4877.42 ms /   917 tokens (    5.32 ms per token,   188.01 tokens per second)\n",
            "llama_print_timings:        eval time =    2245.84 ms /    56 runs   (   40.10 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =    7208.29 ms /   973 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.47 ms /     2 runs   (    0.73 ms per token,  1363.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =     722.66 ms /   143 tokens (    5.05 ms per token,   197.88 tokens per second)\n",
            "llama_print_timings:        eval time =      37.82 ms /     1 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
            "llama_print_timings:       total time =     765.12 ms /   144 tokens\n",
            " 80%|████████  | 28/35 [17:38<03:32, 30.43s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      12.06 ms /    17 runs   (    0.71 ms per token,  1409.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2265.05 ms /   434 tokens (    5.22 ms per token,   191.61 tokens per second)\n",
            "llama_print_timings:        eval time =     619.21 ms /    16 runs   (   38.70 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    2914.73 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.26 ms /     2 runs   (    0.63 ms per token,  1593.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3082.08 ms /   596 tokens (    5.17 ms per token,   193.38 tokens per second)\n",
            "llama_print_timings:        eval time =      38.62 ms /     1 runs   (   38.62 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    3130.53 ms /   597 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     102.10 ms /   147 runs   (    0.69 ms per token,  1439.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2913.66 ms /   563 tokens (    5.18 ms per token,   193.23 tokens per second)\n",
            "llama_print_timings:        eval time =    5720.81 ms /   146 runs   (   39.18 ms per token,    25.52 tokens per second)\n",
            "llama_print_timings:       total time =    8849.83 ms /   709 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1773.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3572.62 ms /   688 tokens (    5.19 ms per token,   192.58 tokens per second)\n",
            "llama_print_timings:        eval time =      77.96 ms /     2 runs   (   38.98 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    3661.39 ms /   690 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.26 ms /     2 runs   (    0.63 ms per token,  1588.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1195.94 ms /   237 tokens (    5.05 ms per token,   198.17 tokens per second)\n",
            "llama_print_timings:        eval time =      37.49 ms /     1 runs   (   37.49 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =    1238.98 ms /   238 tokens\n",
            " 83%|████████▎ | 29/35 [18:01<02:48, 28.08s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.01 ms /    17 runs   (    0.53 ms per token,  1887.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2226.51 ms /   434 tokens (    5.13 ms per token,   194.92 tokens per second)\n",
            "llama_print_timings:        eval time =     614.21 ms /    16 runs   (   38.39 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    2863.22 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      37.79 ms /    71 runs   (    0.53 ms per token,  1878.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6767.24 ms /  1239 tokens (    5.46 ms per token,   183.09 tokens per second)\n",
            "llama_print_timings:        eval time =    2861.76 ms /    70 runs   (   40.88 ms per token,    24.46 tokens per second)\n",
            "llama_print_timings:       total time =    9719.71 ms /  1309 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      59.37 ms /   106 runs   (    0.56 ms per token,  1785.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6580.98 ms /  1206 tokens (    5.46 ms per token,   183.26 tokens per second)\n",
            "llama_print_timings:        eval time =    4296.52 ms /   105 runs   (   40.92 ms per token,    24.44 tokens per second)\n",
            "llama_print_timings:       total time =   11014.10 ms /  1311 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     104.51 ms /   163 runs   (    0.64 ms per token,  1559.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7091.49 ms /  1296 tokens (    5.47 ms per token,   182.75 tokens per second)\n",
            "llama_print_timings:        eval time =    6762.45 ms /   163 runs   (   41.49 ms per token,    24.10 tokens per second)\n",
            "llama_print_timings:       total time =   14096.27 ms /  1459 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1921.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     955.80 ms /   190 tokens (    5.03 ms per token,   198.79 tokens per second)\n",
            "llama_print_timings:        eval time =      37.78 ms /     1 runs   (   37.78 ms per token,    26.47 tokens per second)\n",
            "llama_print_timings:       total time =     998.14 ms /   191 tokens\n",
            " 86%|████████▌ | 30/35 [18:42<02:40, 32.03s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.14 ms /    17 runs   (    0.54 ms per token,  1859.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.78 ms /   434 tokens (    5.15 ms per token,   194.20 tokens per second)\n",
            "llama_print_timings:        eval time =     618.49 ms /    16 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    2875.14 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1697.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4808.08 ms /   904 tokens (    5.32 ms per token,   188.02 tokens per second)\n",
            "llama_print_timings:        eval time =      80.17 ms /     2 runs   (   40.08 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    4908.27 ms /   906 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      66.64 ms /   110 runs   (    0.61 ms per token,  1650.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4600.17 ms /   872 tokens (    5.28 ms per token,   189.56 tokens per second)\n",
            "llama_print_timings:        eval time =    4356.27 ms /   109 runs   (   39.97 ms per token,    25.02 tokens per second)\n",
            "llama_print_timings:       total time =    9101.56 ms /   981 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      66.95 ms /   113 runs   (    0.59 ms per token,  1687.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5192.74 ms /   968 tokens (    5.36 ms per token,   186.41 tokens per second)\n",
            "llama_print_timings:        eval time =    4500.48 ms /   112 runs   (   40.18 ms per token,    24.89 tokens per second)\n",
            "llama_print_timings:       total time =    9838.42 ms /  1080 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1860.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     954.26 ms /   192 tokens (    4.97 ms per token,   201.20 tokens per second)\n",
            "llama_print_timings:        eval time =      75.92 ms /     2 runs   (   37.96 ms per token,    26.34 tokens per second)\n",
            "llama_print_timings:       total time =    1034.56 ms /   194 tokens\n",
            " 89%|████████▊ | 31/35 [19:13<02:07, 31.87s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.37 ms /    17 runs   (    0.67 ms per token,  1495.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2238.47 ms /   434 tokens (    5.16 ms per token,   193.88 tokens per second)\n",
            "llama_print_timings:        eval time =     625.26 ms /    16 runs   (   39.08 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    2891.29 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1694.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4321.94 ms /   824 tokens (    5.25 ms per token,   190.66 tokens per second)\n",
            "llama_print_timings:        eval time =      38.80 ms /     1 runs   (   38.80 ms per token,    25.78 tokens per second)\n",
            "llama_print_timings:       total time =    4372.61 ms /   825 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     154.99 ms /   240 runs   (    0.65 ms per token,  1548.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4149.24 ms /   791 tokens (    5.25 ms per token,   190.64 tokens per second)\n",
            "llama_print_timings:        eval time =    9534.34 ms /   239 runs   (   39.89 ms per token,    25.07 tokens per second)\n",
            "llama_print_timings:       total time =   14033.63 ms /  1030 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.60 ms /     2 runs   (    0.80 ms per token,  1252.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5424.33 ms /  1016 tokens (    5.34 ms per token,   187.30 tokens per second)\n",
            "llama_print_timings:        eval time =      81.16 ms /     2 runs   (   40.58 ms per token,    24.64 tokens per second)\n",
            "llama_print_timings:       total time =    5520.37 ms /  1018 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.29 ms /     2 runs   (    0.65 ms per token,  1546.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1669.80 ms /   323 tokens (    5.17 ms per token,   193.44 tokens per second)\n",
            "llama_print_timings:        eval time =      38.36 ms /     1 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    1715.44 ms /   324 tokens\n",
            " 91%|█████████▏| 32/35 [19:46<01:36, 32.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.91 ms /    17 runs   (    0.58 ms per token,  1715.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2258.53 ms /   434 tokens (    5.20 ms per token,   192.16 tokens per second)\n",
            "llama_print_timings:        eval time =     617.47 ms /    16 runs   (   38.59 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2901.54 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      45.60 ms /    73 runs   (    0.62 ms per token,  1600.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8764.43 ms /  1566 tokens (    5.60 ms per token,   178.68 tokens per second)\n",
            "llama_print_timings:        eval time =    3042.88 ms /    72 runs   (   42.26 ms per token,    23.66 tokens per second)\n",
            "llama_print_timings:       total time =   11921.65 ms /  1638 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      53.86 ms /    79 runs   (    0.68 ms per token,  1466.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8554.01 ms /  1533 tokens (    5.58 ms per token,   179.21 tokens per second)\n",
            "llama_print_timings:        eval time =    3298.48 ms /    78 runs   (   42.29 ms per token,    23.65 tokens per second)\n",
            "llama_print_timings:       total time =   11985.03 ms /  1611 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      81.87 ms /   119 runs   (    0.69 ms per token,  1453.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8961.69 ms /  1595 tokens (    5.62 ms per token,   177.98 tokens per second)\n",
            "llama_print_timings:        eval time =    4998.07 ms /   118 runs   (   42.36 ms per token,    23.61 tokens per second)\n",
            "llama_print_timings:       total time =   14156.26 ms /  1713 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.55 ms per token,  1808.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     835.88 ms /   165 tokens (    5.07 ms per token,   197.40 tokens per second)\n",
            "llama_print_timings:        eval time =      37.56 ms /     1 runs   (   37.56 ms per token,    26.62 tokens per second)\n",
            "llama_print_timings:       total time =     878.01 ms /   166 tokens\n",
            " 94%|█████████▍| 33/35 [20:31<01:11, 35.89s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.11 ms /    17 runs   (    0.54 ms per token,  1866.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2233.83 ms /   434 tokens (    5.15 ms per token,   194.29 tokens per second)\n",
            "llama_print_timings:        eval time =     616.20 ms /    16 runs   (   38.51 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    2871.46 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1718.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3939.80 ms /   752 tokens (    5.24 ms per token,   190.87 tokens per second)\n",
            "llama_print_timings:        eval time =      78.52 ms /     2 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    4029.77 ms /   754 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     110.79 ms /   183 runs   (    0.61 ms per token,  1651.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3789.32 ms /   720 tokens (    5.26 ms per token,   190.01 tokens per second)\n",
            "llama_print_timings:        eval time =    7171.01 ms /   182 runs   (   39.40 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =   11206.32 ms /   902 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      64.16 ms /    92 runs   (    0.70 ms per token,  1433.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4740.17 ms /   891 tokens (    5.32 ms per token,   187.97 tokens per second)\n",
            "llama_print_timings:        eval time =    3658.77 ms /    91 runs   (   40.21 ms per token,    24.87 tokens per second)\n",
            "llama_print_timings:       total time =    8544.27 ms /   982 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.33 ms /     2 runs   (    0.67 ms per token,  1502.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1317.92 ms /   264 tokens (    4.99 ms per token,   200.32 tokens per second)\n",
            "llama_print_timings:        eval time =      37.64 ms /     1 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
            "llama_print_timings:       total time =    1361.61 ms /   265 tokens\n",
            " 97%|█████████▋| 34/35 [21:01<00:34, 34.10s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.08 ms /    17 runs   (    0.53 ms per token,  1871.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2232.71 ms /   434 tokens (    5.14 ms per token,   194.38 tokens per second)\n",
            "llama_print_timings:        eval time =     617.87 ms /    16 runs   (   38.62 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    2872.69 ms /   450 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.51 ms /     2 runs   (    0.75 ms per token,  1326.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5073.83 ms /   950 tokens (    5.34 ms per token,   187.24 tokens per second)\n",
            "llama_print_timings:        eval time =      39.72 ms /     1 runs   (   39.72 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =    5128.71 ms /   951 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      73.77 ms /   129 runs   (    0.57 ms per token,  1748.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4894.88 ms /   917 tokens (    5.34 ms per token,   187.34 tokens per second)\n",
            "llama_print_timings:        eval time =    5112.29 ms /   128 runs   (   39.94 ms per token,    25.04 tokens per second)\n",
            "llama_print_timings:       total time =   10165.70 ms /  1045 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      71.50 ms /   120 runs   (    0.60 ms per token,  1678.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5516.48 ms /  1024 tokens (    5.39 ms per token,   185.63 tokens per second)\n",
            "llama_print_timings:        eval time =    4850.24 ms /   120 runs   (   40.42 ms per token,    24.74 tokens per second)\n",
            "llama_print_timings:       total time =   10522.05 ms /  1144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1865.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1117.69 ms /   219 tokens (    5.10 ms per token,   195.94 tokens per second)\n",
            "llama_print_timings:        eval time =      37.21 ms /     1 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
            "llama_print_timings:       total time =    1161.28 ms /   220 tokens\n",
            "100%|██████████| 35/35 [21:33<00:00, 36.97s/it]\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.18 ms /    15 runs   (    0.55 ms per token,  1834.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2235.75 ms /   434 tokens (    5.15 ms per token,   194.12 tokens per second)\n",
            "llama_print_timings:        eval time =     536.73 ms /    14 runs   (   38.34 ms per token,    26.08 tokens per second)\n",
            "llama_print_timings:       total time =    2793.16 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      42.84 ms /    76 runs   (    0.56 ms per token,  1773.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6942.05 ms /  1269 tokens (    5.47 ms per token,   182.80 tokens per second)\n",
            "llama_print_timings:        eval time =    3080.94 ms /    75 runs   (   41.08 ms per token,    24.34 tokens per second)\n",
            "llama_print_timings:       total time =   10128.04 ms /  1344 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      97.67 ms /   164 runs   (    0.60 ms per token,  1679.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6741.93 ms /  1236 tokens (    5.45 ms per token,   183.33 tokens per second)\n",
            "llama_print_timings:        eval time =    6789.06 ms /   163 runs   (   41.65 ms per token,    24.01 tokens per second)\n",
            "llama_print_timings:       total time =   13775.94 ms /  1399 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.46 ms /    66 runs   (    0.66 ms per token,  1518.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7717.11 ms /  1386 tokens (    5.57 ms per token,   179.60 tokens per second)\n",
            "llama_print_timings:        eval time =    2732.61 ms /    65 runs   (   42.04 ms per token,    23.79 tokens per second)\n",
            "llama_print_timings:       total time =   10561.29 ms /  1451 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      54.64 ms /    91 runs   (    0.60 ms per token,  1665.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1256.80 ms /   247 tokens (    5.09 ms per token,   196.53 tokens per second)\n",
            "llama_print_timings:        eval time =    3452.23 ms /    90 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    4816.92 ms /   337 tokens\n",
            "  3%|▎         | 1/35 [00:53<30:02, 53.03s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.90 ms /    15 runs   (    0.53 ms per token,  1899.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2237.06 ms /   434 tokens (    5.15 ms per token,   194.00 tokens per second)\n",
            "llama_print_timings:        eval time =     536.78 ms /    14 runs   (   38.34 ms per token,    26.08 tokens per second)\n",
            "llama_print_timings:       total time =    2793.94 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1872.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7230.08 ms /  1326 tokens (    5.45 ms per token,   183.40 tokens per second)\n",
            "llama_print_timings:        eval time =      40.51 ms /     1 runs   (   40.51 ms per token,    24.69 tokens per second)\n",
            "llama_print_timings:       total time =    7288.69 ms /  1327 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     112.20 ms /   204 runs   (    0.55 ms per token,  1818.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7102.15 ms /  1293 tokens (    5.49 ms per token,   182.06 tokens per second)\n",
            "llama_print_timings:        eval time =    8418.46 ms /   203 runs   (   41.47 ms per token,    24.11 tokens per second)\n",
            "llama_print_timings:       total time =   15794.08 ms /  1496 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     157.86 ms /   256 runs   (    0.62 ms per token,  1621.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8272.84 ms /  1479 tokens (    5.59 ms per token,   178.78 tokens per second)\n",
            "llama_print_timings:        eval time =   10768.84 ms /   255 runs   (   42.23 ms per token,    23.68 tokens per second)\n",
            "llama_print_timings:       total time =   19433.41 ms /  1734 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1792.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1481.34 ms /   291 tokens (    5.09 ms per token,   196.44 tokens per second)\n",
            "llama_print_timings:        eval time =      37.54 ms /     1 runs   (   37.54 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =    1525.32 ms /   292 tokens\n",
            "  6%|▌         | 2/35 [01:48<30:03, 54.65s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.13 ms /    15 runs   (    0.54 ms per token,  1845.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.11 ms /   434 tokens (    5.15 ms per token,   194.26 tokens per second)\n",
            "llama_print_timings:        eval time =     537.68 ms /    14 runs   (   38.41 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    2797.74 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1939.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5625.24 ms /  1052 tokens (    5.35 ms per token,   187.01 tokens per second)\n",
            "llama_print_timings:        eval time =      39.46 ms /     1 runs   (   39.46 ms per token,    25.34 tokens per second)\n",
            "llama_print_timings:       total time =    5678.59 ms /  1053 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      53.91 ms /    88 runs   (    0.61 ms per token,  1632.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5480.18 ms /  1019 tokens (    5.38 ms per token,   185.94 tokens per second)\n",
            "llama_print_timings:        eval time =    3515.31 ms /    87 runs   (   40.41 ms per token,    24.75 tokens per second)\n",
            "llama_print_timings:       total time =    9118.22 ms /  1106 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      44.78 ms /    67 runs   (    0.67 ms per token,  1496.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5935.15 ms /  1099 tokens (    5.40 ms per token,   185.17 tokens per second)\n",
            "llama_print_timings:        eval time =    2685.93 ms /    66 runs   (   40.70 ms per token,    24.57 tokens per second)\n",
            "llama_print_timings:       total time =    8717.38 ms /  1165 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.53 ms /     2 runs   (    0.77 ms per token,  1307.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     846.26 ms /   165 tokens (    5.13 ms per token,   194.98 tokens per second)\n",
            "llama_print_timings:        eval time =      37.98 ms /     1 runs   (   37.98 ms per token,    26.33 tokens per second)\n",
            "llama_print_timings:       total time =     890.55 ms /   166 tokens\n",
            "  9%|▊         | 3/35 [02:22<24:09, 45.30s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.98 ms /    15 runs   (    0.60 ms per token,  1670.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2259.26 ms /   434 tokens (    5.21 ms per token,   192.10 tokens per second)\n",
            "llama_print_timings:        eval time =     546.22 ms /    14 runs   (   39.02 ms per token,    25.63 tokens per second)\n",
            "llama_print_timings:       total time =    2829.08 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.65 ms per token,  1529.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6441.53 ms /  1184 tokens (    5.44 ms per token,   183.81 tokens per second)\n",
            "llama_print_timings:        eval time =      40.70 ms /     1 runs   (   40.70 ms per token,    24.57 tokens per second)\n",
            "llama_print_timings:       total time =    6501.44 ms /  1185 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     133.06 ms /   224 runs   (    0.59 ms per token,  1683.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6228.31 ms /  1151 tokens (    5.41 ms per token,   184.80 tokens per second)\n",
            "llama_print_timings:        eval time =    9197.41 ms /   223 runs   (   41.24 ms per token,    24.25 tokens per second)\n",
            "llama_print_timings:       total time =   15749.65 ms /  1374 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      32.54 ms /    53 runs   (    0.61 ms per token,  1628.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7541.08 ms /  1364 tokens (    5.53 ms per token,   180.88 tokens per second)\n",
            "llama_print_timings:        eval time =    2164.78 ms /    52 runs   (   41.63 ms per token,    24.02 tokens per second)\n",
            "llama_print_timings:       total time =    9793.04 ms /  1416 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.39 ms /     2 runs   (    0.69 ms per token,  1444.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1545.88 ms /   304 tokens (    5.09 ms per token,   196.65 tokens per second)\n",
            "llama_print_timings:        eval time =      38.26 ms /     1 runs   (   38.26 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =    1591.86 ms /   305 tokens\n",
            " 11%|█▏        | 4/35 [03:06<23:02, 44.61s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.91 ms /    15 runs   (    0.53 ms per token,  1896.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2246.41 ms /   434 tokens (    5.18 ms per token,   193.20 tokens per second)\n",
            "llama_print_timings:        eval time =     536.86 ms /    14 runs   (   38.35 ms per token,    26.08 tokens per second)\n",
            "llama_print_timings:       total time =    2803.90 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     122.93 ms /   228 runs   (    0.54 ms per token,  1854.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10381.39 ms /  1818 tokens (    5.71 ms per token,   175.12 tokens per second)\n",
            "llama_print_timings:        eval time =    9754.68 ms /   227 runs   (   42.97 ms per token,    23.27 tokens per second)\n",
            "llama_print_timings:       total time =   20454.60 ms /  2045 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      86.57 ms /   141 runs   (    0.61 ms per token,  1628.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10139.63 ms /  1784 tokens (    5.68 ms per token,   175.94 tokens per second)\n",
            "llama_print_timings:        eval time =    6068.70 ms /   141 runs   (   43.04 ms per token,    23.23 tokens per second)\n",
            "llama_print_timings:       total time =   16431.16 ms /  1925 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      42.82 ms /    64 runs   (    0.67 ms per token,  1494.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11012.66 ms /  1917 tokens (    5.74 ms per token,   174.07 tokens per second)\n",
            "llama_print_timings:        eval time =    2742.80 ms /    63 runs   (   43.54 ms per token,    22.97 tokens per second)\n",
            "llama_print_timings:       total time =   13873.04 ms /  1980 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1850.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1126.89 ms /   219 tokens (    5.15 ms per token,   194.34 tokens per second)\n",
            "llama_print_timings:        eval time =      37.06 ms /     1 runs   (   37.06 ms per token,    26.98 tokens per second)\n",
            "llama_print_timings:       total time =    1169.90 ms /   220 tokens\n",
            " 14%|█▍        | 5/35 [04:05<24:50, 49.69s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.91 ms /    15 runs   (    0.53 ms per token,  1897.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2235.81 ms /   434 tokens (    5.15 ms per token,   194.11 tokens per second)\n",
            "llama_print_timings:        eval time =     540.79 ms /    14 runs   (   38.63 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    2796.49 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.66 ms per token,  1520.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5725.79 ms /  1062 tokens (    5.39 ms per token,   185.48 tokens per second)\n",
            "llama_print_timings:        eval time =      39.76 ms /     1 runs   (   39.76 ms per token,    25.15 tokens per second)\n",
            "llama_print_timings:       total time =    5782.22 ms /  1063 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      56.33 ms /    95 runs   (    0.59 ms per token,  1686.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5504.20 ms /  1029 tokens (    5.35 ms per token,   186.95 tokens per second)\n",
            "llama_print_timings:        eval time =    3802.12 ms /    94 runs   (   40.45 ms per token,    24.72 tokens per second)\n",
            "llama_print_timings:       total time =    9436.31 ms /  1123 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      46.11 ms /    92 runs   (    0.50 ms per token,  1995.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6035.68 ms /  1106 tokens (    5.46 ms per token,   183.24 tokens per second)\n",
            "llama_print_timings:        eval time =    3710.23 ms /    91 runs   (   40.77 ms per token,    24.53 tokens per second)\n",
            "llama_print_timings:       total time =    9856.17 ms /  1197 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1921.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     916.50 ms /   182 tokens (    5.04 ms per token,   198.58 tokens per second)\n",
            "llama_print_timings:        eval time =      38.27 ms /     1 runs   (   38.27 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =     960.64 ms /   183 tokens\n",
            " 17%|█▋        | 6/35 [04:41<21:47, 45.07s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.50 ms /    15 runs   (    0.70 ms per token,  1428.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2242.11 ms /   434 tokens (    5.17 ms per token,   193.57 tokens per second)\n",
            "llama_print_timings:        eval time =     541.25 ms /    14 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    2811.03 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      50.96 ms /    88 runs   (    0.58 ms per token,  1726.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11907.92 ms /  2053 tokens (    5.80 ms per token,   172.41 tokens per second)\n",
            "llama_print_timings:        eval time =    3789.80 ms /    87 runs   (   43.56 ms per token,    22.96 tokens per second)\n",
            "llama_print_timings:       total time =   15827.70 ms /  2140 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      79.35 ms /   146 runs   (    0.54 ms per token,  1839.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11762.16 ms /  2020 tokens (    5.82 ms per token,   171.74 tokens per second)\n",
            "llama_print_timings:        eval time =    6314.82 ms /   145 runs   (   43.55 ms per token,    22.96 tokens per second)\n",
            "llama_print_timings:       total time =   18271.87 ms /  2165 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      39.30 ms /    66 runs   (    0.60 ms per token,  1679.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12628.89 ms /  2155 tokens (    5.86 ms per token,   170.64 tokens per second)\n",
            "llama_print_timings:        eval time =    2845.68 ms /    65 runs   (   43.78 ms per token,    22.84 tokens per second)\n",
            "llama_print_timings:       total time =   15583.90 ms /  2220 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.30 ms /     2 runs   (    0.65 ms per token,  1534.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1161.13 ms /   226 tokens (    5.14 ms per token,   194.64 tokens per second)\n",
            "llama_print_timings:        eval time =      37.54 ms /     1 runs   (   37.54 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =    1208.52 ms /   227 tokens\n",
            " 20%|██        | 7/35 [05:42<23:27, 50.28s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.11 ms /    15 runs   (    0.67 ms per token,  1483.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2261.65 ms /   434 tokens (    5.21 ms per token,   191.89 tokens per second)\n",
            "llama_print_timings:        eval time =     539.92 ms /    14 runs   (   38.57 ms per token,    25.93 tokens per second)\n",
            "llama_print_timings:       total time =    2828.67 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.56 ms /     2 runs   (    0.78 ms per token,  1277.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4682.60 ms /   883 tokens (    5.30 ms per token,   188.57 tokens per second)\n",
            "llama_print_timings:        eval time =      39.53 ms /     1 runs   (   39.53 ms per token,    25.30 tokens per second)\n",
            "llama_print_timings:       total time =    4736.33 ms /   884 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      66.18 ms /   114 runs   (    0.58 ms per token,  1722.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4508.62 ms /   850 tokens (    5.30 ms per token,   188.53 tokens per second)\n",
            "llama_print_timings:        eval time =    4470.69 ms /   113 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    9117.93 ms /   963 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     105.98 ms /   182 runs   (    0.58 ms per token,  1717.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5082.58 ms /   948 tokens (    5.36 ms per token,   186.52 tokens per second)\n",
            "llama_print_timings:        eval time =    7332.86 ms /   181 runs   (   40.51 ms per token,    24.68 tokens per second)\n",
            "llama_print_timings:       total time =   12649.10 ms /  1129 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1690.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     998.88 ms /   199 tokens (    5.02 ms per token,   199.22 tokens per second)\n",
            "llama_print_timings:        eval time =      37.50 ms /     1 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =    1040.71 ms /   200 tokens\n",
            " 23%|██▎       | 8/35 [06:20<20:53, 46.42s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.99 ms /    15 runs   (    0.53 ms per token,  1878.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2245.28 ms /   434 tokens (    5.17 ms per token,   193.29 tokens per second)\n",
            "llama_print_timings:        eval time =     545.02 ms /    14 runs   (   38.93 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    2813.21 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1722.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7589.57 ms /  1378 tokens (    5.51 ms per token,   181.57 tokens per second)\n",
            "llama_print_timings:        eval time =      40.66 ms /     1 runs   (   40.66 ms per token,    24.60 tokens per second)\n",
            "llama_print_timings:       total time =    7648.67 ms /  1379 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      38.40 ms /    72 runs   (    0.53 ms per token,  1874.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7429.67 ms /  1344 tokens (    5.53 ms per token,   180.90 tokens per second)\n",
            "llama_print_timings:        eval time =    2982.38 ms /    72 runs   (   41.42 ms per token,    24.14 tokens per second)\n",
            "llama_print_timings:       total time =   10505.47 ms /  1416 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      38.86 ms /    71 runs   (    0.55 ms per token,  1827.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7825.34 ms /  1404 tokens (    5.57 ms per token,   179.42 tokens per second)\n",
            "llama_print_timings:        eval time =    2911.19 ms /    70 runs   (   41.59 ms per token,    24.05 tokens per second)\n",
            "llama_print_timings:       total time =   10832.04 ms /  1474 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1853.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     793.81 ms /   154 tokens (    5.15 ms per token,   194.00 tokens per second)\n",
            "llama_print_timings:        eval time =      37.16 ms /     1 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
            "llama_print_timings:       total time =     834.94 ms /   155 tokens\n",
            " 26%|██▌       | 9/35 [06:59<19:07, 44.12s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.07 ms /    15 runs   (    0.54 ms per token,  1858.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2232.28 ms /   434 tokens (    5.14 ms per token,   194.42 tokens per second)\n",
            "llama_print_timings:        eval time =     538.25 ms /    14 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2791.24 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      56.11 ms /    85 runs   (    0.66 ms per token,  1514.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8923.48 ms /  1600 tokens (    5.58 ms per token,   179.30 tokens per second)\n",
            "llama_print_timings:        eval time =    3553.56 ms /    84 runs   (   42.30 ms per token,    23.64 tokens per second)\n",
            "llama_print_timings:       total time =   12616.21 ms /  1684 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     106.33 ms /   174 runs   (    0.61 ms per token,  1636.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8765.88 ms /  1567 tokens (    5.59 ms per token,   178.76 tokens per second)\n",
            "llama_print_timings:        eval time =    7377.38 ms /   173 runs   (   42.64 ms per token,    23.45 tokens per second)\n",
            "llama_print_timings:       total time =   16403.68 ms /  1740 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      74.73 ms /   130 runs   (    0.57 ms per token,  1739.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9817.32 ms /  1723 tokens (    5.70 ms per token,   175.51 tokens per second)\n",
            "llama_print_timings:        eval time =    5503.65 ms /   129 runs   (   42.66 ms per token,    23.44 tokens per second)\n",
            "llama_print_timings:       total time =   15499.34 ms /  1852 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      88.53 ms /   150 runs   (    0.59 ms per token,  1694.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1317.86 ms /   261 tokens (    5.05 ms per token,   198.05 tokens per second)\n",
            "llama_print_timings:        eval time =    5716.36 ms /   149 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    7226.87 ms /   410 tokens\n",
            " 29%|██▊       | 10/35 [08:02<20:47, 49.91s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.90 ms /    15 runs   (    0.59 ms per token,  1685.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2256.43 ms /   434 tokens (    5.20 ms per token,   192.34 tokens per second)\n",
            "llama_print_timings:        eval time =     541.03 ms /    14 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2822.11 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.52 ms /    75 runs   (    0.58 ms per token,  1723.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10772.20 ms /  1884 tokens (    5.72 ms per token,   174.89 tokens per second)\n",
            "llama_print_timings:        eval time =    3175.49 ms /    74 runs   (   42.91 ms per token,    23.30 tokens per second)\n",
            "llama_print_timings:       total time =   14060.37 ms /  1958 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     154.10 ms /   256 runs   (    0.60 ms per token,  1661.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10666.47 ms /  1851 tokens (    5.76 ms per token,   173.53 tokens per second)\n",
            "llama_print_timings:        eval time =   11133.42 ms /   255 runs   (   43.66 ms per token,    22.90 tokens per second)\n",
            "llama_print_timings:       total time =   22195.79 ms /  2106 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     153.22 ms /   256 runs   (    0.60 ms per token,  1670.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12241.58 ms /  2093 tokens (    5.85 ms per token,   170.97 tokens per second)\n",
            "llama_print_timings:        eval time =   11197.33 ms /   255 runs   (   43.91 ms per token,    22.77 tokens per second)\n",
            "llama_print_timings:       total time =   23862.21 ms /  2348 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1853.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1730.49 ms /   341 tokens (    5.07 ms per token,   197.05 tokens per second)\n",
            "llama_print_timings:        eval time =      37.44 ms /     1 runs   (   37.44 ms per token,    26.71 tokens per second)\n",
            "llama_print_timings:       total time =    1774.29 ms /   342 tokens\n",
            " 31%|███▏      | 11/35 [09:17<23:02, 57.62s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.89 ms /    15 runs   (    0.53 ms per token,  1902.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2226.22 ms /   434 tokens (    5.13 ms per token,   194.95 tokens per second)\n",
            "llama_print_timings:        eval time =     535.69 ms /    14 runs   (   38.26 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =    2782.58 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      94.58 ms /   156 runs   (    0.61 ms per token,  1649.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11202.75 ms /  1960 tokens (    5.72 ms per token,   174.96 tokens per second)\n",
            "llama_print_timings:        eval time =    6729.02 ms /   155 runs   (   43.41 ms per token,    23.03 tokens per second)\n",
            "llama_print_timings:       total time =   18160.47 ms /  2115 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      86.09 ms /   166 runs   (    0.52 ms per token,  1928.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11173.77 ms /  1927 tokens (    5.80 ms per token,   172.46 tokens per second)\n",
            "llama_print_timings:        eval time =    7208.75 ms /   165 runs   (   43.69 ms per token,    22.89 tokens per second)\n",
            "llama_print_timings:       total time =   18596.78 ms /  2092 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      59.29 ms /   100 runs   (    0.59 ms per token,  1686.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12201.28 ms /  2083 tokens (    5.86 ms per token,   170.72 tokens per second)\n",
            "llama_print_timings:        eval time =    4330.64 ms /    99 runs   (   43.74 ms per token,    22.86 tokens per second)\n",
            "llama_print_timings:       total time =   16691.75 ms /  2182 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.54 ms /     2 runs   (    0.77 ms per token,  1300.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1253.19 ms /   245 tokens (    5.12 ms per token,   195.50 tokens per second)\n",
            "llama_print_timings:        eval time =      37.62 ms /     1 runs   (   37.62 ms per token,    26.58 tokens per second)\n",
            "llama_print_timings:       total time =    1298.75 ms /   246 tokens\n",
            " 34%|███▍      | 12/35 [10:24<23:09, 60.43s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.96 ms /    15 runs   (    0.53 ms per token,  1884.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2252.17 ms /   434 tokens (    5.19 ms per token,   192.70 tokens per second)\n",
            "llama_print_timings:        eval time =     538.77 ms /    14 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2810.97 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.34 ms /     2 runs   (    0.67 ms per token,  1489.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3984.71 ms /   759 tokens (    5.25 ms per token,   190.48 tokens per second)\n",
            "llama_print_timings:        eval time =      38.89 ms /     1 runs   (   38.89 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    4036.33 ms /   760 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      33.06 ms /    56 runs   (    0.59 ms per token,  1693.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3768.22 ms /   726 tokens (    5.19 ms per token,   192.66 tokens per second)\n",
            "llama_print_timings:        eval time =    2165.14 ms /    55 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    6003.66 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      64.70 ms /    93 runs   (    0.70 ms per token,  1437.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4013.83 ms /   767 tokens (    5.23 ms per token,   191.09 tokens per second)\n",
            "llama_print_timings:        eval time =    3666.86 ms /    92 runs   (   39.86 ms per token,    25.09 tokens per second)\n",
            "llama_print_timings:       total time =    7824.07 ms /   859 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1801.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     710.23 ms /   140 tokens (    5.07 ms per token,   197.12 tokens per second)\n",
            "llama_print_timings:        eval time =      36.98 ms /     1 runs   (   36.98 ms per token,    27.04 tokens per second)\n",
            "llama_print_timings:       total time =     751.95 ms /   141 tokens\n",
            " 37%|███▋      | 13/35 [10:55<18:52, 51.47s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.79 ms /    15 runs   (    0.52 ms per token,  1925.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.08 ms /   434 tokens (    5.15 ms per token,   194.26 tokens per second)\n",
            "llama_print_timings:        eval time =     539.32 ms /    14 runs   (   38.52 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    2792.86 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.54 ms per token,  1841.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4651.59 ms /   878 tokens (    5.30 ms per token,   188.75 tokens per second)\n",
            "llama_print_timings:        eval time =      39.35 ms /     1 runs   (   39.35 ms per token,    25.41 tokens per second)\n",
            "llama_print_timings:       total time =    4703.10 ms /   879 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      30.49 ms /    44 runs   (    0.69 ms per token,  1443.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4483.55 ms /   845 tokens (    5.31 ms per token,   188.47 tokens per second)\n",
            "llama_print_timings:        eval time =    1723.51 ms /    43 runs   (   40.08 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    6278.88 ms /   888 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      98.67 ms /   172 runs   (    0.57 ms per token,  1743.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4707.36 ms /   878 tokens (    5.36 ms per token,   186.52 tokens per second)\n",
            "llama_print_timings:        eval time =    6874.19 ms /   171 runs   (   40.20 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =   11794.27 ms /  1049 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.00 ms /     2 runs   (    0.50 ms per token,  1996.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     634.59 ms /   124 tokens (    5.12 ms per token,   195.40 tokens per second)\n",
            "llama_print_timings:        eval time =      37.62 ms /     1 runs   (   37.62 ms per token,    26.58 tokens per second)\n",
            "llama_print_timings:       total time =     676.05 ms /   125 tokens\n",
            " 40%|████      | 14/35 [11:29<16:13, 46.35s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.84 ms /    15 runs   (    0.66 ms per token,  1524.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2262.56 ms /   434 tokens (    5.21 ms per token,   191.82 tokens per second)\n",
            "llama_print_timings:        eval time =     549.82 ms /    14 runs   (   39.27 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =    2837.43 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1739.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4675.82 ms /   886 tokens (    5.28 ms per token,   189.49 tokens per second)\n",
            "llama_print_timings:        eval time =      39.42 ms /     1 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    4727.04 ms /   887 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     101.37 ms /   160 runs   (    0.63 ms per token,  1578.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4518.86 ms /   853 tokens (    5.30 ms per token,   188.76 tokens per second)\n",
            "llama_print_timings:        eval time =    6343.07 ms /   159 runs   (   39.89 ms per token,    25.07 tokens per second)\n",
            "llama_print_timings:       total time =   11083.07 ms /  1012 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      76.83 ms /   111 runs   (    0.69 ms per token,  1444.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5369.29 ms /  1004 tokens (    5.35 ms per token,   186.99 tokens per second)\n",
            "llama_print_timings:        eval time =    4461.96 ms /   110 runs   (   40.56 ms per token,    24.65 tokens per second)\n",
            "llama_print_timings:       total time =   10003.56 ms /  1114 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1771.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1201.93 ms /   238 tokens (    5.05 ms per token,   198.01 tokens per second)\n",
            "llama_print_timings:        eval time =      37.26 ms /     1 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
            "llama_print_timings:       total time =    1244.22 ms /   239 tokens\n",
            " 43%|████▎     | 15/35 [12:04<14:16, 42.83s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.12 ms /    15 runs   (    0.54 ms per token,  1846.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2229.26 ms /   434 tokens (    5.14 ms per token,   194.68 tokens per second)\n",
            "llama_print_timings:        eval time =     538.44 ms /    14 runs   (   38.46 ms per token,    26.00 tokens per second)\n",
            "llama_print_timings:       total time =    2789.35 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1766.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6336.18 ms /  1165 tokens (    5.44 ms per token,   183.86 tokens per second)\n",
            "llama_print_timings:        eval time =      40.06 ms /     1 runs   (   40.06 ms per token,    24.96 tokens per second)\n",
            "llama_print_timings:       total time =    6395.02 ms /  1166 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      89.25 ms /   129 runs   (    0.69 ms per token,  1445.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6110.03 ms /  1132 tokens (    5.40 ms per token,   185.27 tokens per second)\n",
            "llama_print_timings:        eval time =    5253.92 ms /   128 runs   (   41.05 ms per token,    24.36 tokens per second)\n",
            "llama_print_timings:       total time =   11570.83 ms /  1260 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      97.94 ms /   155 runs   (    0.63 ms per token,  1582.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6768.05 ms /  1239 tokens (    5.46 ms per token,   183.07 tokens per second)\n",
            "llama_print_timings:        eval time =    6384.28 ms /   154 runs   (   41.46 ms per token,    24.12 tokens per second)\n",
            "llama_print_timings:       total time =   13387.45 ms /  1393 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.40 ms /     2 runs   (    0.70 ms per token,  1424.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1133.80 ms /   220 tokens (    5.15 ms per token,   194.04 tokens per second)\n",
            "llama_print_timings:        eval time =      37.66 ms /     1 runs   (   37.66 ms per token,    26.55 tokens per second)\n",
            "llama_print_timings:       total time =    1179.87 ms /   221 tokens\n",
            " 46%|████▌     | 16/35 [12:46<13:28, 42.57s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.20 ms /    15 runs   (    0.55 ms per token,  1829.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2240.54 ms /   434 tokens (    5.16 ms per token,   193.70 tokens per second)\n",
            "llama_print_timings:        eval time =     538.33 ms /    14 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2799.18 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      67.27 ms /   122 runs   (    0.55 ms per token,  1813.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6776.83 ms /  1236 tokens (    5.48 ms per token,   182.39 tokens per second)\n",
            "llama_print_timings:        eval time =    4960.96 ms /   121 runs   (   41.00 ms per token,    24.39 tokens per second)\n",
            "llama_print_timings:       total time =   11891.86 ms /  1357 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     142.45 ms /   256 runs   (    0.56 ms per token,  1797.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6561.70 ms /  1203 tokens (    5.45 ms per token,   183.34 tokens per second)\n",
            "llama_print_timings:        eval time =   10509.12 ms /   255 runs   (   41.21 ms per token,    24.26 tokens per second)\n",
            "llama_print_timings:       total time =   17420.37 ms /  1458 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      93.45 ms /   167 runs   (    0.56 ms per token,  1787.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8058.22 ms /  1444 tokens (    5.58 ms per token,   179.20 tokens per second)\n",
            "llama_print_timings:        eval time =    6938.55 ms /   166 runs   (   41.80 ms per token,    23.92 tokens per second)\n",
            "llama_print_timings:       total time =   15211.54 ms /  1610 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.50 ms /     2 runs   (    0.75 ms per token,  1336.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1750.48 ms /   342 tokens (    5.12 ms per token,   195.37 tokens per second)\n",
            "llama_print_timings:        eval time =      38.36 ms /     1 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    1796.26 ms /   343 tokens\n",
            " 49%|████▊     | 17/35 [13:40<13:48, 46.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.00 ms /    15 runs   (    0.67 ms per token,  1500.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2264.61 ms /   434 tokens (    5.22 ms per token,   191.64 tokens per second)\n",
            "llama_print_timings:        eval time =     541.57 ms /    14 runs   (   38.68 ms per token,    25.85 tokens per second)\n",
            "llama_print_timings:       total time =    2831.48 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1867.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4496.19 ms /   851 tokens (    5.28 ms per token,   189.27 tokens per second)\n",
            "llama_print_timings:        eval time =      38.98 ms /     1 runs   (   38.98 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    4546.84 ms /   852 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      39.40 ms /    62 runs   (    0.64 ms per token,  1573.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4347.91 ms /   818 tokens (    5.32 ms per token,   188.14 tokens per second)\n",
            "llama_print_timings:        eval time =    2436.08 ms /    61 runs   (   39.94 ms per token,    25.04 tokens per second)\n",
            "llama_print_timings:       total time =    6877.33 ms /   879 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      73.48 ms /   125 runs   (    0.59 ms per token,  1701.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4638.37 ms /   877 tokens (    5.29 ms per token,   189.07 tokens per second)\n",
            "llama_print_timings:        eval time =    4943.78 ms /   124 runs   (   39.87 ms per token,    25.08 tokens per second)\n",
            "llama_print_timings:       total time =    9741.24 ms /  1001 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.46 ms /     2 runs   (    0.73 ms per token,  1371.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =     687.94 ms /   134 tokens (    5.13 ms per token,   194.79 tokens per second)\n",
            "llama_print_timings:        eval time =      37.47 ms /     1 runs   (   37.47 ms per token,    26.69 tokens per second)\n",
            "llama_print_timings:       total time =     730.40 ms /   135 tokens\n",
            " 51%|█████▏    | 18/35 [14:09<11:34, 40.86s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      12.22 ms /    15 runs   (    0.81 ms per token,  1227.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2257.85 ms /   434 tokens (    5.20 ms per token,   192.22 tokens per second)\n",
            "llama_print_timings:        eval time =     541.16 ms /    14 runs   (   38.65 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    2826.52 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.46 ms /     2 runs   (    0.73 ms per token,  1371.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4917.03 ms /   924 tokens (    5.32 ms per token,   187.92 tokens per second)\n",
            "llama_print_timings:        eval time =      40.07 ms /     1 runs   (   40.07 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    4971.32 ms /   925 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      51.63 ms /    91 runs   (    0.57 ms per token,  1762.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4757.67 ms /   891 tokens (    5.34 ms per token,   187.28 tokens per second)\n",
            "llama_print_timings:        eval time =    3587.52 ms /    90 runs   (   39.86 ms per token,    25.09 tokens per second)\n",
            "llama_print_timings:       total time =    8460.05 ms /   981 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.62 ms /     2 runs   (    0.81 ms per token,  1236.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5253.66 ms /   979 tokens (    5.37 ms per token,   186.35 tokens per second)\n",
            "llama_print_timings:        eval time =      41.61 ms /     1 runs   (   41.61 ms per token,    24.03 tokens per second)\n",
            "llama_print_timings:       total time =    5309.54 ms /   980 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.50 ms /     2 runs   (    0.75 ms per token,  1333.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =     843.89 ms /   163 tokens (    5.18 ms per token,   193.15 tokens per second)\n",
            "llama_print_timings:        eval time =      38.60 ms /     1 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =     887.71 ms /   164 tokens\n",
            " 54%|█████▍    | 19/35 [14:38<09:57, 37.32s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      10.99 ms /    15 runs   (    0.73 ms per token,  1364.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2269.17 ms /   434 tokens (    5.23 ms per token,   191.26 tokens per second)\n",
            "llama_print_timings:        eval time =     549.52 ms /    14 runs   (   39.25 ms per token,    25.48 tokens per second)\n",
            "llama_print_timings:       total time =    2845.67 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1921.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3703.70 ms /   712 tokens (    5.20 ms per token,   192.24 tokens per second)\n",
            "llama_print_timings:        eval time =      39.30 ms /     1 runs   (   39.30 ms per token,    25.45 tokens per second)\n",
            "llama_print_timings:       total time =    3753.68 ms /   713 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      67.21 ms /   103 runs   (    0.65 ms per token,  1532.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3557.51 ms /   679 tokens (    5.24 ms per token,   190.86 tokens per second)\n",
            "llama_print_timings:        eval time =    4012.37 ms /   102 runs   (   39.34 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    7713.35 ms /   781 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      40.03 ms /    69 runs   (    0.58 ms per token,  1723.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4066.18 ms /   776 tokens (    5.24 ms per token,   190.84 tokens per second)\n",
            "llama_print_timings:        eval time =    2725.64 ms /    69 runs   (   39.50 ms per token,    25.32 tokens per second)\n",
            "llama_print_timings:       total time =    6877.23 ms /   845 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1818.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     874.65 ms /   176 tokens (    4.97 ms per token,   201.22 tokens per second)\n",
            "llama_print_timings:        eval time =      76.84 ms /     2 runs   (   38.42 ms per token,    26.03 tokens per second)\n",
            "llama_print_timings:       total time =     955.79 ms /   178 tokens\n",
            " 57%|█████▋    | 20/35 [15:05<08:33, 34.22s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.98 ms /    15 runs   (    0.67 ms per token,  1503.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2258.22 ms /   434 tokens (    5.20 ms per token,   192.19 tokens per second)\n",
            "llama_print_timings:        eval time =     548.51 ms /    14 runs   (   39.18 ms per token,    25.52 tokens per second)\n",
            "llama_print_timings:       total time =    2832.43 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      54.62 ms /   108 runs   (    0.51 ms per token,  1977.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10064.29 ms /  1772 tokens (    5.68 ms per token,   176.07 tokens per second)\n",
            "llama_print_timings:        eval time =    4560.18 ms /   107 runs   (   42.62 ms per token,    23.46 tokens per second)\n",
            "llama_print_timings:       total time =   14764.35 ms /  1879 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      41.90 ms /    87 runs   (    0.48 ms per token,  2076.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9932.24 ms /  1739 tokens (    5.71 ms per token,   175.09 tokens per second)\n",
            "llama_print_timings:        eval time =    3676.98 ms /    86 runs   (   42.76 ms per token,    23.39 tokens per second)\n",
            "llama_print_timings:       total time =   13723.75 ms /  1825 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.92 ms /    84 runs   (    0.52 ms per token,  1912.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10444.01 ms /  1824 tokens (    5.73 ms per token,   174.65 tokens per second)\n",
            "llama_print_timings:        eval time =    3599.09 ms /    84 runs   (   42.85 ms per token,    23.34 tokens per second)\n",
            "llama_print_timings:       total time =   14158.38 ms /  1908 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       0.96 ms /     2 runs   (    0.48 ms per token,  2079.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =     792.34 ms /   157 tokens (    5.05 ms per token,   198.15 tokens per second)\n",
            "llama_print_timings:        eval time =      37.53 ms /     1 runs   (   37.53 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =     834.87 ms /   158 tokens\n",
            " 60%|██████    | 21/35 [15:58<09:17, 39.84s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.86 ms /    15 runs   (    0.52 ms per token,  1909.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.41 ms /   434 tokens (    5.15 ms per token,   194.23 tokens per second)\n",
            "llama_print_timings:        eval time =     537.70 ms /    14 runs   (   38.41 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    2790.95 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      36.83 ms /    49 runs   (    0.75 ms per token,  1330.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7238.68 ms /  1326 tokens (    5.46 ms per token,   183.18 tokens per second)\n",
            "llama_print_timings:        eval time =    1992.35 ms /    48 runs   (   41.51 ms per token,    24.09 tokens per second)\n",
            "llama_print_timings:       total time =    9318.90 ms /  1374 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      54.61 ms /   100 runs   (    0.55 ms per token,  1831.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7098.77 ms /  1293 tokens (    5.49 ms per token,   182.14 tokens per second)\n",
            "llama_print_timings:        eval time =    4088.73 ms /    99 runs   (   41.30 ms per token,    24.21 tokens per second)\n",
            "llama_print_timings:       total time =   11314.61 ms /  1392 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1793.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7745.35 ms /  1387 tokens (    5.58 ms per token,   179.08 tokens per second)\n",
            "llama_print_timings:        eval time =      40.99 ms /     1 runs   (   40.99 ms per token,    24.40 tokens per second)\n",
            "llama_print_timings:       total time =    7807.01 ms /  1388 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.52 ms per token,  1912.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     873.97 ms /   175 tokens (    4.99 ms per token,   200.24 tokens per second)\n",
            "llama_print_timings:        eval time =      38.19 ms /     1 runs   (   38.19 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =     916.60 ms /   176 tokens\n",
            " 63%|██████▎   | 22/35 [16:37<08:35, 39.68s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.31 ms /    15 runs   (    0.55 ms per token,  1805.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2240.77 ms /   434 tokens (    5.16 ms per token,   193.68 tokens per second)\n",
            "llama_print_timings:        eval time =     538.25 ms /    14 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2801.41 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1768.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4512.87 ms /   844 tokens (    5.35 ms per token,   187.02 tokens per second)\n",
            "llama_print_timings:        eval time =      38.96 ms /     1 runs   (   38.96 ms per token,    25.67 tokens per second)\n",
            "llama_print_timings:       total time =    4565.86 ms /   845 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      22.42 ms /    39 runs   (    0.57 ms per token,  1739.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4277.69 ms /   811 tokens (    5.27 ms per token,   189.59 tokens per second)\n",
            "llama_print_timings:        eval time =    1497.46 ms /    38 runs   (   39.41 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    5825.54 ms /   849 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      76.61 ms /   114 runs   (    0.67 ms per token,  1488.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4479.85 ms /   847 tokens (    5.29 ms per token,   189.07 tokens per second)\n",
            "llama_print_timings:        eval time =    4530.91 ms /   113 runs   (   40.10 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =    9176.17 ms /   960 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1877.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     550.48 ms /   111 tokens (    4.96 ms per token,   201.64 tokens per second)\n",
            "llama_print_timings:        eval time =      37.60 ms /     1 runs   (   37.60 ms per token,    26.59 tokens per second)\n",
            "llama_print_timings:       total time =     591.01 ms /   112 tokens\n",
            " 66%|██████▌   | 23/35 [17:04<07:08, 35.72s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.26 ms /    15 runs   (    0.55 ms per token,  1815.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.16 ms /   434 tokens (    5.15 ms per token,   194.26 tokens per second)\n",
            "llama_print_timings:        eval time =     536.81 ms /    14 runs   (   38.34 ms per token,    26.08 tokens per second)\n",
            "llama_print_timings:       total time =    2791.47 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4768.64 ms /   894 tokens (    5.33 ms per token,   187.47 tokens per second)\n",
            "llama_print_timings:        eval time =      40.06 ms /     1 runs   (   40.06 ms per token,    24.97 tokens per second)\n",
            "llama_print_timings:       total time =    4823.64 ms /   895 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      67.22 ms /   117 runs   (    0.57 ms per token,  1740.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4545.51 ms /   861 tokens (    5.28 ms per token,   189.42 tokens per second)\n",
            "llama_print_timings:        eval time =    4599.70 ms /   116 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    9293.23 ms /   977 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      74.79 ms /   131 runs   (    0.57 ms per token,  1751.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5238.70 ms /   974 tokens (    5.38 ms per token,   185.92 tokens per second)\n",
            "llama_print_timings:        eval time =    5224.65 ms /   130 runs   (   40.19 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =   10629.87 ms /  1104 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1663.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     952.40 ms /   190 tokens (    5.01 ms per token,   199.50 tokens per second)\n",
            "llama_print_timings:        eval time =      37.66 ms /     1 runs   (   37.66 ms per token,    26.56 tokens per second)\n",
            "llama_print_timings:       total time =     994.78 ms /   191 tokens\n",
            " 69%|██████▊   | 24/35 [17:36<06:22, 34.77s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.02 ms /    15 runs   (    0.53 ms per token,  1871.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.75 ms /   434 tokens (    5.15 ms per token,   194.21 tokens per second)\n",
            "llama_print_timings:        eval time =     539.58 ms /    14 runs   (   38.54 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    2794.58 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1853.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5045.68 ms /   947 tokens (    5.33 ms per token,   187.69 tokens per second)\n",
            "llama_print_timings:        eval time =      39.28 ms /     1 runs   (   39.28 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =    5098.16 ms /   948 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      94.36 ms /   147 runs   (    0.64 ms per token,  1557.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4874.07 ms /   914 tokens (    5.33 ms per token,   187.52 tokens per second)\n",
            "llama_print_timings:        eval time =    5885.34 ms /   146 runs   (   40.31 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =   10971.92 ms /  1060 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.19 ms /     2 runs   (    0.59 ms per token,  1687.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5619.20 ms /  1048 tokens (    5.36 ms per token,   186.50 tokens per second)\n",
            "llama_print_timings:        eval time =      39.91 ms /     1 runs   (   39.91 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =    5672.78 ms /  1049 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1857.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1158.73 ms /   229 tokens (    5.06 ms per token,   197.63 tokens per second)\n",
            "llama_print_timings:        eval time =      37.41 ms /     1 runs   (   37.41 ms per token,    26.73 tokens per second)\n",
            "llama_print_timings:       total time =    1200.59 ms /   230 tokens\n",
            " 71%|███████▏  | 25/35 [18:07<05:34, 33.48s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.85 ms /    15 runs   (    0.66 ms per token,  1522.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2261.80 ms /   434 tokens (    5.21 ms per token,   191.88 tokens per second)\n",
            "llama_print_timings:        eval time =     541.82 ms /    14 runs   (   38.70 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    2829.63 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      48.13 ms /    67 runs   (    0.72 ms per token,  1392.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5890.50 ms /  1096 tokens (    5.37 ms per token,   186.06 tokens per second)\n",
            "llama_print_timings:        eval time =    2695.73 ms /    66 runs   (   40.84 ms per token,    24.48 tokens per second)\n",
            "llama_print_timings:       total time =    8697.32 ms /  1162 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      51.29 ms /    96 runs   (    0.53 ms per token,  1871.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5725.98 ms /  1063 tokens (    5.39 ms per token,   185.65 tokens per second)\n",
            "llama_print_timings:        eval time =    3850.52 ms /    95 runs   (   40.53 ms per token,    24.67 tokens per second)\n",
            "llama_print_timings:       total time =    9693.72 ms /  1158 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      59.02 ms /   109 runs   (    0.54 ms per token,  1846.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6276.94 ms /  1146 tokens (    5.48 ms per token,   182.57 tokens per second)\n",
            "llama_print_timings:        eval time =    4404.82 ms /   108 runs   (   40.79 ms per token,    24.52 tokens per second)\n",
            "llama_print_timings:       total time =   10816.18 ms /  1254 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.31 ms /     2 runs   (    0.65 ms per token,  1529.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     912.62 ms /   178 tokens (    5.13 ms per token,   195.04 tokens per second)\n",
            "llama_print_timings:        eval time =      37.96 ms /     1 runs   (   37.96 ms per token,    26.35 tokens per second)\n",
            "llama_print_timings:       total time =     954.73 ms /   179 tokens\n",
            " 74%|███████▍  | 26/35 [18:46<05:17, 35.28s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.97 ms /    15 runs   (    0.53 ms per token,  1881.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2235.48 ms /   434 tokens (    5.15 ms per token,   194.14 tokens per second)\n",
            "llama_print_timings:        eval time =     538.87 ms /    14 runs   (   38.49 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2794.95 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.22 ms /     2 runs   (    0.61 ms per token,  1642.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7200.94 ms /  1319 tokens (    5.46 ms per token,   183.17 tokens per second)\n",
            "llama_print_timings:        eval time =      41.05 ms /     1 runs   (   41.05 ms per token,    24.36 tokens per second)\n",
            "llama_print_timings:       total time =    7260.83 ms /  1320 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      34.15 ms /    63 runs   (    0.54 ms per token,  1844.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7078.26 ms /  1286 tokens (    5.50 ms per token,   181.68 tokens per second)\n",
            "llama_print_timings:        eval time =    2554.42 ms /    62 runs   (   41.20 ms per token,    24.27 tokens per second)\n",
            "llama_print_timings:       total time =    9721.23 ms /  1348 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      64.22 ms /   110 runs   (    0.58 ms per token,  1712.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7476.48 ms /  1350 tokens (    5.54 ms per token,   180.57 tokens per second)\n",
            "llama_print_timings:        eval time =    4532.71 ms /   109 runs   (   41.58 ms per token,    24.05 tokens per second)\n",
            "llama_print_timings:       total time =   12156.74 ms /  1459 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.55 ms per token,  1803.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     670.59 ms /   131 tokens (    5.12 ms per token,   195.35 tokens per second)\n",
            "llama_print_timings:        eval time =      37.14 ms /     1 runs   (   37.14 ms per token,    26.93 tokens per second)\n",
            "llama_print_timings:       total time =     711.41 ms /   132 tokens\n",
            " 77%|███████▋  | 27/35 [19:26<04:54, 36.76s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.81 ms /    15 runs   (    0.52 ms per token,  1920.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2238.94 ms /   434 tokens (    5.16 ms per token,   193.84 tokens per second)\n",
            "llama_print_timings:        eval time =     540.26 ms /    14 runs   (   38.59 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    2798.50 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1754.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7210.50 ms /  1320 tokens (    5.46 ms per token,   183.07 tokens per second)\n",
            "llama_print_timings:        eval time =      40.30 ms /     1 runs   (   40.30 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =    7267.94 ms /  1321 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      81.02 ms /   142 runs   (    0.57 ms per token,  1752.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7077.87 ms /  1287 tokens (    5.50 ms per token,   181.83 tokens per second)\n",
            "llama_print_timings:        eval time =    5818.99 ms /   141 runs   (   41.27 ms per token,    24.23 tokens per second)\n",
            "llama_print_timings:       total time =   13078.09 ms /  1428 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     145.87 ms /   256 runs   (    0.57 ms per token,  1755.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7952.45 ms /  1427 tokens (    5.57 ms per token,   179.44 tokens per second)\n",
            "llama_print_timings:        eval time =   10686.79 ms /   255 runs   (   41.91 ms per token,    23.86 tokens per second)\n",
            "llama_print_timings:       total time =   18988.22 ms /  1682 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.38 ms /     2 runs   (    0.69 ms per token,  1452.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1086.56 ms /   213 tokens (    5.10 ms per token,   196.03 tokens per second)\n",
            "llama_print_timings:        eval time =      37.59 ms /     1 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    1130.93 ms /   214 tokens\n",
            " 80%|████████  | 28/35 [20:15<04:41, 40.26s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.97 ms /    15 runs   (    0.53 ms per token,  1882.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2256.87 ms /   434 tokens (    5.20 ms per token,   192.30 tokens per second)\n",
            "llama_print_timings:        eval time =     539.37 ms /    14 runs   (   38.53 ms per token,    25.96 tokens per second)\n",
            "llama_print_timings:       total time =    2819.33 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.64 ms per token,  1574.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4054.60 ms /   776 tokens (    5.23 ms per token,   191.39 tokens per second)\n",
            "llama_print_timings:        eval time =      79.58 ms /     2 runs   (   39.79 ms per token,    25.13 tokens per second)\n",
            "llama_print_timings:       total time =    4145.40 ms /   778 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      70.12 ms /   110 runs   (    0.64 ms per token,  1568.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3899.24 ms /   744 tokens (    5.24 ms per token,   190.81 tokens per second)\n",
            "llama_print_timings:        eval time =    4325.36 ms /   109 runs   (   39.68 ms per token,    25.20 tokens per second)\n",
            "llama_print_timings:       total time =    8377.23 ms /   853 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      41.20 ms /    69 runs   (    0.60 ms per token,  1674.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4370.44 ms /   832 tokens (    5.25 ms per token,   190.37 tokens per second)\n",
            "llama_print_timings:        eval time =    2732.79 ms /    69 runs   (   39.61 ms per token,    25.25 tokens per second)\n",
            "llama_print_timings:       total time =    7193.25 ms /   901 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.70 ms per token,  1418.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     998.31 ms /   200 tokens (    4.99 ms per token,   200.34 tokens per second)\n",
            "llama_print_timings:        eval time =      39.16 ms /     1 runs   (   39.16 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    1042.81 ms /   201 tokens\n",
            " 83%|████████▎ | 29/35 [20:41<03:37, 36.19s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      11.36 ms /    15 runs   (    0.76 ms per token,  1320.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2268.43 ms /   434 tokens (    5.23 ms per token,   191.32 tokens per second)\n",
            "llama_print_timings:        eval time =     544.91 ms /    14 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    2841.05 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1784.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5503.04 ms /  1032 tokens (    5.33 ms per token,   187.53 tokens per second)\n",
            "llama_print_timings:        eval time =      80.48 ms /     2 runs   (   40.24 ms per token,    24.85 tokens per second)\n",
            "llama_print_timings:       total time =    5598.12 ms /  1034 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      24.37 ms /    46 runs   (    0.53 ms per token,  1887.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5374.53 ms /  1000 tokens (    5.37 ms per token,   186.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1807.05 ms /    45 runs   (   40.16 ms per token,    24.90 tokens per second)\n",
            "llama_print_timings:       total time =    7246.22 ms /  1045 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     131.40 ms /   204 runs   (    0.64 ms per token,  1552.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5517.24 ms /  1031 tokens (    5.35 ms per token,   186.87 tokens per second)\n",
            "llama_print_timings:        eval time =    8277.17 ms /   203 runs   (   40.77 ms per token,    24.53 tokens per second)\n",
            "llama_print_timings:       total time =   14097.20 ms /  1234 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1888.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     670.57 ms /   130 tokens (    5.16 ms per token,   193.87 tokens per second)\n",
            "llama_print_timings:        eval time =      37.21 ms /     1 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
            "llama_print_timings:       total time =     711.54 ms /   131 tokens\n",
            " 86%|████████▌ | 30/35 [21:17<02:59, 35.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.92 ms /    15 runs   (    0.53 ms per token,  1893.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2238.85 ms /   434 tokens (    5.16 ms per token,   193.85 tokens per second)\n",
            "llama_print_timings:        eval time =     537.54 ms /    14 runs   (   38.40 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    2796.69 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1800.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3542.85 ms /   680 tokens (    5.21 ms per token,   191.94 tokens per second)\n",
            "llama_print_timings:        eval time =      38.53 ms /     1 runs   (   38.53 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    3592.21 ms /   681 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      82.86 ms /   139 runs   (    0.60 ms per token,  1677.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3346.58 ms /   647 tokens (    5.17 ms per token,   193.33 tokens per second)\n",
            "llama_print_timings:        eval time =    5446.63 ms /   138 runs   (   39.47 ms per token,    25.34 tokens per second)\n",
            "llama_print_timings:       total time =    8972.39 ms /   785 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      65.79 ms /   113 runs   (    0.58 ms per token,  1717.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4104.57 ms /   772 tokens (    5.32 ms per token,   188.08 tokens per second)\n",
            "llama_print_timings:        eval time =    4426.69 ms /   112 runs   (   39.52 ms per token,    25.30 tokens per second)\n",
            "llama_print_timings:       total time =    8678.68 ms /   884 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1703.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1117.96 ms /   222 tokens (    5.04 ms per token,   198.58 tokens per second)\n",
            "llama_print_timings:        eval time =      37.52 ms /     1 runs   (   37.52 ms per token,    26.65 tokens per second)\n",
            "llama_print_timings:       total time =    1160.95 ms /   223 tokens\n",
            " 89%|████████▊ | 31/35 [21:48<02:18, 34.64s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.94 ms /    15 runs   (    0.53 ms per token,  1890.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2233.62 ms /   434 tokens (    5.15 ms per token,   194.30 tokens per second)\n",
            "llama_print_timings:        eval time =     538.66 ms /    14 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2792.12 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.38 ms /     2 runs   (    0.69 ms per token,  1451.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6688.96 ms /  1229 tokens (    5.44 ms per token,   183.74 tokens per second)\n",
            "llama_print_timings:        eval time =      40.86 ms /     1 runs   (   40.86 ms per token,    24.48 tokens per second)\n",
            "llama_print_timings:       total time =    6747.33 ms /  1230 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      69.17 ms /   128 runs   (    0.54 ms per token,  1850.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6530.85 ms /  1196 tokens (    5.46 ms per token,   183.13 tokens per second)\n",
            "llama_print_timings:        eval time =    5218.32 ms /   127 runs   (   41.09 ms per token,    24.34 tokens per second)\n",
            "llama_print_timings:       total time =   11909.95 ms /  1323 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1780.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7266.58 ms /  1310 tokens (    5.55 ms per token,   180.28 tokens per second)\n",
            "llama_print_timings:        eval time =      40.62 ms /     1 runs   (   40.62 ms per token,    24.62 tokens per second)\n",
            "llama_print_timings:       total time =    7330.94 ms /  1311 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1705.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1078.13 ms /   211 tokens (    5.11 ms per token,   195.71 tokens per second)\n",
            "llama_print_timings:        eval time =      37.21 ms /     1 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
            "llama_print_timings:       total time =    1120.56 ms /   212 tokens\n",
            " 91%|█████████▏| 32/35 [22:28<01:47, 35.96s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.01 ms /    15 runs   (    0.53 ms per token,  1873.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2243.95 ms /   434 tokens (    5.17 ms per token,   193.41 tokens per second)\n",
            "llama_print_timings:        eval time =     539.69 ms /    14 runs   (   38.55 ms per token,    25.94 tokens per second)\n",
            "llama_print_timings:       total time =    2803.91 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1800.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4608.63 ms /   863 tokens (    5.34 ms per token,   187.26 tokens per second)\n",
            "llama_print_timings:        eval time =      39.48 ms /     1 runs   (   39.48 ms per token,    25.33 tokens per second)\n",
            "llama_print_timings:       total time =    4662.26 ms /   864 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      44.57 ms /    85 runs   (    0.52 ms per token,  1907.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4371.18 ms /   830 tokens (    5.27 ms per token,   189.88 tokens per second)\n",
            "llama_print_timings:        eval time =    3316.77 ms /    84 runs   (   39.49 ms per token,    25.33 tokens per second)\n",
            "llama_print_timings:       total time =    7785.06 ms /   914 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      41.97 ms /    67 runs   (    0.63 ms per token,  1596.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4810.16 ms /   898 tokens (    5.36 ms per token,   186.69 tokens per second)\n",
            "llama_print_timings:        eval time =    2627.95 ms /    66 runs   (   39.82 ms per token,    25.11 tokens per second)\n",
            "llama_print_timings:       total time =    7532.17 ms /   964 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1883.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     871.27 ms /   171 tokens (    5.10 ms per token,   196.27 tokens per second)\n",
            "llama_print_timings:        eval time =      37.78 ms /     1 runs   (   37.78 ms per token,    26.47 tokens per second)\n",
            "llama_print_timings:       total time =     913.17 ms /   172 tokens\n",
            " 94%|█████████▍| 33/35 [22:54<01:06, 33.24s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.80 ms /    15 runs   (    0.59 ms per token,  1705.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2227.84 ms /   434 tokens (    5.13 ms per token,   194.81 tokens per second)\n",
            "llama_print_timings:        eval time =     537.66 ms /    14 runs   (   38.40 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    2788.07 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.53 ms /     2 runs   (    0.77 ms per token,  1303.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3489.54 ms /   670 tokens (    5.21 ms per token,   192.00 tokens per second)\n",
            "llama_print_timings:        eval time =      38.72 ms /     1 runs   (   38.72 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    3540.44 ms /   671 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      69.49 ms /   121 runs   (    0.57 ms per token,  1741.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3338.33 ms /   637 tokens (    5.24 ms per token,   190.81 tokens per second)\n",
            "llama_print_timings:        eval time =    4724.63 ms /   120 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    8208.37 ms /   757 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      50.87 ms /    73 runs   (    0.70 ms per token,  1434.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3917.63 ms /   746 tokens (    5.25 ms per token,   190.42 tokens per second)\n",
            "llama_print_timings:        eval time =    2855.84 ms /    72 runs   (   39.66 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    6882.02 ms /   818 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.52 ms /     2 runs   (    0.76 ms per token,  1317.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1048.02 ms /   202 tokens (    5.19 ms per token,   192.74 tokens per second)\n",
            "llama_print_timings:        eval time =      37.54 ms /     1 runs   (   37.54 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =    1092.79 ms /   203 tokens\n",
            " 97%|█████████▋| 34/35 [23:20<00:30, 30.88s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       8.21 ms /    15 runs   (    0.55 ms per token,  1826.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2247.75 ms /   434 tokens (    5.18 ms per token,   193.08 tokens per second)\n",
            "llama_print_timings:        eval time =     539.89 ms /    14 runs   (   38.56 ms per token,    25.93 tokens per second)\n",
            "llama_print_timings:       total time =    2809.83 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1699.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4417.41 ms /   840 tokens (    5.26 ms per token,   190.16 tokens per second)\n",
            "llama_print_timings:        eval time =      79.67 ms /     2 runs   (   39.83 ms per token,    25.11 tokens per second)\n",
            "llama_print_timings:       total time =    4509.17 ms /   842 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     113.36 ms /   198 runs   (    0.57 ms per token,  1746.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4287.92 ms /   808 tokens (    5.31 ms per token,   188.44 tokens per second)\n",
            "llama_print_timings:        eval time =    7827.91 ms /   197 runs   (   39.74 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =   12364.21 ms /  1005 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      41.87 ms /    70 runs   (    0.60 ms per token,  1672.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5303.78 ms /   984 tokens (    5.39 ms per token,   185.53 tokens per second)\n",
            "llama_print_timings:        eval time =    2821.76 ms /    70 runs   (   40.31 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =    8220.63 ms /  1054 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1824.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1447.19 ms /   288 tokens (    5.02 ms per token,   199.01 tokens per second)\n",
            "llama_print_timings:        eval time =      37.76 ms /     1 runs   (   37.76 ms per token,    26.48 tokens per second)\n",
            "llama_print_timings:       total time =    1490.11 ms /   289 tokens\n",
            "100%|██████████| 35/35 [23:54<00:00, 40.98s/it]\n",
            "  0%|          | 0/35 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.22 ms /    10 runs   (    0.52 ms per token,  1914.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2240.77 ms /   434 tokens (    5.16 ms per token,   193.68 tokens per second)\n",
            "llama_print_timings:        eval time =     342.25 ms /     9 runs   (   38.03 ms per token,    26.30 tokens per second)\n",
            "llama_print_timings:       total time =    2599.15 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.42 ms /     2 runs   (    0.71 ms per token,  1407.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6149.56 ms /  1144 tokens (    5.38 ms per token,   186.03 tokens per second)\n",
            "llama_print_timings:        eval time =      40.06 ms /     1 runs   (   40.06 ms per token,    24.96 tokens per second)\n",
            "llama_print_timings:       total time =    6206.19 ms /  1145 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      49.49 ms /    97 runs   (    0.51 ms per token,  1959.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6019.59 ms /  1111 tokens (    5.42 ms per token,   184.56 tokens per second)\n",
            "llama_print_timings:        eval time =    3902.86 ms /    96 runs   (   40.65 ms per token,    24.60 tokens per second)\n",
            "llama_print_timings:       total time =   10042.34 ms /  1207 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      44.06 ms /    67 runs   (    0.66 ms per token,  1520.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6560.20 ms /  1194 tokens (    5.49 ms per token,   182.01 tokens per second)\n",
            "llama_print_timings:        eval time =    2725.77 ms /    66 runs   (   41.30 ms per token,    24.21 tokens per second)\n",
            "llama_print_timings:       total time =    9389.76 ms /  1260 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1976.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     915.62 ms /   180 tokens (    5.09 ms per token,   196.59 tokens per second)\n",
            "llama_print_timings:        eval time =      38.28 ms /     1 runs   (   38.28 ms per token,    26.12 tokens per second)\n",
            "llama_print_timings:       total time =     959.53 ms /   181 tokens\n",
            "  3%|▎         | 1/35 [00:39<22:11, 39.15s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.36 ms /    10 runs   (    0.54 ms per token,  1866.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2247.30 ms /   434 tokens (    5.18 ms per token,   193.12 tokens per second)\n",
            "llama_print_timings:        eval time =     345.43 ms /     9 runs   (   38.38 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    2608.40 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      49.33 ms /    70 runs   (    0.70 ms per token,  1419.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8820.82 ms /  1580 tokens (    5.58 ms per token,   179.12 tokens per second)\n",
            "llama_print_timings:        eval time =    2930.27 ms /    69 runs   (   42.47 ms per token,    23.55 tokens per second)\n",
            "llama_print_timings:       total time =   11874.86 ms /  1649 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      83.32 ms /   131 runs   (    0.64 ms per token,  1572.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8677.43 ms /  1547 tokens (    5.61 ms per token,   178.28 tokens per second)\n",
            "llama_print_timings:        eval time =    5544.74 ms /   130 runs   (   42.65 ms per token,    23.45 tokens per second)\n",
            "llama_print_timings:       total time =   14433.10 ms /  1677 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.83 ms /    60 runs   (    0.73 ms per token,  1368.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9371.55 ms /  1660 tokens (    5.65 ms per token,   177.13 tokens per second)\n",
            "llama_print_timings:        eval time =    2511.76 ms /    59 runs   (   42.57 ms per token,    23.49 tokens per second)\n",
            "llama_print_timings:       total time =   11993.68 ms /  1719 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.70 ms per token,  1422.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1130.05 ms /   218 tokens (    5.18 ms per token,   192.91 tokens per second)\n",
            "llama_print_timings:        eval time =      37.55 ms /     1 runs   (   37.55 ms per token,    26.63 tokens per second)\n",
            "llama_print_timings:       total time =    1173.56 ms /   219 tokens\n",
            "  6%|▌         | 2/35 [01:30<25:34, 46.50s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.35 ms /    10 runs   (    0.54 ms per token,  1869.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2251.04 ms /   434 tokens (    5.19 ms per token,   192.80 tokens per second)\n",
            "llama_print_timings:        eval time =     346.43 ms /     9 runs   (   38.49 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    2615.84 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      46.16 ms /    84 runs   (    0.55 ms per token,  1819.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10206.59 ms /  1795 tokens (    5.69 ms per token,   175.87 tokens per second)\n",
            "llama_print_timings:        eval time =    3536.56 ms /    83 runs   (   42.61 ms per token,    23.47 tokens per second)\n",
            "llama_print_timings:       total time =   13861.93 ms /  1878 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      37.37 ms /    72 runs   (    0.52 ms per token,  1926.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10082.53 ms /  1762 tokens (    5.72 ms per token,   174.76 tokens per second)\n",
            "llama_print_timings:        eval time =    3043.22 ms /    71 runs   (   42.86 ms per token,    23.33 tokens per second)\n",
            "llama_print_timings:       total time =   13226.89 ms /  1833 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      38.39 ms /    71 runs   (    0.54 ms per token,  1849.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10517.25 ms /  1826 tokens (    5.76 ms per token,   173.62 tokens per second)\n",
            "llama_print_timings:        eval time =    3004.38 ms /    70 runs   (   42.92 ms per token,    23.30 tokens per second)\n",
            "llama_print_timings:       total time =   13622.57 ms /  1896 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.34 ms /     2 runs   (    0.67 ms per token,  1497.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     752.90 ms /   149 tokens (    5.05 ms per token,   197.90 tokens per second)\n",
            "llama_print_timings:        eval time =      37.64 ms /     1 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
            "llama_print_timings:       total time =     795.53 ms /   150 tokens\n",
            "  9%|▊         | 3/35 [02:22<26:07, 49.00s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.42 ms /    10 runs   (    0.64 ms per token,  1558.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.84 ms /   434 tokens (    5.15 ms per token,   194.20 tokens per second)\n",
            "llama_print_timings:        eval time =     346.34 ms /     9 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2598.51 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.28 ms /     2 runs   (    0.64 ms per token,  1558.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5637.35 ms /  1054 tokens (    5.35 ms per token,   186.97 tokens per second)\n",
            "llama_print_timings:        eval time =      39.84 ms /     1 runs   (   39.84 ms per token,    25.10 tokens per second)\n",
            "llama_print_timings:       total time =    5692.61 ms /  1055 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     114.53 ms /   210 runs   (    0.55 ms per token,  1833.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5494.81 ms /  1021 tokens (    5.38 ms per token,   185.81 tokens per second)\n",
            "llama_print_timings:        eval time =    8475.76 ms /   209 runs   (   40.55 ms per token,    24.66 tokens per second)\n",
            "llama_print_timings:       total time =   14244.37 ms /  1230 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      41.86 ms /    74 runs   (    0.57 ms per token,  1767.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6713.89 ms /  1220 tokens (    5.50 ms per token,   181.71 tokens per second)\n",
            "llama_print_timings:        eval time =    3006.00 ms /    73 runs   (   41.18 ms per token,    24.28 tokens per second)\n",
            "llama_print_timings:       total time =    9817.11 ms /  1293 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1705.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1493.04 ms /   290 tokens (    5.15 ms per token,   194.23 tokens per second)\n",
            "llama_print_timings:        eval time =      37.57 ms /     1 runs   (   37.57 ms per token,    26.61 tokens per second)\n",
            "llama_print_timings:       total time =    1537.14 ms /   291 tokens\n",
            " 11%|█▏        | 4/35 [03:05<24:07, 46.69s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.01 ms /    10 runs   (    0.70 ms per token,  1426.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2246.81 ms /   434 tokens (    5.18 ms per token,   193.16 tokens per second)\n",
            "llama_print_timings:        eval time =     352.32 ms /     9 runs   (   39.15 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    2617.88 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     137.79 ms /   256 runs   (    0.54 ms per token,  1857.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9674.09 ms /  1708 tokens (    5.66 ms per token,   176.55 tokens per second)\n",
            "llama_print_timings:        eval time =   10908.61 ms /   255 runs   (   42.78 ms per token,    23.38 tokens per second)\n",
            "llama_print_timings:       total time =   20933.39 ms /  1963 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      25.70 ms /    51 runs   (    0.50 ms per token,  1984.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9527.41 ms /  1675 tokens (    5.69 ms per token,   175.81 tokens per second)\n",
            "llama_print_timings:        eval time =    2121.48 ms /    50 runs   (   42.43 ms per token,    23.57 tokens per second)\n",
            "llama_print_timings:       total time =   11726.84 ms /  1725 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      88.09 ms /   160 runs   (    0.55 ms per token,  1816.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9778.60 ms /  1717 tokens (    5.70 ms per token,   175.59 tokens per second)\n",
            "llama_print_timings:        eval time =    6781.99 ms /   159 runs   (   42.65 ms per token,    23.44 tokens per second)\n",
            "llama_print_timings:       total time =   16779.70 ms /  1876 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.46 ms /     2 runs   (    0.73 ms per token,  1372.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     637.87 ms /   128 tokens (    4.98 ms per token,   200.67 tokens per second)\n",
            "llama_print_timings:        eval time =      75.69 ms /     2 runs   (   37.84 ms per token,    26.42 tokens per second)\n",
            "llama_print_timings:       total time =     721.22 ms /   130 tokens\n",
            " 14%|█▍        | 5/35 [04:06<25:51, 51.71s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.34 ms /    10 runs   (    0.53 ms per token,  1873.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2254.22 ms /   434 tokens (    5.19 ms per token,   192.53 tokens per second)\n",
            "llama_print_timings:        eval time =     343.87 ms /     9 runs   (   38.21 ms per token,    26.17 tokens per second)\n",
            "llama_print_timings:       total time =    2613.61 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      47.56 ms /    85 runs   (    0.56 ms per token,  1787.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9510.07 ms /  1688 tokens (    5.63 ms per token,   177.50 tokens per second)\n",
            "llama_print_timings:        eval time =    3598.29 ms /    85 runs   (   42.33 ms per token,    23.62 tokens per second)\n",
            "llama_print_timings:       total time =   13227.49 ms /  1773 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      92.89 ms /   169 runs   (    0.55 ms per token,  1819.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9363.42 ms /  1656 tokens (    5.65 ms per token,   176.86 tokens per second)\n",
            "llama_print_timings:        eval time =    7154.45 ms /   168 runs   (   42.59 ms per token,    23.48 tokens per second)\n",
            "llama_print_timings:       total time =   16739.28 ms /  1824 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     152.23 ms /   233 runs   (    0.65 ms per token,  1530.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10355.68 ms /  1807 tokens (    5.73 ms per token,   174.49 tokens per second)\n",
            "llama_print_timings:        eval time =   10029.72 ms /   232 runs   (   43.23 ms per token,    23.13 tokens per second)\n",
            "llama_print_timings:       total time =   20761.49 ms /  2039 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1870.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1284.84 ms /   256 tokens (    5.02 ms per token,   199.25 tokens per second)\n",
            "llama_print_timings:        eval time =      37.35 ms /     1 runs   (   37.35 ms per token,    26.78 tokens per second)\n",
            "llama_print_timings:       total time =    1327.11 ms /   257 tokens\n",
            " 17%|█▋        | 6/35 [05:07<26:31, 54.87s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.39 ms /    10 runs   (    0.54 ms per token,  1855.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2230.35 ms /   434 tokens (    5.14 ms per token,   194.59 tokens per second)\n",
            "llama_print_timings:        eval time =     345.24 ms /     9 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    2591.57 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     151.78 ms /   256 runs   (    0.59 ms per token,  1686.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11864.70 ms /  2054 tokens (    5.78 ms per token,   173.12 tokens per second)\n",
            "llama_print_timings:        eval time =   11190.43 ms /   255 runs   (   43.88 ms per token,    22.79 tokens per second)\n",
            "llama_print_timings:       total time =   23422.38 ms /  2309 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     100.81 ms /   166 runs   (    0.61 ms per token,  1646.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11794.05 ms /  2021 tokens (    5.84 ms per token,   171.36 tokens per second)\n",
            "llama_print_timings:        eval time =    7237.65 ms /   165 runs   (   43.86 ms per token,    22.80 tokens per second)\n",
            "llama_print_timings:       total time =   19300.36 ms /  2186 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     140.82 ms /   256 runs   (    0.55 ms per token,  1817.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12734.08 ms /  2176 tokens (    5.85 ms per token,   170.88 tokens per second)\n",
            "llama_print_timings:        eval time =   11234.88 ms /   255 runs   (   44.06 ms per token,    22.70 tokens per second)\n",
            "llama_print_timings:       total time =   24332.63 ms /  2431 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.42 ms /     2 runs   (    0.71 ms per token,  1413.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1256.30 ms /   246 tokens (    5.11 ms per token,   195.81 tokens per second)\n",
            "llama_print_timings:        eval time =      38.03 ms /     1 runs   (   38.03 ms per token,    26.30 tokens per second)\n",
            "llama_print_timings:       total time =    1302.57 ms /   247 tokens\n",
            " 20%|██        | 7/35 [06:26<29:17, 62.79s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.78 ms /    10 runs   (    0.98 ms per token,  1022.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2257.97 ms /   434 tokens (    5.20 ms per token,   192.21 tokens per second)\n",
            "llama_print_timings:        eval time =     343.99 ms /     9 runs   (   38.22 ms per token,    26.16 tokens per second)\n",
            "llama_print_timings:       total time =    2624.24 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.38 ms /    75 runs   (    0.58 ms per token,  1728.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7811.03 ms /  1415 tokens (    5.52 ms per token,   181.15 tokens per second)\n",
            "llama_print_timings:        eval time =    3068.25 ms /    74 runs   (   41.46 ms per token,    24.12 tokens per second)\n",
            "llama_print_timings:       total time =   10984.63 ms /  1489 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      78.78 ms /   132 runs   (    0.60 ms per token,  1675.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7621.70 ms /  1382 tokens (    5.51 ms per token,   181.32 tokens per second)\n",
            "llama_print_timings:        eval time =    5501.88 ms /   131 runs   (   42.00 ms per token,    23.81 tokens per second)\n",
            "llama_print_timings:       total time =   13317.49 ms /  1513 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      52.57 ms /    74 runs   (    0.71 ms per token,  1407.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8398.19 ms /  1498 tokens (    5.61 ms per token,   178.37 tokens per second)\n",
            "llama_print_timings:        eval time =    3093.66 ms /    73 runs   (   42.38 ms per token,    23.60 tokens per second)\n",
            "llama_print_timings:       total time =   11623.66 ms /  1571 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1792.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1085.02 ms /   216 tokens (    5.02 ms per token,   199.07 tokens per second)\n",
            "llama_print_timings:        eval time =      75.15 ms /     2 runs   (   37.58 ms per token,    26.61 tokens per second)\n",
            "llama_print_timings:       total time =    1164.85 ms /   218 tokens\n",
            " 23%|██▎       | 8/35 [07:12<25:52, 57.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.22 ms /    10 runs   (    0.52 ms per token,  1914.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2238.74 ms /   434 tokens (    5.16 ms per token,   193.86 tokens per second)\n",
            "llama_print_timings:        eval time =     344.55 ms /     9 runs   (   38.28 ms per token,    26.12 tokens per second)\n",
            "llama_print_timings:       total time =    2599.96 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1793.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6724.00 ms /  1229 tokens (    5.47 ms per token,   182.78 tokens per second)\n",
            "llama_print_timings:        eval time =      40.18 ms /     1 runs   (   40.18 ms per token,    24.89 tokens per second)\n",
            "llama_print_timings:       total time =    6783.56 ms /  1230 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      96.91 ms /   139 runs   (    0.70 ms per token,  1434.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6487.01 ms /  1196 tokens (    5.42 ms per token,   184.37 tokens per second)\n",
            "llama_print_timings:        eval time =    5692.72 ms /   138 runs   (   41.25 ms per token,    24.24 tokens per second)\n",
            "llama_print_timings:       total time =   12403.79 ms /  1334 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      84.88 ms /   131 runs   (    0.65 ms per token,  1543.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7272.30 ms /  1322 tokens (    5.50 ms per token,   181.79 tokens per second)\n",
            "llama_print_timings:        eval time =    5410.22 ms /   130 runs   (   41.62 ms per token,    24.03 tokens per second)\n",
            "llama_print_timings:       total time =   12881.99 ms /  1452 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.55 ms per token,  1831.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1126.12 ms /   221 tokens (    5.10 ms per token,   196.25 tokens per second)\n",
            "llama_print_timings:        eval time =      37.18 ms /     1 runs   (   37.18 ms per token,    26.89 tokens per second)\n",
            "llama_print_timings:       total time =    1170.61 ms /   222 tokens\n",
            " 26%|██▌       | 9/35 [07:55<22:53, 52.83s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.62 ms /    10 runs   (    0.56 ms per token,  1780.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2232.72 ms /   434 tokens (    5.14 ms per token,   194.38 tokens per second)\n",
            "llama_print_timings:        eval time =     344.74 ms /     9 runs   (   38.30 ms per token,    26.11 tokens per second)\n",
            "llama_print_timings:       total time =    2593.33 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      60.04 ms /    82 runs   (    0.73 ms per token,  1365.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9374.58 ms /  1669 tokens (    5.62 ms per token,   178.03 tokens per second)\n",
            "llama_print_timings:        eval time =    3461.02 ms /    81 runs   (   42.73 ms per token,    23.40 tokens per second)\n",
            "llama_print_timings:       total time =   12983.96 ms /  1750 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     157.25 ms /   256 runs   (    0.61 ms per token,  1627.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9237.56 ms /  1636 tokens (    5.65 ms per token,   177.10 tokens per second)\n",
            "llama_print_timings:        eval time =   10967.37 ms /   255 runs   (   43.01 ms per token,    23.25 tokens per second)\n",
            "llama_print_timings:       total time =   20588.21 ms /  1891 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     164.58 ms /   256 runs   (    0.64 ms per token,  1555.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10808.23 ms /  1875 tokens (    5.76 ms per token,   173.48 tokens per second)\n",
            "llama_print_timings:        eval time =   11040.38 ms /   255 runs   (   43.30 ms per token,    23.10 tokens per second)\n",
            "llama_print_timings:       total time =   22270.65 ms /  2130 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1865.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1731.25 ms /   344 tokens (    5.03 ms per token,   198.70 tokens per second)\n",
            "llama_print_timings:        eval time =      37.53 ms /     1 runs   (   37.53 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =    1777.12 ms /   345 tokens\n",
            " 29%|██▊       | 10/35 [09:06<24:21, 58.47s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.45 ms /    10 runs   (    0.55 ms per token,  1833.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2227.62 ms /   434 tokens (    5.13 ms per token,   194.83 tokens per second)\n",
            "llama_print_timings:        eval time =     342.70 ms /     9 runs   (   38.08 ms per token,    26.26 tokens per second)\n",
            "llama_print_timings:       total time =    2585.47 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      87.09 ms /   125 runs   (    0.70 ms per token,  1435.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10124.59 ms /  1792 tokens (    5.65 ms per token,   176.99 tokens per second)\n",
            "llama_print_timings:        eval time =    5378.07 ms /   125 runs   (   43.02 ms per token,    23.24 tokens per second)\n",
            "llama_print_timings:       total time =   15716.40 ms /  1917 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     126.72 ms /   205 runs   (    0.62 ms per token,  1617.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10012.08 ms /  1760 tokens (    5.69 ms per token,   175.79 tokens per second)\n",
            "llama_print_timings:        eval time =    8825.27 ms /   204 runs   (   43.26 ms per token,    23.12 tokens per second)\n",
            "llama_print_timings:       total time =   19139.42 ms /  1964 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     156.40 ms /   256 runs   (    0.61 ms per token,  1636.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11282.66 ms /  1950 tokens (    5.79 ms per token,   172.83 tokens per second)\n",
            "llama_print_timings:        eval time =   11092.56 ms /   255 runs   (   43.50 ms per token,    22.99 tokens per second)\n",
            "llama_print_timings:       total time =   22783.73 ms /  2205 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.54 ms per token,  1838.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1442.74 ms /   288 tokens (    5.01 ms per token,   199.62 tokens per second)\n",
            "llama_print_timings:        eval time =      75.64 ms /     2 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
            "llama_print_timings:       total time =    1523.95 ms /   290 tokens\n",
            " 31%|███▏      | 11/35 [10:15<24:41, 61.75s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.30 ms /    10 runs   (    0.53 ms per token,  1886.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2226.23 ms /   434 tokens (    5.13 ms per token,   194.95 tokens per second)\n",
            "llama_print_timings:        eval time =     345.30 ms /     9 runs   (   38.37 ms per token,    26.06 tokens per second)\n",
            "llama_print_timings:       total time =    2586.96 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.43 ms /     2 runs   (    0.71 ms per token,  1403.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6150.06 ms /  1139 tokens (    5.40 ms per token,   185.20 tokens per second)\n",
            "llama_print_timings:        eval time =      39.95 ms /     1 runs   (   39.95 ms per token,    25.03 tokens per second)\n",
            "llama_print_timings:       total time =    6206.61 ms /  1140 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      56.55 ms /    79 runs   (    0.72 ms per token,  1396.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5969.05 ms /  1106 tokens (    5.40 ms per token,   185.29 tokens per second)\n",
            "llama_print_timings:        eval time =    3191.75 ms /    78 runs   (   40.92 ms per token,    24.44 tokens per second)\n",
            "llama_print_timings:       total time =    9293.07 ms /  1184 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      61.53 ms /   106 runs   (    0.58 ms per token,  1722.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6392.28 ms /  1175 tokens (    5.44 ms per token,   183.82 tokens per second)\n",
            "llama_print_timings:        eval time =    4321.21 ms /   105 runs   (   41.15 ms per token,    24.30 tokens per second)\n",
            "llama_print_timings:       total time =   10854.28 ms /  1280 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.45 ms /     2 runs   (    0.73 ms per token,  1378.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     800.71 ms /   158 tokens (    5.07 ms per token,   197.33 tokens per second)\n",
            "llama_print_timings:        eval time =      37.74 ms /     1 runs   (   37.74 ms per token,    26.50 tokens per second)\n",
            "llama_print_timings:       total time =     849.23 ms /   159 tokens\n",
            " 34%|███▍      | 12/35 [10:53<20:53, 54.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.84 ms /    10 runs   (    0.68 ms per token,  1462.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2274.83 ms /   434 tokens (    5.24 ms per token,   190.78 tokens per second)\n",
            "llama_print_timings:        eval time =     352.10 ms /     9 runs   (   39.12 ms per token,    25.56 tokens per second)\n",
            "llama_print_timings:       total time =    2646.53 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1693.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6870.13 ms /  1256 tokens (    5.47 ms per token,   182.82 tokens per second)\n",
            "llama_print_timings:        eval time =      82.23 ms /     2 runs   (   41.11 ms per token,    24.32 tokens per second)\n",
            "llama_print_timings:       total time =    6971.80 ms /  1258 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      24.99 ms /    38 runs   (    0.66 ms per token,  1520.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6646.48 ms /  1224 tokens (    5.43 ms per token,   184.16 tokens per second)\n",
            "llama_print_timings:        eval time =    1523.61 ms /    37 runs   (   41.18 ms per token,    24.28 tokens per second)\n",
            "llama_print_timings:       total time =    8232.63 ms /  1261 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      70.77 ms /   125 runs   (    0.57 ms per token,  1766.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6873.10 ms /  1247 tokens (    5.51 ms per token,   181.43 tokens per second)\n",
            "llama_print_timings:        eval time =    5133.49 ms /   124 runs   (   41.40 ms per token,    24.16 tokens per second)\n",
            "llama_print_timings:       total time =   12171.53 ms /  1371 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       0.97 ms /     2 runs   (    0.48 ms per token,  2061.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     632.41 ms /   122 tokens (    5.18 ms per token,   192.91 tokens per second)\n",
            "llama_print_timings:        eval time =      37.25 ms /     1 runs   (   37.25 ms per token,    26.84 tokens per second)\n",
            "llama_print_timings:       total time =     673.52 ms /   123 tokens\n",
            " 37%|███▋      | 13/35 [11:34<18:31, 50.50s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.10 ms /    10 runs   (    0.71 ms per token,  1408.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2261.68 ms /   434 tokens (    5.21 ms per token,   191.89 tokens per second)\n",
            "llama_print_timings:        eval time =     350.45 ms /     9 runs   (   38.94 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =    2633.86 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.54 ms /     2 runs   (    0.77 ms per token,  1295.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5550.68 ms /  1040 tokens (    5.34 ms per token,   187.36 tokens per second)\n",
            "llama_print_timings:        eval time =      80.52 ms /     2 runs   (   40.26 ms per token,    24.84 tokens per second)\n",
            "llama_print_timings:       total time =    5647.84 ms /  1042 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      79.88 ms /   142 runs   (    0.56 ms per token,  1777.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5400.52 ms /  1008 tokens (    5.36 ms per token,   186.65 tokens per second)\n",
            "llama_print_timings:        eval time =    5684.01 ms /   141 runs   (   40.31 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =   11259.91 ms /  1149 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      39.69 ms /    71 runs   (    0.56 ms per token,  1788.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6238.28 ms /  1139 tokens (    5.48 ms per token,   182.58 tokens per second)\n",
            "llama_print_timings:        eval time =    2855.12 ms /    70 runs   (   40.79 ms per token,    24.52 tokens per second)\n",
            "llama_print_timings:       total time =    9186.86 ms /  1209 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1694.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1119.60 ms /   222 tokens (    5.04 ms per token,   198.28 tokens per second)\n",
            "llama_print_timings:        eval time =      37.40 ms /     1 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
            "llama_print_timings:       total time =    1162.72 ms /   223 tokens\n",
            " 40%|████      | 14/35 [12:12<16:21, 46.74s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.11 ms /    10 runs   (    0.51 ms per token,  1957.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2235.11 ms /   434 tokens (    5.15 ms per token,   194.17 tokens per second)\n",
            "llama_print_timings:        eval time =     347.67 ms /     9 runs   (   38.63 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    2597.35 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1853.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4692.42 ms /   875 tokens (    5.36 ms per token,   186.47 tokens per second)\n",
            "llama_print_timings:        eval time =      39.26 ms /     1 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    4746.07 ms /   876 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      94.97 ms /   156 runs   (    0.61 ms per token,  1642.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4454.89 ms /   842 tokens (    5.29 ms per token,   189.01 tokens per second)\n",
            "llama_print_timings:        eval time =    6175.88 ms /   155 runs   (   39.84 ms per token,    25.10 tokens per second)\n",
            "llama_print_timings:       total time =   10842.46 ms /   997 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      50.75 ms /    86 runs   (    0.59 ms per token,  1694.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5312.46 ms /   989 tokens (    5.37 ms per token,   186.17 tokens per second)\n",
            "llama_print_timings:        eval time =    3418.96 ms /    85 runs   (   40.22 ms per token,    24.86 tokens per second)\n",
            "llama_print_timings:       total time =    8843.01 ms /  1074 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.21 ms /     2 runs   (    0.60 ms per token,  1658.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1196.20 ms /   234 tokens (    5.11 ms per token,   195.62 tokens per second)\n",
            "llama_print_timings:        eval time =      37.35 ms /     1 runs   (   37.35 ms per token,    26.77 tokens per second)\n",
            "llama_print_timings:       total time =    1238.86 ms /   235 tokens\n",
            " 43%|████▎     | 15/35 [12:44<14:05, 42.26s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       7.09 ms /    10 runs   (    0.71 ms per token,  1410.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2233.00 ms /   434 tokens (    5.15 ms per token,   194.36 tokens per second)\n",
            "llama_print_timings:        eval time =     352.76 ms /     9 runs   (   39.20 ms per token,    25.51 tokens per second)\n",
            "llama_print_timings:       total time =    2605.66 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      57.96 ms /    84 runs   (    0.69 ms per token,  1449.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4902.92 ms /   927 tokens (    5.29 ms per token,   189.07 tokens per second)\n",
            "llama_print_timings:        eval time =    3326.94 ms /    83 runs   (   40.08 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    8362.28 ms /  1010 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      73.00 ms /   128 runs   (    0.57 ms per token,  1753.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4779.09 ms /   894 tokens (    5.35 ms per token,   187.06 tokens per second)\n",
            "llama_print_timings:        eval time =    5079.73 ms /   127 runs   (   40.00 ms per token,    25.00 tokens per second)\n",
            "llama_print_timings:       total time =   10017.06 ms /  1021 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.65 ms /     2 runs   (    0.82 ms per token,  1214.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5385.60 ms /  1000 tokens (    5.39 ms per token,   185.68 tokens per second)\n",
            "llama_print_timings:        eval time =      40.61 ms /     1 runs   (   40.61 ms per token,    24.62 tokens per second)\n",
            "llama_print_timings:       total time =    5444.66 ms /  1001 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.37 ms /     2 runs   (    0.68 ms per token,  1460.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1131.68 ms /   219 tokens (    5.17 ms per token,   193.52 tokens per second)\n",
            "llama_print_timings:        eval time =      37.54 ms /     1 runs   (   37.54 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =    1175.34 ms /   220 tokens\n",
            " 46%|████▌     | 16/35 [13:19<12:40, 40.02s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.46 ms /    10 runs   (    0.55 ms per token,  1832.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2250.35 ms /   434 tokens (    5.19 ms per token,   192.86 tokens per second)\n",
            "llama_print_timings:        eval time =     345.17 ms /     9 runs   (   38.35 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    2611.01 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.48 ms /     2 runs   (    0.74 ms per token,  1353.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4926.65 ms /   928 tokens (    5.31 ms per token,   188.36 tokens per second)\n",
            "llama_print_timings:        eval time =      80.65 ms /     2 runs   (   40.33 ms per token,    24.80 tokens per second)\n",
            "llama_print_timings:       total time =    5022.50 ms /   930 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     128.49 ms /   223 runs   (    0.58 ms per token,  1735.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4784.17 ms /   896 tokens (    5.34 ms per token,   187.28 tokens per second)\n",
            "llama_print_timings:        eval time =    8901.31 ms /   222 runs   (   40.10 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =   13975.07 ms /  1118 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1765.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5986.89 ms /  1103 tokens (    5.43 ms per token,   184.24 tokens per second)\n",
            "llama_print_timings:        eval time =      39.84 ms /     1 runs   (   39.84 ms per token,    25.10 tokens per second)\n",
            "llama_print_timings:       total time =    6046.04 ms /  1104 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1752.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1563.12 ms /   308 tokens (    5.08 ms per token,   197.04 tokens per second)\n",
            "llama_print_timings:        eval time =      37.52 ms /     1 runs   (   37.52 ms per token,    26.65 tokens per second)\n",
            "llama_print_timings:       total time =    1607.14 ms /   309 tokens\n",
            " 49%|████▊     | 17/35 [13:52<11:20, 37.82s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.23 ms /    10 runs   (    0.52 ms per token,  1912.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2235.05 ms /   434 tokens (    5.15 ms per token,   194.18 tokens per second)\n",
            "llama_print_timings:        eval time =     346.75 ms /     9 runs   (   38.53 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    2596.20 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.24 ms /     2 runs   (    0.62 ms per token,  1615.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6873.16 ms /  1250 tokens (    5.50 ms per token,   181.87 tokens per second)\n",
            "llama_print_timings:        eval time =      40.72 ms /     1 runs   (   40.72 ms per token,    24.56 tokens per second)\n",
            "llama_print_timings:       total time =    6932.72 ms /  1251 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      57.34 ms /    92 runs   (    0.62 ms per token,  1604.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6592.73 ms /  1216 tokens (    5.42 ms per token,   184.45 tokens per second)\n",
            "llama_print_timings:        eval time =    3776.47 ms /    92 runs   (   41.05 ms per token,    24.36 tokens per second)\n",
            "llama_print_timings:       total time =   10501.39 ms /  1308 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1771.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7196.15 ms /  1306 tokens (    5.51 ms per token,   181.49 tokens per second)\n",
            "llama_print_timings:        eval time =      40.50 ms /     1 runs   (   40.50 ms per token,    24.69 tokens per second)\n",
            "llama_print_timings:       total time =    7257.40 ms /  1307 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.19 ms /     2 runs   (    0.59 ms per token,  1687.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     829.21 ms /   164 tokens (    5.06 ms per token,   197.78 tokens per second)\n",
            "llama_print_timings:        eval time =      37.77 ms /     1 runs   (   37.77 ms per token,    26.48 tokens per second)\n",
            "llama_print_timings:       total time =     872.27 ms /   165 tokens\n",
            " 51%|█████▏    | 18/35 [14:22<10:05, 35.61s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.55 ms /    10 runs   (    0.56 ms per token,  1801.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2226.56 ms /   434 tokens (    5.13 ms per token,   194.92 tokens per second)\n",
            "llama_print_timings:        eval time =     343.79 ms /     9 runs   (   38.20 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    2586.43 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1819.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5036.16 ms /   951 tokens (    5.30 ms per token,   188.83 tokens per second)\n",
            "llama_print_timings:        eval time =      38.95 ms /     1 runs   (   38.95 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =    5087.95 ms /   952 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      56.07 ms /    96 runs   (    0.58 ms per token,  1712.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4903.53 ms /   918 tokens (    5.34 ms per token,   187.21 tokens per second)\n",
            "llama_print_timings:        eval time =    3793.77 ms /    95 runs   (   39.93 ms per token,    25.04 tokens per second)\n",
            "llama_print_timings:       total time =    8820.87 ms /  1013 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      47.47 ms /    66 runs   (    0.72 ms per token,  1390.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5433.59 ms /  1011 tokens (    5.37 ms per token,   186.06 tokens per second)\n",
            "llama_print_timings:        eval time =    2644.42 ms /    65 runs   (   40.68 ms per token,    24.58 tokens per second)\n",
            "llama_print_timings:       total time =    8180.11 ms /  1076 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.42 ms /     2 runs   (    0.71 ms per token,  1411.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     843.80 ms /   168 tokens (    5.02 ms per token,   199.10 tokens per second)\n",
            "llama_print_timings:        eval time =      38.22 ms /     1 runs   (   38.22 ms per token,    26.16 tokens per second)\n",
            "llama_print_timings:       total time =     891.27 ms /   169 tokens\n",
            " 54%|█████▍    | 19/35 [14:56<09:20, 35.03s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.36 ms /    10 runs   (    0.54 ms per token,  1864.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2267.50 ms /   434 tokens (    5.22 ms per token,   191.40 tokens per second)\n",
            "llama_print_timings:        eval time =     349.28 ms /     9 runs   (   38.81 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    2633.22 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1853.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4425.56 ms /   840 tokens (    5.27 ms per token,   189.81 tokens per second)\n",
            "llama_print_timings:        eval time =      78.75 ms /     2 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    4516.45 ms /   842 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      61.47 ms /   106 runs   (    0.58 ms per token,  1724.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4300.70 ms /   808 tokens (    5.32 ms per token,   187.88 tokens per second)\n",
            "llama_print_timings:        eval time =    4154.20 ms /   105 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    8593.26 ms /   913 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     105.67 ms /   154 runs   (    0.69 ms per token,  1457.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4830.69 ms /   909 tokens (    5.31 ms per token,   188.17 tokens per second)\n",
            "llama_print_timings:        eval time =    6166.46 ms /   153 runs   (   40.30 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =   11235.66 ms /  1062 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1662.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     916.16 ms /   180 tokens (    5.09 ms per token,   196.47 tokens per second)\n",
            "llama_print_timings:        eval time =      37.87 ms /     1 runs   (   37.87 ms per token,    26.40 tokens per second)\n",
            "llama_print_timings:       total time =     959.61 ms /   181 tokens\n",
            " 57%|█████▋    | 20/35 [15:29<08:34, 34.29s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.30 ms /    10 runs   (    0.53 ms per token,  1886.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.49 ms /   434 tokens (    5.14 ms per token,   194.49 tokens per second)\n",
            "llama_print_timings:        eval time =     344.76 ms /     9 runs   (   38.31 ms per token,    26.10 tokens per second)\n",
            "llama_print_timings:       total time =    2591.35 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      48.52 ms /    85 runs   (    0.57 ms per token,  1751.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10738.93 ms /  1886 tokens (    5.69 ms per token,   175.62 tokens per second)\n",
            "llama_print_timings:        eval time =    3625.19 ms /    84 runs   (   43.16 ms per token,    23.17 tokens per second)\n",
            "llama_print_timings:       total time =   14494.63 ms /  1970 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       9.68 ms /    16 runs   (    0.61 ms per token,  1652.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10640.83 ms /  1853 tokens (    5.74 ms per token,   174.14 tokens per second)\n",
            "llama_print_timings:        eval time =     654.43 ms /    15 runs   (   43.63 ms per token,    22.92 tokens per second)\n",
            "llama_print_timings:       total time =   11338.92 ms /  1868 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      75.87 ms /   108 runs   (    0.70 ms per token,  1423.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10768.57 ms /  1868 tokens (    5.76 ms per token,   173.47 tokens per second)\n",
            "llama_print_timings:        eval time =    4646.48 ms /   107 runs   (   43.43 ms per token,    23.03 tokens per second)\n",
            "llama_print_timings:       total time =   15607.55 ms /  1975 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       0.98 ms /     2 runs   (    0.49 ms per token,  2047.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     435.97 ms /    86 tokens (    5.07 ms per token,   197.26 tokens per second)\n",
            "llama_print_timings:        eval time =      37.32 ms /     1 runs   (   37.32 ms per token,    26.80 tokens per second)\n",
            "llama_print_timings:       total time =     476.55 ms /    87 tokens\n",
            " 60%|██████    | 21/35 [16:22<09:18, 39.92s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.22 ms /    10 runs   (    0.52 ms per token,  1915.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2232.77 ms /   434 tokens (    5.14 ms per token,   194.38 tokens per second)\n",
            "llama_print_timings:        eval time =     346.25 ms /     9 runs   (   38.47 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2594.24 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.47 ms /     2 runs   (    0.73 ms per token,  1363.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6770.14 ms /  1240 tokens (    5.46 ms per token,   183.16 tokens per second)\n",
            "llama_print_timings:        eval time =      82.81 ms /     2 runs   (   41.40 ms per token,    24.15 tokens per second)\n",
            "llama_print_timings:       total time =    6872.32 ms /  1242 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     132.00 ms /   213 runs   (    0.62 ms per token,  1613.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6541.66 ms /  1208 tokens (    5.42 ms per token,   184.66 tokens per second)\n",
            "llama_print_timings:        eval time =    8730.74 ms /   212 runs   (   41.18 ms per token,    24.28 tokens per second)\n",
            "llama_print_timings:       total time =   15593.24 ms /  1420 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.26 ms /     2 runs   (    0.63 ms per token,  1592.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7798.78 ms /  1415 tokens (    5.51 ms per token,   181.44 tokens per second)\n",
            "llama_print_timings:        eval time =      41.03 ms /     1 runs   (   41.03 ms per token,    24.37 tokens per second)\n",
            "llama_print_timings:       total time =    7859.09 ms /  1416 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.44 ms /     2 runs   (    0.72 ms per token,  1391.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1455.28 ms /   288 tokens (    5.05 ms per token,   197.90 tokens per second)\n",
            "llama_print_timings:        eval time =      38.26 ms /     1 runs   (   38.26 ms per token,    26.14 tokens per second)\n",
            "llama_print_timings:       total time =    1500.78 ms /   289 tokens\n",
            " 63%|██████▎   | 22/35 [17:00<08:34, 39.61s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.76 ms /    10 runs   (    0.68 ms per token,  1478.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2260.54 ms /   434 tokens (    5.21 ms per token,   191.99 tokens per second)\n",
            "llama_print_timings:        eval time =     349.30 ms /     9 runs   (   38.81 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    2629.67 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.50 ms /     2 runs   (    0.75 ms per token,  1337.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4962.28 ms /   936 tokens (    5.30 ms per token,   188.62 tokens per second)\n",
            "llama_print_timings:        eval time =      39.99 ms /     1 runs   (   39.99 ms per token,    25.01 tokens per second)\n",
            "llama_print_timings:       total time =    5017.01 ms /   937 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      20.91 ms /    37 runs   (    0.57 ms per token,  1769.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4796.97 ms /   903 tokens (    5.31 ms per token,   188.24 tokens per second)\n",
            "llama_print_timings:        eval time =    1435.85 ms /    36 runs   (   39.88 ms per token,    25.07 tokens per second)\n",
            "llama_print_timings:       total time =    6285.04 ms /   939 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      19.78 ms /    33 runs   (    0.60 ms per token,  1668.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4968.77 ms /   936 tokens (    5.31 ms per token,   188.38 tokens per second)\n",
            "llama_print_timings:        eval time =    1319.61 ms /    33 runs   (   39.99 ms per token,    25.01 tokens per second)\n",
            "llama_print_timings:       total time =    6339.06 ms /   969 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.29 ms /     2 runs   (    0.64 ms per token,  1552.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     556.81 ms /   109 tokens (    5.11 ms per token,   195.76 tokens per second)\n",
            "llama_print_timings:        eval time =      37.50 ms /     1 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =     598.53 ms /   110 tokens\n",
            " 66%|██████▌   | 23/35 [17:29<07:15, 36.29s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.70 ms /    10 runs   (    0.67 ms per token,  1493.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2269.24 ms /   434 tokens (    5.23 ms per token,   191.25 tokens per second)\n",
            "llama_print_timings:        eval time =     349.77 ms /     9 runs   (   38.86 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    2638.67 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      30.85 ms /    43 runs   (    0.72 ms per token,  1393.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8850.28 ms /  1573 tokens (    5.63 ms per token,   177.73 tokens per second)\n",
            "llama_print_timings:        eval time =    1782.58 ms /    42 runs   (   42.44 ms per token,    23.56 tokens per second)\n",
            "llama_print_timings:       total time =   10714.48 ms /  1615 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      50.34 ms /    86 runs   (    0.59 ms per token,  1708.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8610.22 ms /  1540 tokens (    5.59 ms per token,   178.86 tokens per second)\n",
            "llama_print_timings:        eval time =    3586.85 ms /    85 runs   (   42.20 ms per token,    23.70 tokens per second)\n",
            "llama_print_timings:       total time =   12326.67 ms /  1625 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      34.29 ms /    65 runs   (    0.53 ms per token,  1895.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9121.52 ms /  1622 tokens (    5.62 ms per token,   177.82 tokens per second)\n",
            "llama_print_timings:        eval time =    2697.09 ms /    64 runs   (   42.14 ms per token,    23.73 tokens per second)\n",
            "llama_print_timings:       total time =   11910.20 ms /  1686 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.46 ms /     2 runs   (    0.73 ms per token,  1374.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     798.74 ms /   159 tokens (    5.02 ms per token,   199.06 tokens per second)\n",
            "llama_print_timings:        eval time =      37.95 ms /     1 runs   (   37.95 ms per token,    26.35 tokens per second)\n",
            "llama_print_timings:       total time =     841.70 ms /   160 tokens\n",
            " 69%|██████▊   | 24/35 [18:12<07:01, 38.36s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.87 ms /    10 runs   (    0.69 ms per token,  1455.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2254.11 ms /   434 tokens (    5.19 ms per token,   192.54 tokens per second)\n",
            "llama_print_timings:        eval time =     348.03 ms /     9 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    2623.97 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      61.13 ms /   114 runs   (    0.54 ms per token,  1864.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9243.31 ms /  1644 tokens (    5.62 ms per token,   177.86 tokens per second)\n",
            "llama_print_timings:        eval time =    4768.33 ms /   113 runs   (   42.20 ms per token,    23.70 tokens per second)\n",
            "llama_print_timings:       total time =   14159.37 ms /  1757 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     147.36 ms /   256 runs   (    0.58 ms per token,  1737.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9105.12 ms /  1611 tokens (    5.65 ms per token,   176.93 tokens per second)\n",
            "llama_print_timings:        eval time =   10878.69 ms /   255 runs   (   42.66 ms per token,    23.44 tokens per second)\n",
            "llama_print_timings:       total time =   20346.59 ms /  1866 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      97.30 ms /   149 runs   (    0.65 ms per token,  1531.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10633.76 ms /  1855 tokens (    5.73 ms per token,   174.44 tokens per second)\n",
            "llama_print_timings:        eval time =    6395.43 ms /   148 runs   (   43.21 ms per token,    23.14 tokens per second)\n",
            "llama_print_timings:       total time =   17276.05 ms /  2003 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1913.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1727.71 ms /   339 tokens (    5.10 ms per token,   196.21 tokens per second)\n",
            "llama_print_timings:        eval time =      37.45 ms /     1 runs   (   37.45 ms per token,    26.70 tokens per second)\n",
            "llama_print_timings:       total time =    1771.57 ms /   340 tokens\n",
            " 71%|███████▏  | 25/35 [19:15<07:36, 45.65s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.51 ms /    10 runs   (    0.55 ms per token,  1813.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2234.27 ms /   434 tokens (    5.15 ms per token,   194.25 tokens per second)\n",
            "llama_print_timings:        eval time =     345.08 ms /     9 runs   (   38.34 ms per token,    26.08 tokens per second)\n",
            "llama_print_timings:       total time =    2595.42 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      76.81 ms /   137 runs   (    0.56 ms per token,  1783.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8328.43 ms /  1496 tokens (    5.57 ms per token,   179.63 tokens per second)\n",
            "llama_print_timings:        eval time =    5725.63 ms /   137 runs   (   41.79 ms per token,    23.93 tokens per second)\n",
            "llama_print_timings:       total time =   14233.31 ms /  1633 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      58.78 ms /   108 runs   (    0.54 ms per token,  1837.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8137.37 ms /  1464 tokens (    5.56 ms per token,   179.91 tokens per second)\n",
            "llama_print_timings:        eval time =    4464.24 ms /   107 runs   (   41.72 ms per token,    23.97 tokens per second)\n",
            "llama_print_timings:       total time =   12743.27 ms /  1571 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      51.66 ms /    92 runs   (    0.56 ms per token,  1780.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8730.45 ms /  1559 tokens (    5.60 ms per token,   178.57 tokens per second)\n",
            "llama_print_timings:        eval time =    3827.09 ms /    91 runs   (   42.06 ms per token,    23.78 tokens per second)\n",
            "llama_print_timings:       total time =   12681.10 ms /  1650 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1771.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     954.86 ms /   190 tokens (    5.03 ms per token,   198.98 tokens per second)\n",
            "llama_print_timings:        eval time =      37.64 ms /     1 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
            "llama_print_timings:       total time =     996.84 ms /   191 tokens\n",
            " 74%|███████▍  | 26/35 [20:00<06:50, 45.61s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.31 ms /    10 runs   (    0.53 ms per token,  1883.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.03 ms /   434 tokens (    5.14 ms per token,   194.53 tokens per second)\n",
            "llama_print_timings:        eval time =     343.72 ms /     9 runs   (   38.19 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    2589.23 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1879.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5453.55 ms /  1024 tokens (    5.33 ms per token,   187.77 tokens per second)\n",
            "llama_print_timings:        eval time =      39.53 ms /     1 runs   (   39.53 ms per token,    25.29 tokens per second)\n",
            "llama_print_timings:       total time =    5507.23 ms /  1025 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      56.56 ms /    95 runs   (    0.60 ms per token,  1679.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5315.52 ms /   991 tokens (    5.36 ms per token,   186.44 tokens per second)\n",
            "llama_print_timings:        eval time =    3797.10 ms /    94 runs   (   40.39 ms per token,    24.76 tokens per second)\n",
            "llama_print_timings:       total time =    9240.43 ms /  1085 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1697.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5858.34 ms /  1087 tokens (    5.39 ms per token,   185.55 tokens per second)\n",
            "llama_print_timings:        eval time =      40.26 ms /     1 runs   (   40.26 ms per token,    24.84 tokens per second)\n",
            "llama_print_timings:       total time =    5913.36 ms /  1088 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.35 ms /     2 runs   (    0.67 ms per token,  1485.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     830.66 ms /   163 tokens (    5.10 ms per token,   196.23 tokens per second)\n",
            "llama_print_timings:        eval time =      37.55 ms /     1 runs   (   37.55 ms per token,    26.63 tokens per second)\n",
            "llama_print_timings:       total time =     875.39 ms /   164 tokens\n",
            " 77%|███████▋  | 27/35 [20:31<05:30, 41.27s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.70 ms /    10 runs   (    0.67 ms per token,  1492.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2261.12 ms /   434 tokens (    5.21 ms per token,   191.94 tokens per second)\n",
            "llama_print_timings:        eval time =     348.82 ms /     9 runs   (   38.76 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =    2628.43 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1706.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5467.09 ms /  1018 tokens (    5.37 ms per token,   186.21 tokens per second)\n",
            "llama_print_timings:        eval time =      39.59 ms /     1 runs   (   39.59 ms per token,    25.26 tokens per second)\n",
            "llama_print_timings:       total time =    5520.73 ms /  1019 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      59.24 ms /    95 runs   (    0.62 ms per token,  1603.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5272.29 ms /   984 tokens (    5.36 ms per token,   186.64 tokens per second)\n",
            "llama_print_timings:        eval time =    3835.97 ms /    95 runs   (   40.38 ms per token,    24.77 tokens per second)\n",
            "llama_print_timings:       total time =    9237.73 ms /  1079 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1790.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5801.31 ms /  1078 tokens (    5.38 ms per token,   185.82 tokens per second)\n",
            "llama_print_timings:        eval time =      39.73 ms /     1 runs   (   39.73 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =    5855.63 ms /  1079 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1784.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     831.09 ms /   166 tokens (    5.01 ms per token,   199.74 tokens per second)\n",
            "llama_print_timings:        eval time =      37.39 ms /     1 runs   (   37.39 ms per token,    26.74 tokens per second)\n",
            "llama_print_timings:       total time =     873.04 ms /   167 tokens\n",
            " 80%|████████  | 28/35 [21:00<04:22, 37.56s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.60 ms /    10 runs   (    0.66 ms per token,  1514.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2252.36 ms /   434 tokens (    5.19 ms per token,   192.69 tokens per second)\n",
            "llama_print_timings:        eval time =     350.49 ms /     9 runs   (   38.94 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =    2622.80 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      43.33 ms /    63 runs   (    0.69 ms per token,  1453.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6880.15 ms /  1264 tokens (    5.44 ms per token,   183.72 tokens per second)\n",
            "llama_print_timings:        eval time =    2597.14 ms /    63 runs   (   41.22 ms per token,    24.26 tokens per second)\n",
            "llama_print_timings:       total time =    9580.90 ms /  1327 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      60.42 ms /    97 runs   (    0.62 ms per token,  1605.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6693.47 ms /  1232 tokens (    5.43 ms per token,   184.06 tokens per second)\n",
            "llama_print_timings:        eval time =    3961.90 ms /    96 runs   (   41.27 ms per token,    24.23 tokens per second)\n",
            "llama_print_timings:       total time =   10792.26 ms /  1328 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      39.31 ms /    71 runs   (    0.55 ms per token,  1806.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7249.06 ms /  1308 tokens (    5.54 ms per token,   180.44 tokens per second)\n",
            "llama_print_timings:        eval time =    2900.71 ms /    70 runs   (   41.44 ms per token,    24.13 tokens per second)\n",
            "llama_print_timings:       total time =   10244.21 ms /  1378 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1860.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     957.87 ms /   187 tokens (    5.12 ms per token,   195.22 tokens per second)\n",
            "llama_print_timings:        eval time =      37.58 ms /     1 runs   (   37.58 ms per token,    26.61 tokens per second)\n",
            "llama_print_timings:       total time =    1001.40 ms /   188 tokens\n",
            " 83%|████████▎ | 29/35 [21:42<03:53, 38.91s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.79 ms /    10 runs   (    0.68 ms per token,  1473.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2243.57 ms /   434 tokens (    5.17 ms per token,   193.44 tokens per second)\n",
            "llama_print_timings:        eval time =     350.01 ms /     9 runs   (   38.89 ms per token,    25.71 tokens per second)\n",
            "llama_print_timings:       total time =    2612.76 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     145.88 ms /   256 runs   (    0.57 ms per token,  1754.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11794.41 ms /  2040 tokens (    5.78 ms per token,   172.96 tokens per second)\n",
            "llama_print_timings:        eval time =   11169.55 ms /   255 runs   (   43.80 ms per token,    22.83 tokens per second)\n",
            "llama_print_timings:       total time =   23346.08 ms /  2295 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     139.54 ms /   247 runs   (    0.56 ms per token,  1770.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11613.71 ms /  2007 tokens (    5.79 ms per token,   172.81 tokens per second)\n",
            "llama_print_timings:        eval time =   10766.82 ms /   246 runs   (   43.77 ms per token,    22.85 tokens per second)\n",
            "llama_print_timings:       total time =   22749.43 ms /  2253 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =     144.24 ms /   256 runs   (    0.56 ms per token,  1774.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13184.42 ms /  2239 tokens (    5.89 ms per token,   169.82 tokens per second)\n",
            "llama_print_timings:        eval time =   11449.01 ms /   255 runs   (   44.90 ms per token,    22.27 tokens per second)\n",
            "llama_print_timings:       total time =   25021.36 ms /  2494 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1736.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1683.51 ms /   331 tokens (    5.09 ms per token,   196.61 tokens per second)\n",
            "llama_print_timings:        eval time =      37.44 ms /     1 runs   (   37.44 ms per token,    26.71 tokens per second)\n",
            "llama_print_timings:       total time =    1727.39 ms /   332 tokens\n",
            " 86%|████████▌ | 30/35 [23:05<04:19, 51.87s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.24 ms /    10 runs   (    0.52 ms per token,  1910.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2231.73 ms /   434 tokens (    5.14 ms per token,   194.47 tokens per second)\n",
            "llama_print_timings:        eval time =     346.03 ms /     9 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2592.61 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1734.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3682.88 ms /   708 tokens (    5.20 ms per token,   192.24 tokens per second)\n",
            "llama_print_timings:        eval time =      38.67 ms /     1 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    3732.69 ms /   709 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      68.63 ms /    99 runs   (    0.69 ms per token,  1442.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3514.41 ms /   675 tokens (    5.21 ms per token,   192.07 tokens per second)\n",
            "llama_print_timings:        eval time =    3865.22 ms /    98 runs   (   39.44 ms per token,    25.35 tokens per second)\n",
            "llama_print_timings:       total time =    7519.87 ms /   773 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      45.84 ms /    79 runs   (    0.58 ms per token,  1723.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3999.37 ms /   760 tokens (    5.26 ms per token,   190.03 tokens per second)\n",
            "llama_print_timings:        eval time =    3084.91 ms /    78 runs   (   39.55 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    7186.10 ms /   838 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1760.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =     914.16 ms /   182 tokens (    5.02 ms per token,   199.09 tokens per second)\n",
            "llama_print_timings:        eval time =      38.09 ms /     1 runs   (   38.09 ms per token,    26.26 tokens per second)\n",
            "llama_print_timings:       total time =     956.69 ms /   183 tokens\n",
            " 89%|████████▊ | 31/35 [23:32<02:58, 44.61s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.38 ms /    10 runs   (    0.54 ms per token,  1859.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2236.01 ms /   434 tokens (    5.15 ms per token,   194.10 tokens per second)\n",
            "llama_print_timings:        eval time =     344.07 ms /     9 runs   (   38.23 ms per token,    26.16 tokens per second)\n",
            "llama_print_timings:       total time =    2594.49 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1748.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4653.03 ms /   878 tokens (    5.30 ms per token,   188.69 tokens per second)\n",
            "llama_print_timings:        eval time =      39.44 ms /     1 runs   (   39.44 ms per token,    25.35 tokens per second)\n",
            "llama_print_timings:       total time =    4705.12 ms /   879 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      88.42 ms /   126 runs   (    0.70 ms per token,  1425.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4467.55 ms /   845 tokens (    5.29 ms per token,   189.14 tokens per second)\n",
            "llama_print_timings:        eval time =    5021.57 ms /   125 runs   (   40.17 ms per token,    24.89 tokens per second)\n",
            "llama_print_timings:       total time =    9682.92 ms /   970 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      77.51 ms /   130 runs   (    0.60 ms per token,  1677.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5108.36 ms /   957 tokens (    5.34 ms per token,   187.34 tokens per second)\n",
            "llama_print_timings:        eval time =    5189.65 ms /   129 runs   (   40.23 ms per token,    24.86 tokens per second)\n",
            "llama_print_timings:       total time =   10466.07 ms /  1086 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.46 ms /     2 runs   (    0.73 ms per token,  1370.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1056.62 ms /   208 tokens (    5.08 ms per token,   196.85 tokens per second)\n",
            "llama_print_timings:        eval time =      75.82 ms /     2 runs   (   37.91 ms per token,    26.38 tokens per second)\n",
            "llama_print_timings:       total time =    1137.87 ms /   210 tokens\n",
            " 91%|█████████▏| 32/35 [24:05<02:03, 41.06s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.85 ms /    10 runs   (    0.69 ms per token,  1459.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2263.94 ms /   434 tokens (    5.22 ms per token,   191.70 tokens per second)\n",
            "llama_print_timings:        eval time =     349.59 ms /     9 runs   (   38.84 ms per token,    25.74 tokens per second)\n",
            "llama_print_timings:       total time =    2632.29 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.36 ms /     2 runs   (    0.68 ms per token,  1469.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6359.62 ms /  1171 tokens (    5.43 ms per token,   184.13 tokens per second)\n",
            "llama_print_timings:        eval time =      40.09 ms /     1 runs   (   40.09 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =    6416.84 ms /  1172 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      89.86 ms /   163 runs   (    0.55 ms per token,  1813.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6171.93 ms /  1138 tokens (    5.42 ms per token,   184.38 tokens per second)\n",
            "llama_print_timings:        eval time =    6602.78 ms /   162 runs   (   40.76 ms per token,    24.54 tokens per second)\n",
            "llama_print_timings:       total time =   12979.00 ms /  1300 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      59.62 ms /   107 runs   (    0.56 ms per token,  1794.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7078.82 ms /  1284 tokens (    5.51 ms per token,   181.39 tokens per second)\n",
            "llama_print_timings:        eval time =    4369.83 ms /   106 runs   (   41.22 ms per token,    24.26 tokens per second)\n",
            "llama_print_timings:       total time =   11584.82 ms /  1390 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1730.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1238.98 ms /   248 tokens (    5.00 ms per token,   200.16 tokens per second)\n",
            "llama_print_timings:        eval time =      75.45 ms /     2 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
            "llama_print_timings:       total time =    1319.55 ms /   250 tokens\n",
            " 94%|█████████▍| 33/35 [24:46<01:22, 41.00s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       6.74 ms /    10 runs   (    0.67 ms per token,  1483.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2255.23 ms /   434 tokens (    5.20 ms per token,   192.44 tokens per second)\n",
            "llama_print_timings:        eval time =     354.74 ms /     9 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    2629.66 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.30 ms /     2 runs   (    0.65 ms per token,  1543.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6993.78 ms /  1280 tokens (    5.46 ms per token,   183.02 tokens per second)\n",
            "llama_print_timings:        eval time =      41.09 ms /     1 runs   (   41.09 ms per token,    24.34 tokens per second)\n",
            "llama_print_timings:       total time =    7054.87 ms /  1281 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      82.34 ms /   128 runs   (    0.64 ms per token,  1554.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6788.46 ms /  1247 tokens (    5.44 ms per token,   183.69 tokens per second)\n",
            "llama_print_timings:        eval time =    5269.09 ms /   127 runs   (   41.49 ms per token,    24.10 tokens per second)\n",
            "llama_print_timings:       total time =   12254.07 ms /  1374 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      72.38 ms /   120 runs   (    0.60 ms per token,  1657.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7591.54 ms /  1363 tokens (    5.57 ms per token,   179.54 tokens per second)\n",
            "llama_print_timings:        eval time =    4976.49 ms /   119 runs   (   41.82 ms per token,    23.91 tokens per second)\n",
            "llama_print_timings:       total time =   12738.54 ms /  1482 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.40 ms /     2 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1050.39 ms /   208 tokens (    5.05 ms per token,   198.02 tokens per second)\n",
            "llama_print_timings:        eval time =      75.64 ms /     2 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
            "llama_print_timings:       total time =    1132.37 ms /   210 tokens\n",
            " 97%|█████████▋| 34/35 [25:32<00:42, 42.47s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       5.38 ms /    10 runs   (    0.54 ms per token,  1859.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2261.62 ms /   434 tokens (    5.21 ms per token,   191.90 tokens per second)\n",
            "llama_print_timings:        eval time =     345.33 ms /     9 runs   (   38.37 ms per token,    26.06 tokens per second)\n",
            "llama_print_timings:       total time =    2625.09 ms /   443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.38 ms /     2 runs   (    0.69 ms per token,  1454.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5796.37 ms /  1076 tokens (    5.39 ms per token,   185.63 tokens per second)\n",
            "llama_print_timings:        eval time =      40.24 ms /     1 runs   (   40.24 ms per token,    24.85 tokens per second)\n",
            "llama_print_timings:       total time =    5853.05 ms /  1077 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =      69.58 ms /   134 runs   (    0.52 ms per token,  1925.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5616.48 ms /  1043 tokens (    5.38 ms per token,   185.70 tokens per second)\n",
            "llama_print_timings:        eval time =    5370.38 ms /   133 runs   (   40.38 ms per token,    24.77 tokens per second)\n",
            "llama_print_timings:       total time =   11145.33 ms /  1176 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1885.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6320.59 ms /  1156 tokens (    5.47 ms per token,   182.89 tokens per second)\n",
            "llama_print_timings:        eval time =      40.03 ms /     1 runs   (   40.03 ms per token,    24.98 tokens per second)\n",
            "llama_print_timings:       total time =    6380.02 ms /  1157 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     498.88 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1749.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1114.62 ms /   224 tokens (    4.98 ms per token,   200.97 tokens per second)\n",
            "llama_print_timings:        eval time =      37.19 ms /     1 runs   (   37.19 ms per token,    26.89 tokens per second)\n",
            "llama_print_timings:       total time =    1157.05 ms /   225 tokens\n",
            "100%|██████████| 35/35 [26:05<00:00, 44.72s/it]\n"
          ]
        }
      ]
    }
  ]
}