### embeddings
1. General Text Embeddings (GTE) model **gte-large**. The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including GTE-large, GTE-base, and GTE-small. The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. The model achieves impressive results (even outperforming the current commercial embedding API of OpenAI) by effectively harnessing multi-stage contrastive learning, offering a text embedding with broad applicability across various NLP and code-related tasks including information retrieval, semantic textual similarity, text reranking, etc. [Towards General Text Embeddings with Multi-stage Contrastive Learning paper](https://arxiv.org/pdf/2308.03281.pdf) and [thenlper/gte-large model on HuggingFace](https://huggingface.co/thenlper/gte-large)
