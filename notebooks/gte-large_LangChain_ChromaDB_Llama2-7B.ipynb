{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMv3ZV+3yIo7Cgt3W3LZWo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winterForestStump/thesis/blob/main/notebooks/gte-large_LangChain_ChromaDB_Llama2-7B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1: Install All the Required Packages and Libraries**"
      ],
      "metadata": {
        "id": "C2t8sB89uYE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mD_xiot7spAE"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install huggingface_hub\n",
        "!pip install langchain\n",
        "!pip install sentence-transformers\n",
        "!pip install chromadb\n",
        "!pip install -q -U transformers peft accelerate optimum\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n",
        "!pip install GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "#from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.vectorstores import Chroma\n",
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import PromptTemplate\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from textwrap import fill\n",
        "from huggingface_hub import hf_hub_download\n",
        "import GPUtil\n",
        "import requests"
      ],
      "metadata": {
        "id": "TfRurammx9K6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1A: Recursively split by character**"
      ],
      "metadata": {
        "id": "8Is_2-PlnDXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter"
      ],
      "metadata": {
        "id": "9o23wjh9nNrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KODP-ytG3zXv",
        "outputId": "cd412f7a-8337-4b52-a51f-c156d2fc0788"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 18 11:30:36 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Initial GPU Usage:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKAkGsrX0eQy",
        "outputId": "db0f0386-af7b-4647-d148-ce0dc716a381"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial GPU Usage:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  0% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = requests.get('https://raw.githubusercontent.com/winterForestStump/thesis/main/data/txt_examp_ver2.txt').text"
      ],
      "metadata": {
        "id": "kuNUwE0FeeS_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=32,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False)\n",
        "\n",
        "texts = text_splitter.create_documents([text])\n",
        "\n",
        "print('GPU Usage after text splitting:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "id": "cTW2X5uXnOpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e9f0655-14d2-4222-b943-4be724738823"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after text splitting:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  0% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iob0lrsVo1As",
        "outputId": "a0a4a874-19bb-4238-8330-ffa5211f0ddd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1079"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1B: Embedding**"
      ],
      "metadata": {
        "id": "aSCMq8L2qkDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"thenlper/gte-large\",\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "#Limitation\n",
        "#This model exclusively caters to English texts, and any lengthy texts will be truncated to a maximum of 512 tokens.\n",
        "\n",
        "print('GPU Usage after embedding:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "id": "43EoE1n9qhq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d764510f-7f05-4803-a70e-be5793574c02"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after embedding:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% |  0% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma.from_documents(texts, embeddings, persist_directory=\"db\")\n",
        "results = db.similarity_search(\"What was the revenue of the company in 2023 and how it changed since 2021?\", k=8)\n",
        "print(results[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2ieg34jrOc1",
        "outputId": "2bf9cc5e-c205-4d7c-a220-9f651d54351e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In fiscal year 2023, we achieved earnings growth through pricing discipline in our merchant business as well as improved on-site volumes, including higher demand for hydrogen, despite inflation, higher maintenance activities, and higher costs to support our long-term strategy. Due to the structure of our contracts, which generally contain fixed monthly charges and/or minimum purchase requirements, our on-site business generates stable cash flow and consistently contributes about half our total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU Usage after chroma DB:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7OKcs-91Jh4",
        "outputId": "44b6c977-746f-4dd4-c324-2f0aaed215db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after chroma DB:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  2% | 20% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Download the Model**"
      ],
      "metadata": {
        "id": "GPSnZbp4yCfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
        "# Context Length: 4096\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map=\"auto\")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 1024\n",
        "generation_config.temperature = 0.0001\n",
        "generation_config.top_p = 0.95\n",
        "generation_config.do_sample = True\n",
        "generation_config.repetition_penalty = 1.15\n",
        "\n",
        "text_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1JEeVREu8GQ",
        "outputId": "3f703e8f-df23-40c3-e42f-32fb780cb358"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU Usage after LLM and tokenizer initialization:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t01eYNT1ORR",
        "outputId": "5cb74ed2-d28b-4e6b-f708-72d3cac0adfb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after LLM and tokenizer initialization:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% | 36% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a plot template"
      ],
      "metadata": {
        "id": "Ma4nQyB3IVUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Act like an expert in financial analysis and accounting. Use the following information from company annual reports and answer the question at the end.\n",
        "If you don't know the answer, don't make up an answer, but answer \"I don't know\".\n",
        "<</SYS>>\n",
        "\n",
        "{context}\n",
        "\n",
        "{question} [/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 8}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")"
      ],
      "metadata": {
        "id": "_XEjxzLgsRgf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU Usage after RetrievalQA:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7gqbOJR4ktR",
        "outputId": "1b7beb28-3510-4361-902b-fd51f3cf613c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after RetrievalQA:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  0% | 36% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\n",
        "    \"What was the revenue (sales) of the company in 2023 and how it changed since 2021?\"\n",
        ")\n",
        "print(fill(result[\"result\"].strip(), width=90))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE24Iu7Z4j3p",
        "outputId": "de9eb928-a509-4e2e-b99e-8ec2ed658303"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided information, here are the answers to your questions: Revenue (sales)\n",
            "of the company in 2023: $12.6 billion Change in revenue (sales) between 2021 and 2023: In\n",
            "2021, the company's sales were $970.8 million, which decreased by 8% to $889.0 million in\n",
            "2023.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU Usage after the retrieval 1st prompt:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3j2eOWp1U9x",
        "outputId": "e4efee2f-01d5-4210-825f-3c765709a185"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after the retrieval 1st prompt:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 70% | 44% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "other prompts:"
      ],
      "metadata": {
        "id": "TwHIMnJ8tskG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\n",
        "    \"Summarize the financial results (revenue, costs and net incomes) of the company in 2023 and possible risks which can influence the results in the future in 10-12 sentences.\"\n",
        ")\n",
        "print(fill(result[\"result\"].strip(), width=90))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZzwosyjtruE",
        "outputId": "20240bb0-574a-420d-98da-f4bd7f3df0d6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided financial statements, here are the key financial results of the\n",
            "company in 2023: Revenue: The company generated $5,192.9 million in net sales in 2023,\n",
            "representing a decrease of $1,031.5 million compared to 2022. Gross profit was $2,465.5\n",
            "million, down by $233.4 million from the previous year. Costs: Total operating expenses\n",
            "were $1,145.7 million in 2023, an increase of $133.9 million from 2022. This includes cost\n",
            "of goods sold of $827.9 million, selling, general, and administrative expenses of $264.8\n",
            "million, and research and development expenses of $15.0 million. Net Income: The company\n",
            "reported a net income of $1,027.7 million in 2023, a decline of $233.4 million from the\n",
            "previous year. Possible Risks: Some potential risks that could impact the company's\n",
            "financial performance in the future include economic downturns, increased competition, and\n",
            "supply chain disruptions. Additionally, the company's reliance on a few major customers\n",
            "and suppliers may pose a risk if any of these relationships were to be disrupted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU Usage after the retrieval 2st prompt:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odcaIST_1cQJ",
        "outputId": "480c9646-cc37-44c2-9a24-9950b8434755"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after the retrieval 2st prompt:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 91% | 51% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\n",
        "    \"What were the net incomes from continuing operations of the company in the period from 2021 to 2023?\"\n",
        ")\n",
        "print(fill(result[\"result\"].strip(), width=90))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmapFPe7ktq3",
        "outputId": "030fadbe-8a58-48b3-e00b-31deb8b966de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided in the annual report, the net income from continuing\n",
            "operations of the company in each of the periods from 2021 to 2023 is as follows: * For\n",
            "fiscal year 2021, the net income from continuing operations was $2,281.4 million. * For\n",
            "fiscal year 2022, the net income from continuing operations was $2,338.8 million. * For\n",
            "fiscal year 2023, the net income from continuing operations was $2,494.6 million.\n",
            "Therefore, the net income from continuing operations increased by $157.4 million from 2021\n",
            "to 2022 and further increased by $163.2 million from 2022 to 2023.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU Usage after the retrieval 3st prompt:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASZYO9zX1d0a",
        "outputId": "72f2197e-eb03-4290-f153-6f09bc8cb96c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after the retrieval 3st prompt:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 89% | 53% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\n",
        "    \"What were earnings per share (EPS) in 2022-2023 period?\"\n",
        ")\n",
        "print(fill(result[\"result\"].strip(), width=90))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2wq-eAMk4l_",
        "outputId": "066d8935-58ed-412b-bea7-5b5031771f9d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided data, the earnings per share (EPS) for the 2022-2023 period was\n",
            "$10.33.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('GPU Usage after the retrieval 4th prompt:')\n",
        "GPUtil.showUtilization()"
      ],
      "metadata": {
        "id": "xbErje2b3W3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705bffdf-20d2-460e-ba76-9ee9ca3600cc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Usage after the retrieval 4th prompt:\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 | 92% | 55% |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = qa_chain(\n",
        "    \"What was the difference between total current assets and total current liabilities in 2023?\"\n",
        ")\n",
        "print(fill(result[\"result\"].strip(), width=90))"
      ],
      "metadata": {
        "id": "iTiE3ce9lPR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74927bcd-0620-41a2-8ed0-896591fd8203"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information provided in the annual report, the difference between total\n",
            "current assets and total current liabilities in 2023 is: Total Current Assets in 2023 =\n",
            "$36.2 + $83.8 + $36.4 + $97.5 = $254.8 million Total Current Liabilities in 2023 = $5.3 +\n",
            "$0.6 + $5.8 + $0.4 = $6.7 million Therefore, the difference between total current assets\n",
            "and total current liabilities in 2023 is $248.1 million ($254.8 - $6.7).\n"
          ]
        }
      ]
    }
  ]
}