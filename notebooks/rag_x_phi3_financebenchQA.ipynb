{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "badd325abb48469db951a5c9156963a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbef1721e6a14a9fb44af2722c38ef59",
              "IPY_MODEL_88e50b97448744eba1d068a903cc62fd",
              "IPY_MODEL_4467c9ab8a494248a087f6c9848b132a"
            ],
            "layout": "IPY_MODEL_38f05816a9b141cabf2ba035332714cb"
          }
        },
        "bbef1721e6a14a9fb44af2722c38ef59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be97ed5dee974c2b89d6138257df253a",
            "placeholder": "​",
            "style": "IPY_MODEL_6d88f0f24d1643fc88d00f1dfc22651b",
            "value": "modules.json: 100%"
          }
        },
        "88e50b97448744eba1d068a903cc62fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef5efe46318497d8929e757d192e8ea",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_af899a97f0f345a4a38343d2c57ba4f1",
            "value": 349
          }
        },
        "4467c9ab8a494248a087f6c9848b132a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d97a53e63c5f4e73bbe2b3b214ee5ec6",
            "placeholder": "​",
            "style": "IPY_MODEL_e7e0b21951a04c6fb0b3097f409dfcb7",
            "value": " 349/349 [00:00&lt;00:00, 16.7kB/s]"
          }
        },
        "38f05816a9b141cabf2ba035332714cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be97ed5dee974c2b89d6138257df253a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d88f0f24d1643fc88d00f1dfc22651b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ef5efe46318497d8929e757d192e8ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af899a97f0f345a4a38343d2c57ba4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d97a53e63c5f4e73bbe2b3b214ee5ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7e0b21951a04c6fb0b3097f409dfcb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96c0b135740240e4a6c384e31795e38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4b9e5b9d0d6430e86516d8eb5a3cb9f",
              "IPY_MODEL_c5e519f0eb6c464d9f7baa38f938b3aa",
              "IPY_MODEL_b6d922c820d84c098d6fff0f3f45f9e4"
            ],
            "layout": "IPY_MODEL_e26b8706907a457bb8aa0d6a7e3c67fd"
          }
        },
        "a4b9e5b9d0d6430e86516d8eb5a3cb9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f4426aae8c1453981cccdaeda8e4002",
            "placeholder": "​",
            "style": "IPY_MODEL_7b4c3672154f440da0423f44f034d644",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "c5e519f0eb6c464d9f7baa38f938b3aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0ca2e056250423db23755574accd1ef",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_339b14138b9d4a5ca8493ff241e6e873",
            "value": 124
          }
        },
        "b6d922c820d84c098d6fff0f3f45f9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faf254ec40a348c8895608489d592596",
            "placeholder": "​",
            "style": "IPY_MODEL_1c56abdd0b0c45478fe999bc89fb8327",
            "value": " 124/124 [00:00&lt;00:00, 8.24kB/s]"
          }
        },
        "e26b8706907a457bb8aa0d6a7e3c67fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4426aae8c1453981cccdaeda8e4002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4c3672154f440da0423f44f034d644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0ca2e056250423db23755574accd1ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "339b14138b9d4a5ca8493ff241e6e873": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "faf254ec40a348c8895608489d592596": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c56abdd0b0c45478fe999bc89fb8327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59306698ced6480e8d6b106265ccb5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf8aba2cb00d49978c4e43fc2a404930",
              "IPY_MODEL_66ef8bb47b16493e93396adee8d17e03",
              "IPY_MODEL_f60127f2a5e547d3bd26be49fbe5a565"
            ],
            "layout": "IPY_MODEL_9c1dc1ddb53d4c5d9ca611755d8975e4"
          }
        },
        "bf8aba2cb00d49978c4e43fc2a404930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59afcd3b396b4251b383ea3e7b4cf581",
            "placeholder": "​",
            "style": "IPY_MODEL_a1348359cc8442bc90f7f420c13a1503",
            "value": "README.md: 100%"
          }
        },
        "66ef8bb47b16493e93396adee8d17e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23334e93d0c04808a21959e50bba89a1",
            "max": 94783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ce7ccf3b8ba4e4eaa8f81b75a48ade5",
            "value": 94783
          }
        },
        "f60127f2a5e547d3bd26be49fbe5a565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_202c18e4557a4da5b1c2be81c20166ca",
            "placeholder": "​",
            "style": "IPY_MODEL_5eeeb167e5e542e8abba484185a6076d",
            "value": " 94.8k/94.8k [00:00&lt;00:00, 7.01MB/s]"
          }
        },
        "9c1dc1ddb53d4c5d9ca611755d8975e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59afcd3b396b4251b383ea3e7b4cf581": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1348359cc8442bc90f7f420c13a1503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23334e93d0c04808a21959e50bba89a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ce7ccf3b8ba4e4eaa8f81b75a48ade5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "202c18e4557a4da5b1c2be81c20166ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eeeb167e5e542e8abba484185a6076d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6394d9b160eb4c88ae717c896c8ce01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c6e8be5f6394c42a9178fd6c050eb10",
              "IPY_MODEL_49b20ffd0d6e4fbdb16112dfc1a13de6",
              "IPY_MODEL_282af148ee454588a45883f7cf383d12"
            ],
            "layout": "IPY_MODEL_77e8f53869be490d9a29c3a85d56f10e"
          }
        },
        "5c6e8be5f6394c42a9178fd6c050eb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d094a905c774524b74f536ec96eec8e",
            "placeholder": "​",
            "style": "IPY_MODEL_b76f8041b40c4080a270ef4a78ae3324",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "49b20ffd0d6e4fbdb16112dfc1a13de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6933d2ce34ec42f59464eafb763d20b9",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d41936cc8a147219c6248fc3d69da01",
            "value": 52
          }
        },
        "282af148ee454588a45883f7cf383d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0944d372abf4dcdb4c5c01f28ee3334",
            "placeholder": "​",
            "style": "IPY_MODEL_53b2bb68217a419383a14b294b0b1f77",
            "value": " 52.0/52.0 [00:00&lt;00:00, 3.86kB/s]"
          }
        },
        "77e8f53869be490d9a29c3a85d56f10e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d094a905c774524b74f536ec96eec8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b76f8041b40c4080a270ef4a78ae3324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6933d2ce34ec42f59464eafb763d20b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d41936cc8a147219c6248fc3d69da01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0944d372abf4dcdb4c5c01f28ee3334": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53b2bb68217a419383a14b294b0b1f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a307e5f6c35b408497c7748ff30af863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a16db078202a4834b3cbe0b99dcf4bfe",
              "IPY_MODEL_35b626ed79904ea0bae7ec03c1632c1a",
              "IPY_MODEL_179dac83dd3a4b94a7a36c3305df0a75"
            ],
            "layout": "IPY_MODEL_5bcf7cdaf6a544ceb69b02db75eb8064"
          }
        },
        "a16db078202a4834b3cbe0b99dcf4bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_475ea60084db4b999025b3fabf90f245",
            "placeholder": "​",
            "style": "IPY_MODEL_f2f5b0f34a66442d828540fe895eba98",
            "value": "config.json: 100%"
          }
        },
        "35b626ed79904ea0bae7ec03c1632c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e239b51e597408d92f1f5865b87df43",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14a3af55ff594da2bf1d2ceca9a192df",
            "value": 743
          }
        },
        "179dac83dd3a4b94a7a36c3305df0a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f5bc2f0e5a94880a3d690ced85d6736",
            "placeholder": "​",
            "style": "IPY_MODEL_24284f1a602a4005badbfcfc47af1945",
            "value": " 743/743 [00:00&lt;00:00, 56.3kB/s]"
          }
        },
        "5bcf7cdaf6a544ceb69b02db75eb8064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "475ea60084db4b999025b3fabf90f245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2f5b0f34a66442d828540fe895eba98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e239b51e597408d92f1f5865b87df43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14a3af55ff594da2bf1d2ceca9a192df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f5bc2f0e5a94880a3d690ced85d6736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24284f1a602a4005badbfcfc47af1945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd4a5b0130464cf1ba6eeed5f86cbca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0e2ca3d7de5e4b31a139e0da0dea1c80",
              "IPY_MODEL_a0a5c9a5299647f2b51ca995831f3bd0",
              "IPY_MODEL_735aacd5a7a444fb8ab168953b5ae087"
            ],
            "layout": "IPY_MODEL_b3aecc3893c84a5bb5e2c06a357faaa7"
          }
        },
        "0e2ca3d7de5e4b31a139e0da0dea1c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22e193b4e07a462fb6d03298997bbdfd",
            "placeholder": "​",
            "style": "IPY_MODEL_fafc5dad28804b90afb5a5d4e33fc215",
            "value": "model.safetensors: 100%"
          }
        },
        "a0a5c9a5299647f2b51ca995831f3bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b9aa76def746ada700f5c6886e8725",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_372e8636502d4cc7b6c762a8e505b975",
            "value": 133466304
          }
        },
        "735aacd5a7a444fb8ab168953b5ae087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_966bac9d7e8147b1ae04cb51406fd30d",
            "placeholder": "​",
            "style": "IPY_MODEL_c07ae71e07d5451fbe9c2e98e514db24",
            "value": " 133M/133M [00:00&lt;00:00, 323MB/s]"
          }
        },
        "b3aecc3893c84a5bb5e2c06a357faaa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e193b4e07a462fb6d03298997bbdfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fafc5dad28804b90afb5a5d4e33fc215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16b9aa76def746ada700f5c6886e8725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "372e8636502d4cc7b6c762a8e505b975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "966bac9d7e8147b1ae04cb51406fd30d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07ae71e07d5451fbe9c2e98e514db24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a292e96c6344f05b011c679eafd77ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f71b0a6edf44904852d3df557fc750d",
              "IPY_MODEL_c82d64bbb684404b8e67b4f450f3c1d0",
              "IPY_MODEL_e373da4e6d6f4344be773c56899dcc50"
            ],
            "layout": "IPY_MODEL_1442bafbeecd412fa0fa38fc63698fa9"
          }
        },
        "2f71b0a6edf44904852d3df557fc750d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99b1acf098fa4c299b6ae2f776550d97",
            "placeholder": "​",
            "style": "IPY_MODEL_00034e26837340be954ed23f1de98eb5",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c82d64bbb684404b8e67b4f450f3c1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_083c96cecd4b4bbf905d95d8e8b0177e",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_67a1a2f306ee4c049899caffd2914fca",
            "value": 366
          }
        },
        "e373da4e6d6f4344be773c56899dcc50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83868a9d154d444ba27d77db70d46ca9",
            "placeholder": "​",
            "style": "IPY_MODEL_833bece20a7344c9877fbb14b9000136",
            "value": " 366/366 [00:00&lt;00:00, 29.7kB/s]"
          }
        },
        "1442bafbeecd412fa0fa38fc63698fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b1acf098fa4c299b6ae2f776550d97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00034e26837340be954ed23f1de98eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "083c96cecd4b4bbf905d95d8e8b0177e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67a1a2f306ee4c049899caffd2914fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "83868a9d154d444ba27d77db70d46ca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "833bece20a7344c9877fbb14b9000136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1b560454237435a875ff52d7c4eb3a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_673edc8b09f842cab13489ecbe5e4847",
              "IPY_MODEL_a9a3804a9d4a43449da840f5ace502a6",
              "IPY_MODEL_279fca92aff84b7492cb876212c9dfb1"
            ],
            "layout": "IPY_MODEL_fa1a95d0def74f199943d8aa61f0e658"
          }
        },
        "673edc8b09f842cab13489ecbe5e4847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03a97d96ebe84c7e9a25e18e49a3127c",
            "placeholder": "​",
            "style": "IPY_MODEL_27efcfcde19d4870ab8b9947c4f21642",
            "value": "vocab.txt: 100%"
          }
        },
        "a9a3804a9d4a43449da840f5ace502a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac11fcd64cc24f7a9b0afd89bb0b1271",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1465372958b744848e87d2c17ddab5c2",
            "value": 231508
          }
        },
        "279fca92aff84b7492cb876212c9dfb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88d2bf7ff18e401691b365e746cef3d4",
            "placeholder": "​",
            "style": "IPY_MODEL_6c6afd74bccd4661b4708af273ff0460",
            "value": " 232k/232k [00:00&lt;00:00, 13.6MB/s]"
          }
        },
        "fa1a95d0def74f199943d8aa61f0e658": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a97d96ebe84c7e9a25e18e49a3127c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27efcfcde19d4870ab8b9947c4f21642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac11fcd64cc24f7a9b0afd89bb0b1271": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1465372958b744848e87d2c17ddab5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88d2bf7ff18e401691b365e746cef3d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c6afd74bccd4661b4708af273ff0460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7765311816a94fe3b926f2c77512d868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c349f138eefa4814aa1f9c1e8d765a45",
              "IPY_MODEL_77fb073b3c904d9fb6438af2eab8e726",
              "IPY_MODEL_1c8b50d2d59d46768ed5429ffd7ccec9"
            ],
            "layout": "IPY_MODEL_2bbc510eddbf4852ab855e4c4a84fa03"
          }
        },
        "c349f138eefa4814aa1f9c1e8d765a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0e0bdc33b014592b7819a5770170b4d",
            "placeholder": "​",
            "style": "IPY_MODEL_6907b66953634dcb9260046bee00ff09",
            "value": "tokenizer.json: 100%"
          }
        },
        "77fb073b3c904d9fb6438af2eab8e726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d7901b8038745c8a13149d66c41a193",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a485a0e1341347cc9bed71f9103c578e",
            "value": 711396
          }
        },
        "1c8b50d2d59d46768ed5429ffd7ccec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a5943ee9a504177a95849948e4beaed",
            "placeholder": "​",
            "style": "IPY_MODEL_b06543a4b00d4fa1b6771999edd16bcd",
            "value": " 711k/711k [00:00&lt;00:00, 29.0MB/s]"
          }
        },
        "2bbc510eddbf4852ab855e4c4a84fa03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0e0bdc33b014592b7819a5770170b4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6907b66953634dcb9260046bee00ff09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d7901b8038745c8a13149d66c41a193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a485a0e1341347cc9bed71f9103c578e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a5943ee9a504177a95849948e4beaed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b06543a4b00d4fa1b6771999edd16bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72006a0bdcf24fa3be7d5aef98cb794b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_680cd0b22deb4729a8abd5886e48f735",
              "IPY_MODEL_e40cc643a2cc4f4fa43c5796b51cfa66",
              "IPY_MODEL_73c68be393bb4f70b31c0b484d47ead3"
            ],
            "layout": "IPY_MODEL_34b47af9756e4a6cb07faf5634900ebb"
          }
        },
        "680cd0b22deb4729a8abd5886e48f735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_409e92fd2a704e7ca4a15a84fa18e789",
            "placeholder": "​",
            "style": "IPY_MODEL_746ece2c556e4f02ab62a3b06c8aa7ee",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "e40cc643a2cc4f4fa43c5796b51cfa66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c12d061015d4ec09131c911471825bb",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81ef200e024d4b4bac7e0feb1aa789ac",
            "value": 125
          }
        },
        "73c68be393bb4f70b31c0b484d47ead3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f6e041ed19c4b13bf55745e8f10ec6d",
            "placeholder": "​",
            "style": "IPY_MODEL_660238dbf6cd4f1b9d00a11ecb447726",
            "value": " 125/125 [00:00&lt;00:00, 8.75kB/s]"
          }
        },
        "34b47af9756e4a6cb07faf5634900ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "409e92fd2a704e7ca4a15a84fa18e789": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "746ece2c556e4f02ab62a3b06c8aa7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c12d061015d4ec09131c911471825bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81ef200e024d4b4bac7e0feb1aa789ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f6e041ed19c4b13bf55745e8f10ec6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "660238dbf6cd4f1b9d00a11ecb447726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e9ed18f25f44e2081421274a9b78062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ba753837f16f4f1f8355706541656172",
              "IPY_MODEL_514e86a4edb94c94a2745c6f4a8de2ea",
              "IPY_MODEL_41ba9ceb2afc4ee1b5a9772b28162b18"
            ],
            "layout": "IPY_MODEL_b778bbcf29e94f0295db94af1f57fb67"
          }
        },
        "ba753837f16f4f1f8355706541656172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20364f66073d4ca1954ebca53f064273",
            "placeholder": "​",
            "style": "IPY_MODEL_66f8044b8a51402497a019185e1e6272",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "514e86a4edb94c94a2745c6f4a8de2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6bb28bb5eaa497c9f61ded2146aea47",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c85ec246970b48a7b8e1032015cdde64",
            "value": 190
          }
        },
        "41ba9ceb2afc4ee1b5a9772b28162b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_621e58ed1ac04f5f9b7bf084f3e55740",
            "placeholder": "​",
            "style": "IPY_MODEL_42bb696e9691438c9d0be72e827e1e40",
            "value": " 190/190 [00:00&lt;00:00, 12.6kB/s]"
          }
        },
        "b778bbcf29e94f0295db94af1f57fb67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20364f66073d4ca1954ebca53f064273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66f8044b8a51402497a019185e1e6272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6bb28bb5eaa497c9f61ded2146aea47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c85ec246970b48a7b8e1032015cdde64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "621e58ed1ac04f5f9b7bf084f3e55740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42bb696e9691438c9d0be72e827e1e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa9bdeaae7144c47a0e5072231fb48a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_202546a79e3f4286b52c0e65c862fb7a",
              "IPY_MODEL_82fef51cdbb84dd683fd0d63af7db3c3",
              "IPY_MODEL_13fa45e1d9d5430e9f312ad8f629071e"
            ],
            "layout": "IPY_MODEL_ea0ea6ac675c479c8a73aa33cc955a7e"
          }
        },
        "202546a79e3f4286b52c0e65c862fb7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db45dfaeb5ab43c588eb287ee70ad39b",
            "placeholder": "​",
            "style": "IPY_MODEL_7efd0ef3d4ba483dbed1c5c95391c0a3",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "82fef51cdbb84dd683fd0d63af7db3c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80fd104a230347b2813061f6aee6329b",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca9b772d751c4e5aa8d12deefd4ab2c6",
            "value": 443
          }
        },
        "13fa45e1d9d5430e9f312ad8f629071e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f124e795f574f0eba77dba2874fb306",
            "placeholder": "​",
            "style": "IPY_MODEL_160e296fc2e54e5da104eea958998040",
            "value": " 443/443 [00:00&lt;00:00, 32.2kB/s]"
          }
        },
        "ea0ea6ac675c479c8a73aa33cc955a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db45dfaeb5ab43c588eb287ee70ad39b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7efd0ef3d4ba483dbed1c5c95391c0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80fd104a230347b2813061f6aee6329b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca9b772d751c4e5aa8d12deefd4ab2c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f124e795f574f0eba77dba2874fb306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "160e296fc2e54e5da104eea958998040": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0aafd7d59b614232878a10eac0d9b179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95a00eebb7b1476c8d15777c61ad8c3d",
              "IPY_MODEL_57f248fa71394b9e8d41ae2aa077717d",
              "IPY_MODEL_4fc8116a600c4169bf4c5bb84cc631f5"
            ],
            "layout": "IPY_MODEL_f600bbeb12684201a2c7f42deb8537b1"
          }
        },
        "95a00eebb7b1476c8d15777c61ad8c3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f10752b2fa0545d4a9172f0d5808d00a",
            "placeholder": "​",
            "style": "IPY_MODEL_dafd66f5823345d78b6b293b3731ffda",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "57f248fa71394b9e8d41ae2aa077717d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90b90c3e752d4e09a489340b1dff5920",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7ad14fab5534bfd974f11cd8c0f86d1",
            "value": 5069051
          }
        },
        "4fc8116a600c4169bf4c5bb84cc631f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00371cb670ea420fa6fa63c1ce58a855",
            "placeholder": "​",
            "style": "IPY_MODEL_875aa41ff7e74ed59abc8b7eb1e9aa20",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 123MB/s]"
          }
        },
        "f600bbeb12684201a2c7f42deb8537b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f10752b2fa0545d4a9172f0d5808d00a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dafd66f5823345d78b6b293b3731ffda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90b90c3e752d4e09a489340b1dff5920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7ad14fab5534bfd974f11cd8c0f86d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00371cb670ea420fa6fa63c1ce58a855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "875aa41ff7e74ed59abc8b7eb1e9aa20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed2a09d8404494d90b395a252754279": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7194fbf9c28d48b1852519fe35ea8e0d",
              "IPY_MODEL_3e044502d08a463a972481838b3efe3d",
              "IPY_MODEL_3192f3ac085542779903f214abadbe2d"
            ],
            "layout": "IPY_MODEL_ec0c386f538d4b1a99308ab1e062fb58"
          }
        },
        "7194fbf9c28d48b1852519fe35ea8e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3aa5fe29ef104bef9de9df746d570e69",
            "placeholder": "​",
            "style": "IPY_MODEL_92155311616e49b8844cc8e0f83da1e2",
            "value": "tokenizer.json: 100%"
          }
        },
        "3e044502d08a463a972481838b3efe3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928000dc1ea74f008d5b2780ba3f50c5",
            "max": 17098107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_980d8bc2216347e09b5dbab26ec3a997",
            "value": 17098107
          }
        },
        "3192f3ac085542779903f214abadbe2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d64257d5ab484d7b82d2ac44dd87c932",
            "placeholder": "​",
            "style": "IPY_MODEL_fbaaca820d4f41d599b906401a82270d",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 241MB/s]"
          }
        },
        "ec0c386f538d4b1a99308ab1e062fb58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3aa5fe29ef104bef9de9df746d570e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92155311616e49b8844cc8e0f83da1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "928000dc1ea74f008d5b2780ba3f50c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "980d8bc2216347e09b5dbab26ec3a997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d64257d5ab484d7b82d2ac44dd87c932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbaaca820d4f41d599b906401a82270d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60cef83b2d3f4375ada6ce3a351e0aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b724c2625ee845ea9616c1274b844fac",
              "IPY_MODEL_f23935e366fd4b0d9337db693eaa9469",
              "IPY_MODEL_8de7c0a01412415f848c551db76a1d53"
            ],
            "layout": "IPY_MODEL_0eb1b0c6b99f4132b68068e10c906db4"
          }
        },
        "b724c2625ee845ea9616c1274b844fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46680ca6c565491ea303c5f4f4e6cb21",
            "placeholder": "​",
            "style": "IPY_MODEL_4c5c1aa0b29947c5887880cb0f050d8b",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f23935e366fd4b0d9337db693eaa9469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5699c9aae1043df91f97617a134538a",
            "max": 279,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d444472a6842400abe4e00c13c88492b",
            "value": 279
          }
        },
        "8de7c0a01412415f848c551db76a1d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28b9d1ab5bb54ea48124cee3d1bb4cca",
            "placeholder": "​",
            "style": "IPY_MODEL_5449b41d79a549d793ac7cfd265aa047",
            "value": " 279/279 [00:00&lt;00:00, 12.7kB/s]"
          }
        },
        "0eb1b0c6b99f4132b68068e10c906db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46680ca6c565491ea303c5f4f4e6cb21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5c1aa0b29947c5887880cb0f050d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5699c9aae1043df91f97617a134538a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d444472a6842400abe4e00c13c88492b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28b9d1ab5bb54ea48124cee3d1bb4cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5449b41d79a549d793ac7cfd265aa047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2411247bd3746fab11235114075e9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b2511f4ad6842378977bafcb34b231a",
              "IPY_MODEL_19a04ac3be174ca3aa0308e098b43a4c",
              "IPY_MODEL_d85b5eaacae94498a1cea1874fd7a142"
            ],
            "layout": "IPY_MODEL_2c6c0748684b40b5baeb560c9aaa1435"
          }
        },
        "5b2511f4ad6842378977bafcb34b231a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b020538af26740ec8786a8e2562914cd",
            "placeholder": "​",
            "style": "IPY_MODEL_656c6e431612408993bab2fe08c7aa4a",
            "value": "config.json: 100%"
          }
        },
        "19a04ac3be174ca3aa0308e098b43a4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eea5c67ab73e4aed80a80c6787fb7bdb",
            "max": 801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f114bd60657147d590f11c3a2f6ab6f0",
            "value": 801
          }
        },
        "d85b5eaacae94498a1cea1874fd7a142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f25e1b168705470a8f722c017b7e9dda",
            "placeholder": "​",
            "style": "IPY_MODEL_f1352baad2534a469c0b6212f0f73096",
            "value": " 801/801 [00:00&lt;00:00, 32.2kB/s]"
          }
        },
        "2c6c0748684b40b5baeb560c9aaa1435": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b020538af26740ec8786a8e2562914cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "656c6e431612408993bab2fe08c7aa4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eea5c67ab73e4aed80a80c6787fb7bdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f114bd60657147d590f11c3a2f6ab6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f25e1b168705470a8f722c017b7e9dda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1352baad2534a469c0b6212f0f73096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27e0f91ad86a4d379480a0800712dbe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27222b8ba60349c4af3c3ec747a0c2ae",
              "IPY_MODEL_fb8a55c42d3641c1b7bd73207da34fa6",
              "IPY_MODEL_3e19c9eec10c46bab7f0ddb8943a8f89"
            ],
            "layout": "IPY_MODEL_8edda7f4b86547d9b4a598a767a2fc0d"
          }
        },
        "27222b8ba60349c4af3c3ec747a0c2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b92f20e773407ab54c500e2d08ea7e",
            "placeholder": "​",
            "style": "IPY_MODEL_c37fbe0727404cad99f817e52b736b02",
            "value": "model.safetensors: 100%"
          }
        },
        "fb8a55c42d3641c1b7bd73207da34fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_098c1ea984484af9a9c4b9b48eb7a1e3",
            "max": 2239618772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09af792c239d4e55afaadfe3d7a4aa36",
            "value": 2239618772
          }
        },
        "3e19c9eec10c46bab7f0ddb8943a8f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d412033963a14e249ffffc88721357d4",
            "placeholder": "​",
            "style": "IPY_MODEL_82cbfe20ec4e43618c0c9fe23d89fc82",
            "value": " 2.24G/2.24G [00:10&lt;00:00, 243MB/s]"
          }
        },
        "8edda7f4b86547d9b4a598a767a2fc0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4b92f20e773407ab54c500e2d08ea7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c37fbe0727404cad99f817e52b736b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "098c1ea984484af9a9c4b9b48eb7a1e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09af792c239d4e55afaadfe3d7a4aa36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d412033963a14e249ffffc88721357d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82cbfe20ec4e43618c0c9fe23d89fc82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winterForestStump/thesis/blob/main/notebooks/rag_x_phi3_financebenchQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-nomic langchain langchain-core langchain-community chromadb --quiet\n",
        "%pip install sentence_transformers FlagEmbedding --quiet"
      ],
      "metadata": {
        "id": "FvVmzL2j9VE2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LlamaCpp x GPU usage\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUlkK-AQ9VE_",
        "outputId": "30a50c28-8865-41b9-bc8f-65a775505299"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.78.tar.gz (50.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp310-cp310-linux_x86_64.whl size=169130738 sha256=0383155206e400bb73b9cf8a0d3ccbb216551fc68e7fed3eb1d1ae679991ec1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/c5/bd/3b1c20081bd71ce9d28b562573c97915c790bf1ef231879a61\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IfHIpuz9VFB",
        "outputId": "41d2214f-3850-4574-ad4f-61dd95763b10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "import chromadb\n",
        "from langchain.storage.file_system import LocalFileStore\n",
        "from langchain.storage._lc_store import create_kv_docstore\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "4chIcfH79VFC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-fp16.gguf --local-dir ./models --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFSdx4MY9VFD",
        "outputId": "5e21ced1-1f38-4fb6-84b0-df748f855934"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Downloading 'Phi-3-mini-4k-instruct-fp16.gguf' to 'models/.huggingface/download/Phi-3-mini-4k-instruct-fp16.gguf.5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3.incomplete'\n",
            "Phi-3-mini-4k-instruct-fp16.gguf: 100% 7.64G/7.64G [05:17<00:00, 24.1MB/s]\n",
            "Download complete. Moving file to models/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "models/Phi-3-mini-4k-instruct-fp16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEMP = 0\n",
        "N_CTX = 4096\n",
        "N_GPU_L = -1\n",
        "\n",
        "llm_phi3 = LlamaCpp(\n",
        "    model_path=\"/content/models/Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    temperature=TEMP,\n",
        "    n_ctx=N_CTX,\n",
        "    n_gpu_layers = N_GPU_L,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UWQXosB9VFE",
        "outputId": "00e18e98-d8a8-4641-845d-bb20a3cebef7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /content/models/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  130 tensors\n",
            "llm_load_vocab: special tokens cache size = 323\n",
            "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = phi3\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = Phi3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   187.88 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  7100.64 MiB\n",
            "....................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1536.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =    18.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.88 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = pd.read_json('https://raw.githubusercontent.com/patronus-ai/financebench/main/data/financebench_open_source.jsonl', lines=True)\n",
        "questions.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA87mjiJ9VFE",
        "outputId": "2a97eb10-16fa-4de4-aef3-90796e10f2b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 11 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   financebench_id       150 non-null    object\n",
            " 1   company               150 non-null    object\n",
            " 2   doc_name              150 non-null    object\n",
            " 3   question_type         150 non-null    object\n",
            " 4   question_reasoning    100 non-null    object\n",
            " 5   domain_question_num   50 non-null     object\n",
            " 6   question              150 non-null    object\n",
            " 7   answer                150 non-null    object\n",
            " 8   justification         100 non-null    object\n",
            " 9   dataset_subset_label  150 non-null    object\n",
            " 10  evidence              150 non-null    object\n",
            "dtypes: object(11)\n",
            "memory usage: 13.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'}, #gpu\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "badd325abb48469db951a5c9156963a5",
            "bbef1721e6a14a9fb44af2722c38ef59",
            "88e50b97448744eba1d068a903cc62fd",
            "4467c9ab8a494248a087f6c9848b132a",
            "38f05816a9b141cabf2ba035332714cb",
            "be97ed5dee974c2b89d6138257df253a",
            "6d88f0f24d1643fc88d00f1dfc22651b",
            "8ef5efe46318497d8929e757d192e8ea",
            "af899a97f0f345a4a38343d2c57ba4f1",
            "d97a53e63c5f4e73bbe2b3b214ee5ec6",
            "e7e0b21951a04c6fb0b3097f409dfcb7",
            "96c0b135740240e4a6c384e31795e38e",
            "a4b9e5b9d0d6430e86516d8eb5a3cb9f",
            "c5e519f0eb6c464d9f7baa38f938b3aa",
            "b6d922c820d84c098d6fff0f3f45f9e4",
            "e26b8706907a457bb8aa0d6a7e3c67fd",
            "4f4426aae8c1453981cccdaeda8e4002",
            "7b4c3672154f440da0423f44f034d644",
            "d0ca2e056250423db23755574accd1ef",
            "339b14138b9d4a5ca8493ff241e6e873",
            "faf254ec40a348c8895608489d592596",
            "1c56abdd0b0c45478fe999bc89fb8327",
            "59306698ced6480e8d6b106265ccb5c3",
            "bf8aba2cb00d49978c4e43fc2a404930",
            "66ef8bb47b16493e93396adee8d17e03",
            "f60127f2a5e547d3bd26be49fbe5a565",
            "9c1dc1ddb53d4c5d9ca611755d8975e4",
            "59afcd3b396b4251b383ea3e7b4cf581",
            "a1348359cc8442bc90f7f420c13a1503",
            "23334e93d0c04808a21959e50bba89a1",
            "5ce7ccf3b8ba4e4eaa8f81b75a48ade5",
            "202c18e4557a4da5b1c2be81c20166ca",
            "5eeeb167e5e542e8abba484185a6076d",
            "6394d9b160eb4c88ae717c896c8ce01c",
            "5c6e8be5f6394c42a9178fd6c050eb10",
            "49b20ffd0d6e4fbdb16112dfc1a13de6",
            "282af148ee454588a45883f7cf383d12",
            "77e8f53869be490d9a29c3a85d56f10e",
            "0d094a905c774524b74f536ec96eec8e",
            "b76f8041b40c4080a270ef4a78ae3324",
            "6933d2ce34ec42f59464eafb763d20b9",
            "6d41936cc8a147219c6248fc3d69da01",
            "e0944d372abf4dcdb4c5c01f28ee3334",
            "53b2bb68217a419383a14b294b0b1f77",
            "a307e5f6c35b408497c7748ff30af863",
            "a16db078202a4834b3cbe0b99dcf4bfe",
            "35b626ed79904ea0bae7ec03c1632c1a",
            "179dac83dd3a4b94a7a36c3305df0a75",
            "5bcf7cdaf6a544ceb69b02db75eb8064",
            "475ea60084db4b999025b3fabf90f245",
            "f2f5b0f34a66442d828540fe895eba98",
            "0e239b51e597408d92f1f5865b87df43",
            "14a3af55ff594da2bf1d2ceca9a192df",
            "2f5bc2f0e5a94880a3d690ced85d6736",
            "24284f1a602a4005badbfcfc47af1945",
            "fd4a5b0130464cf1ba6eeed5f86cbca9",
            "0e2ca3d7de5e4b31a139e0da0dea1c80",
            "a0a5c9a5299647f2b51ca995831f3bd0",
            "735aacd5a7a444fb8ab168953b5ae087",
            "b3aecc3893c84a5bb5e2c06a357faaa7",
            "22e193b4e07a462fb6d03298997bbdfd",
            "fafc5dad28804b90afb5a5d4e33fc215",
            "16b9aa76def746ada700f5c6886e8725",
            "372e8636502d4cc7b6c762a8e505b975",
            "966bac9d7e8147b1ae04cb51406fd30d",
            "c07ae71e07d5451fbe9c2e98e514db24",
            "0a292e96c6344f05b011c679eafd77ab",
            "2f71b0a6edf44904852d3df557fc750d",
            "c82d64bbb684404b8e67b4f450f3c1d0",
            "e373da4e6d6f4344be773c56899dcc50",
            "1442bafbeecd412fa0fa38fc63698fa9",
            "99b1acf098fa4c299b6ae2f776550d97",
            "00034e26837340be954ed23f1de98eb5",
            "083c96cecd4b4bbf905d95d8e8b0177e",
            "67a1a2f306ee4c049899caffd2914fca",
            "83868a9d154d444ba27d77db70d46ca9",
            "833bece20a7344c9877fbb14b9000136",
            "a1b560454237435a875ff52d7c4eb3a8",
            "673edc8b09f842cab13489ecbe5e4847",
            "a9a3804a9d4a43449da840f5ace502a6",
            "279fca92aff84b7492cb876212c9dfb1",
            "fa1a95d0def74f199943d8aa61f0e658",
            "03a97d96ebe84c7e9a25e18e49a3127c",
            "27efcfcde19d4870ab8b9947c4f21642",
            "ac11fcd64cc24f7a9b0afd89bb0b1271",
            "1465372958b744848e87d2c17ddab5c2",
            "88d2bf7ff18e401691b365e746cef3d4",
            "6c6afd74bccd4661b4708af273ff0460",
            "7765311816a94fe3b926f2c77512d868",
            "c349f138eefa4814aa1f9c1e8d765a45",
            "77fb073b3c904d9fb6438af2eab8e726",
            "1c8b50d2d59d46768ed5429ffd7ccec9",
            "2bbc510eddbf4852ab855e4c4a84fa03",
            "b0e0bdc33b014592b7819a5770170b4d",
            "6907b66953634dcb9260046bee00ff09",
            "4d7901b8038745c8a13149d66c41a193",
            "a485a0e1341347cc9bed71f9103c578e",
            "5a5943ee9a504177a95849948e4beaed",
            "b06543a4b00d4fa1b6771999edd16bcd",
            "72006a0bdcf24fa3be7d5aef98cb794b",
            "680cd0b22deb4729a8abd5886e48f735",
            "e40cc643a2cc4f4fa43c5796b51cfa66",
            "73c68be393bb4f70b31c0b484d47ead3",
            "34b47af9756e4a6cb07faf5634900ebb",
            "409e92fd2a704e7ca4a15a84fa18e789",
            "746ece2c556e4f02ab62a3b06c8aa7ee",
            "7c12d061015d4ec09131c911471825bb",
            "81ef200e024d4b4bac7e0feb1aa789ac",
            "6f6e041ed19c4b13bf55745e8f10ec6d",
            "660238dbf6cd4f1b9d00a11ecb447726",
            "8e9ed18f25f44e2081421274a9b78062",
            "ba753837f16f4f1f8355706541656172",
            "514e86a4edb94c94a2745c6f4a8de2ea",
            "41ba9ceb2afc4ee1b5a9772b28162b18",
            "b778bbcf29e94f0295db94af1f57fb67",
            "20364f66073d4ca1954ebca53f064273",
            "66f8044b8a51402497a019185e1e6272",
            "f6bb28bb5eaa497c9f61ded2146aea47",
            "c85ec246970b48a7b8e1032015cdde64",
            "621e58ed1ac04f5f9b7bf084f3e55740",
            "42bb696e9691438c9d0be72e827e1e40"
          ]
        },
        "id": "QCpr-Zyg9VFG",
        "outputId": "f0a9d8f1-37c2-4971-d6c5-3128cdeebd53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "badd325abb48469db951a5c9156963a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96c0b135740240e4a6c384e31795e38e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59306698ced6480e8d6b106265ccb5c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6394d9b160eb4c88ae717c896c8ce01c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a307e5f6c35b408497c7748ff30af863"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd4a5b0130464cf1ba6eeed5f86cbca9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a292e96c6344f05b011c679eafd77ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1b560454237435a875ff52d7c4eb3a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7765311816a94fe3b926f2c77512d868"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72006a0bdcf24fa3be7d5aef98cb794b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e9ed18f25f44e2081421274a9b78062"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "fa9bdeaae7144c47a0e5072231fb48a0",
            "202546a79e3f4286b52c0e65c862fb7a",
            "82fef51cdbb84dd683fd0d63af7db3c3",
            "13fa45e1d9d5430e9f312ad8f629071e",
            "ea0ea6ac675c479c8a73aa33cc955a7e",
            "db45dfaeb5ab43c588eb287ee70ad39b",
            "7efd0ef3d4ba483dbed1c5c95391c0a3",
            "80fd104a230347b2813061f6aee6329b",
            "ca9b772d751c4e5aa8d12deefd4ab2c6",
            "0f124e795f574f0eba77dba2874fb306",
            "160e296fc2e54e5da104eea958998040",
            "0aafd7d59b614232878a10eac0d9b179",
            "95a00eebb7b1476c8d15777c61ad8c3d",
            "57f248fa71394b9e8d41ae2aa077717d",
            "4fc8116a600c4169bf4c5bb84cc631f5",
            "f600bbeb12684201a2c7f42deb8537b1",
            "f10752b2fa0545d4a9172f0d5808d00a",
            "dafd66f5823345d78b6b293b3731ffda",
            "90b90c3e752d4e09a489340b1dff5920",
            "a7ad14fab5534bfd974f11cd8c0f86d1",
            "00371cb670ea420fa6fa63c1ce58a855",
            "875aa41ff7e74ed59abc8b7eb1e9aa20",
            "bed2a09d8404494d90b395a252754279",
            "7194fbf9c28d48b1852519fe35ea8e0d",
            "3e044502d08a463a972481838b3efe3d",
            "3192f3ac085542779903f214abadbe2d",
            "ec0c386f538d4b1a99308ab1e062fb58",
            "3aa5fe29ef104bef9de9df746d570e69",
            "92155311616e49b8844cc8e0f83da1e2",
            "928000dc1ea74f008d5b2780ba3f50c5",
            "980d8bc2216347e09b5dbab26ec3a997",
            "d64257d5ab484d7b82d2ac44dd87c932",
            "fbaaca820d4f41d599b906401a82270d",
            "60cef83b2d3f4375ada6ce3a351e0aa6",
            "b724c2625ee845ea9616c1274b844fac",
            "f23935e366fd4b0d9337db693eaa9469",
            "8de7c0a01412415f848c551db76a1d53",
            "0eb1b0c6b99f4132b68068e10c906db4",
            "46680ca6c565491ea303c5f4f4e6cb21",
            "4c5c1aa0b29947c5887880cb0f050d8b",
            "e5699c9aae1043df91f97617a134538a",
            "d444472a6842400abe4e00c13c88492b",
            "28b9d1ab5bb54ea48124cee3d1bb4cca",
            "5449b41d79a549d793ac7cfd265aa047",
            "e2411247bd3746fab11235114075e9e6",
            "5b2511f4ad6842378977bafcb34b231a",
            "19a04ac3be174ca3aa0308e098b43a4c",
            "d85b5eaacae94498a1cea1874fd7a142",
            "2c6c0748684b40b5baeb560c9aaa1435",
            "b020538af26740ec8786a8e2562914cd",
            "656c6e431612408993bab2fe08c7aa4a",
            "eea5c67ab73e4aed80a80c6787fb7bdb",
            "f114bd60657147d590f11c3a2f6ab6f0",
            "f25e1b168705470a8f722c017b7e9dda",
            "f1352baad2534a469c0b6212f0f73096",
            "27e0f91ad86a4d379480a0800712dbe5",
            "27222b8ba60349c4af3c3ec747a0c2ae",
            "fb8a55c42d3641c1b7bd73207da34fa6",
            "3e19c9eec10c46bab7f0ddb8943a8f89",
            "8edda7f4b86547d9b4a598a767a2fc0d",
            "d4b92f20e773407ab54c500e2d08ea7e",
            "c37fbe0727404cad99f817e52b736b02",
            "098c1ea984484af9a9c4b9b48eb7a1e3",
            "09af792c239d4e55afaadfe3d7a4aa36",
            "d412033963a14e249ffffc88721357d4",
            "82cbfe20ec4e43618c0c9fe23d89fc82"
          ]
        },
        "id": "Dt9-e49TRv0n",
        "outputId": "58812e80-4f10-47a0-f1c4-8d0aec8f64fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa9bdeaae7144c47a0e5072231fb48a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0aafd7d59b614232878a10eac0d9b179"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bed2a09d8404494d90b395a252754279"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60cef83b2d3f4375ada6ce3a351e0aa6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2411247bd3746fab11235114075e9e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27e0f91ad86a4d379480a0800712dbe5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Metadata company name\n",
        "prompt_metadata = PromptTemplate(\n",
        "template=\"\"\"\n",
        "  <|assistant|> You need to identify which of the companies from the metadata list is mentioned in the users input.\n",
        "  Database metadata with company names: {metadata_list}.\n",
        "  Format your response as a JSON object with only a single key 'company', without any additional commentary or explanations.\n",
        "  You do not need to try to answer the question itself.\n",
        "  <|user|>Users input: {input}<|end|>\n",
        "  <|assistant|>\n",
        "\"\"\",\n",
        "input_variables=[\"input\", \"metadata_list\"])\n",
        "\n",
        "retrieval_metadata = prompt_metadata | llm_phi3 | JsonOutputParser()"
      ],
      "metadata": {
        "id": "kJanQu9w9VFH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persistent_client = chromadb.PersistentClient('/content/drive/MyDrive/Thesis/chromadb')\n",
        "collection = persistent_client.get_or_create_collection(\"reports_l2\")\n",
        "fs = LocalFileStore('/content/drive/MyDrive/Thesis/reports_store_location')\n",
        "store = create_kv_docstore(fs)\n",
        "vectorstore = Chroma(client = persistent_client,\n",
        "                     collection_name=\"reports_l2\",\n",
        "                     embedding_function=bge_embeddings,\n",
        "                     persist_directory='/content/drive/MyDrive/Thesis/chromadb')\n",
        "vectorstore.persist()"
      ],
      "metadata": {
        "id": "ncJiWt939VFI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = vectorstore.get()['metadatas']\n",
        "metadata_list = []\n",
        "for i in range(len(metadata)):\n",
        "  metadata_list.append(metadata[i]['company'])\n",
        "metadata_list = list(set(metadata_list))"
      ],
      "metadata": {
        "id": "Xcq5E9K19VFI"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Retrieval Grader\n",
        "llm_retrieval = llm_phi3\n",
        "\n",
        "prompt_retrieval_grader = PromptTemplate(\n",
        "    template=\"\"\"<|assistant|> You are a grader assessing relevance of a retrieved document to an evidence text.\n",
        "    If the retrieved document contains the same information as an evidence text, grade it as relevant. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the evidence text.<|end|>\n",
        "    <|user|> Here is the retrieved document: {document}\\n Here is the evidence text: {evidence_text} <|end|>\n",
        "    <|assistant|>\n",
        "    \"\"\",\n",
        "    input_variables=[\"evidence_text\", \"document\"],\n",
        ")\n",
        "\n",
        "retrieval_grader = prompt_retrieval_grader | llm_retrieval | StrOutputParser()"
      ],
      "metadata": {
        "id": "aEFPpe0M9VFL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "llm_generate = llm_phi3\n",
        "\n",
        "prompt_generate = PromptTemplate(\n",
        "    template=\"\"\"<|assistant|> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know. Keep the answer concise <|end|>\n",
        "    <|user|> Question: {question}. \\n Context: {documents} \\n Answer: <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"question\", \"documents\"],\n",
        ")\n",
        "\n",
        "rag_chain = prompt_generate | llm_generate | StrOutputParser()"
      ],
      "metadata": {
        "id": "hdn828f89VFM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Hallucination Grader\n",
        "llm_hallucination_grader = llm_phi3\n",
        "\n",
        "# Prompt\n",
        "prompt_hallucination_grader = PromptTemplate(\n",
        "    template=\"\"\" <|assistant|> You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
        "    Give a binary 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts.<|end|>\n",
        "    <|user|> Here are the facts: {documents} \\n Here is the answer: {generation}  <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"generation\", \"documents\"],\n",
        ")\n",
        "\n",
        "hallucination_grader = prompt_hallucination_grader | llm_hallucination_grader | StrOutputParser()"
      ],
      "metadata": {
        "id": "ZIub6HTG9VFM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Answer Grader\n",
        "llm_answer_grader = llm_phi3\n",
        "\n",
        "# Prompt\n",
        "prompt_answer_grader = PromptTemplate(\n",
        "    template=\"\"\"<|assistant|> You are a grader assessing whether a generated answer is correct or incorrect,\n",
        "    assecing it with the ground truth.\n",
        "    Give a binary score 'yes' or 'no' to indicate whether the generated answer equal to or contains the ground truth.<|end|>\n",
        "    <|user|> Here is the answer: {generation} \\n Here is the ground truth: {truth} <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"generation\", \"truth\"],\n",
        ")\n",
        "\n",
        "answer_grader = prompt_answer_grader | llm_answer_grader | StrOutputParser()"
      ],
      "metadata": {
        "id": "vWM7EZxJ9VFN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PAR_CHUNKS = 20\n",
        "N_DOCS_RETURN = 2\n",
        "\n",
        "results_list = []\n",
        "\n",
        "for i in tqdm(range(len(questions))):\n",
        "    query = questions['question'][i]\n",
        "    company = retrieval_metadata.invoke({\"input\": questions['company'][i], \"metadata_list\": metadata_list})\n",
        "\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=256)\n",
        "    big_chunks_retriever = ParentDocumentRetriever(\n",
        "      vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter,\n",
        "      search_kwargs={'filter': {'company': company['company']}, 'k': NUM_PAR_CHUNKS})\n",
        "\n",
        "    passage = big_chunks_retriever.invoke(query)\n",
        "    texts = []\n",
        "    for i in range(len(passage)):\n",
        "      texts.append([query, passage[i].page_content])\n",
        "    try:\n",
        "      if not texts:\n",
        "        raise ValueError('Texts list is empty')\n",
        "      scores = reranker.compute_score(texts)\n",
        "      combined = list(zip(texts, scores))\n",
        "      sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
        "      top_texts = [item[0] for item in sorted_combined[:N_DOCS_RETURN]]\n",
        "      docs = [inner_list[1] for inner_list in top_texts if len(inner_list)>1]\n",
        "\n",
        "      retrieval_grade = retrieval_grader.invoke({\"evidence_text\": questions['evidence'][i], \"document\": docs})\n",
        "      generation = rag_chain.invoke({\"documents\": docs, \"question\": query})\n",
        "      hallucination_grade = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
        "      answer_grade = answer_grader.invoke({\"truth\": questions['answer'][i], \"generation\": generation})\n",
        "\n",
        "      results_list.append(pd.DataFrame({\n",
        "            'question': [query],\n",
        "            'response': [generation],\n",
        "            'context': [docs],\n",
        "            'retrieval_grade': [retrieval_grade],\n",
        "            'hallucination_grade': [hallucination_grade],\n",
        "            'answer_grade': [answer_grade]\n",
        "        }))\n",
        "\n",
        "      results = pd.concat(results_list, ignore_index=True)\n",
        "      results.to_json(f'/content/drive/MyDrive/Thesis/rag_evaluation/financebench150/eval.json')\n",
        "\n",
        "    except ValueError as e:\n",
        "      print(f\"Skipping question {i} due to error: {e}\")\n",
        "      continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro7GaCz69VFO",
        "outputId": "9bfc8f00-662f-4410-fd1f-f75ee0df6597"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/150 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      59.68 ms /    89 runs   (    0.67 ms per token,  1491.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2145.81 ms /   381 tokens (    5.63 ms per token,   177.55 tokens per second)\n",
            "llama_print_timings:        eval time =    3570.59 ms /    88 runs   (   40.57 ms per token,    24.65 tokens per second)\n",
            "llama_print_timings:       total time =    5866.81 ms /   469 tokens\n",
            "  1%|          | 1/150 [00:18<46:29, 18.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6834) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      49.30 ms /    89 runs   (    0.55 ms per token,  1805.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3580.26 ms /    89 runs   (   40.23 ms per token,    24.86 tokens per second)\n",
            "llama_print_timings:       total time =    3691.00 ms /    89 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     164.50 ms /   256 runs   (    0.64 ms per token,  1556.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17832.84 ms /  2768 tokens (    6.44 ms per token,   155.22 tokens per second)\n",
            "llama_print_timings:        eval time =   13593.84 ms /   256 runs   (   53.10 ms per token,    18.83 tokens per second)\n",
            "llama_print_timings:       total time =   31945.58 ms /  3024 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      17.62 ms /    30 runs   (    0.59 ms per token,  1702.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7740.63 ms /  1347 tokens (    5.75 ms per token,   174.02 tokens per second)\n",
            "llama_print_timings:        eval time =    1238.29 ms /    29 runs   (   42.70 ms per token,    23.42 tokens per second)\n",
            "llama_print_timings:       total time =    9037.96 ms /  1376 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      70.06 ms /   106 runs   (    0.66 ms per token,  1512.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7515.36 ms /  1332 tokens (    5.64 ms per token,   177.24 tokens per second)\n",
            "llama_print_timings:        eval time =    4504.52 ms /   105 runs   (   42.90 ms per token,    23.31 tokens per second)\n",
            "llama_print_timings:       total time =   12202.76 ms /  1437 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      34.85 ms /    65 runs   (    0.54 ms per token,  1864.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     808.69 ms /   157 tokens (    5.15 ms per token,   194.14 tokens per second)\n",
            "llama_print_timings:        eval time =    2497.38 ms /    64 runs   (   39.02 ms per token,    25.63 tokens per second)\n",
            "llama_print_timings:       total time =    3375.06 ms /   221 tokens\n",
            "  1%|▏         | 2/150 [01:31<2:05:10, 50.75s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      49.15 ms /    89 runs   (    0.55 ms per token,  1810.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2063.75 ms /   395 tokens (    5.22 ms per token,   191.40 tokens per second)\n",
            "llama_print_timings:        eval time =    3460.90 ms /    88 runs   (   39.33 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    5628.23 ms /   483 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     144.85 ms /   256 runs   (    0.57 ms per token,  1767.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21784.02 ms /  3290 tokens (    6.62 ms per token,   151.03 tokens per second)\n",
            "llama_print_timings:        eval time =   14336.82 ms /   255 runs   (   56.22 ms per token,    17.79 tokens per second)\n",
            "llama_print_timings:       total time =   36590.28 ms /  3545 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      62.27 ms /   105 runs   (    0.59 ms per token,  1686.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5656.77 ms /  1022 tokens (    5.53 ms per token,   180.67 tokens per second)\n",
            "llama_print_timings:        eval time =    4365.36 ms /   104 runs   (   41.97 ms per token,    23.82 tokens per second)\n",
            "llama_print_timings:       total time =   10165.37 ms /  1126 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.21 ms /     2 runs   (    0.60 ms per token,  1657.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6235.89 ms /  1116 tokens (    5.59 ms per token,   178.96 tokens per second)\n",
            "llama_print_timings:        eval time =      40.60 ms /     1 runs   (   40.60 ms per token,    24.63 tokens per second)\n",
            "llama_print_timings:       total time =    6292.93 ms /  1117 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      33.80 ms /    59 runs   (    0.57 ms per token,  1745.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =     926.45 ms /   184 tokens (    5.04 ms per token,   198.61 tokens per second)\n",
            "llama_print_timings:        eval time =    2268.50 ms /    58 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    3265.03 ms /   242 tokens\n",
            "  2%|▏         | 3/150 [02:43<2:27:11, 60.08s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      49.32 ms /    89 runs   (    0.55 ms per token,  1804.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2064.44 ms /   395 tokens (    5.23 ms per token,   191.33 tokens per second)\n",
            "llama_print_timings:        eval time =    3474.62 ms /    88 runs   (   39.48 ms per token,    25.33 tokens per second)\n",
            "llama_print_timings:       total time =    5640.82 ms /   483 tokens\n",
            "  3%|▎         | 4/150 [03:00<1:45:39, 43.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8515) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      55.13 ms /    89 runs   (    0.62 ms per token,  1614.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3482.79 ms /    89 runs   (   39.13 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    3601.06 ms /    89 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     151.86 ms /   256 runs   (    0.59 ms per token,  1685.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =   22837.67 ms /  3398 tokens (    6.72 ms per token,   148.79 tokens per second)\n",
            "llama_print_timings:        eval time =   14547.97 ms /   255 runs   (   57.05 ms per token,    17.53 tokens per second)\n",
            "llama_print_timings:       total time =   37844.44 ms /  3653 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      19.42 ms /    28 runs   (    0.69 ms per token,  1441.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6547.64 ms /  1162 tokens (    5.63 ms per token,   177.47 tokens per second)\n",
            "llama_print_timings:        eval time =    1140.64 ms /    27 runs   (   42.25 ms per token,    23.67 tokens per second)\n",
            "llama_print_timings:       total time =    7743.41 ms /  1189 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1800.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6487.19 ms /  1168 tokens (    5.55 ms per token,   180.05 tokens per second)\n",
            "llama_print_timings:        eval time =      81.89 ms /     2 runs   (   40.95 ms per token,    24.42 tokens per second)\n",
            "llama_print_timings:       total time =    6585.98 ms /  1170 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      40.96 ms /    69 runs   (    0.59 ms per token,  1684.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     523.12 ms /   103 tokens (    5.08 ms per token,   196.90 tokens per second)\n",
            "llama_print_timings:        eval time =    2639.22 ms /    68 runs   (   38.81 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    3241.41 ms /   171 tokens\n",
            "  3%|▎         | 5/150 [04:15<2:11:40, 54.49s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      55.26 ms /    89 runs   (    0.62 ms per token,  1610.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2093.68 ms /   395 tokens (    5.30 ms per token,   188.66 tokens per second)\n",
            "llama_print_timings:        eval time =    3454.48 ms /    88 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    5668.06 ms /   483 tokens\n",
            "  4%|▍         | 6/150 [04:26<1:35:18, 39.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5427) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      50.82 ms /    89 runs   (    0.57 ms per token,  1751.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3500.52 ms /    89 runs   (   39.33 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    3607.09 ms /    89 tokens\n",
            "  5%|▍         | 7/150 [04:39<1:14:24, 31.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6892) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      49.28 ms /    89 runs   (    0.55 ms per token,  1805.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3488.62 ms /    89 runs   (   39.20 ms per token,    25.51 tokens per second)\n",
            "llama_print_timings:       total time =    3588.54 ms /    89 tokens\n",
            "  5%|▌         | 8/150 [04:49<57:29, 24.29s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5484) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      42.27 ms /    77 runs   (    0.55 ms per token,  1821.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =      41.34 ms /     7 tokens (    5.91 ms per token,   169.33 tokens per second)\n",
            "llama_print_timings:        eval time =    2981.96 ms /    76 runs   (   39.24 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    3107.74 ms /    83 tokens\n",
            "  6%|▌         | 9/150 [04:54<42:58, 18.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (7700) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      42.82 ms /    77 runs   (    0.56 ms per token,  1798.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3046.03 ms /    77 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    3132.72 ms /    77 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     108.92 ms /   196 runs   (    0.56 ms per token,  1799.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =   27253.41 ms /  3899 tokens (    6.99 ms per token,   143.06 tokens per second)\n",
            "llama_print_timings:        eval time =   11420.49 ms /   195 runs   (   58.57 ms per token,    17.07 tokens per second)\n",
            "llama_print_timings:       total time =   39072.17 ms /  4094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     157.53 ms /   256 runs   (    0.62 ms per token,  1625.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15408.70 ms /  2495 tokens (    6.18 ms per token,   161.92 tokens per second)\n",
            "llama_print_timings:        eval time =   12973.99 ms /   255 runs   (   50.88 ms per token,    19.65 tokens per second)\n",
            "llama_print_timings:       total time =   28869.05 ms /  2750 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     146.51 ms /   256 runs   (    0.57 ms per token,  1747.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16915.92 ms /  2690 tokens (    6.29 ms per token,   159.02 tokens per second)\n",
            "llama_print_timings:        eval time =   13331.10 ms /   255 runs   (   52.28 ms per token,    19.13 tokens per second)\n",
            "llama_print_timings:       total time =   30668.41 ms /  2945 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      28.27 ms /    43 runs   (    0.66 ms per token,  1521.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1921.45 ms /   363 tokens (    5.29 ms per token,   188.92 tokens per second)\n",
            "llama_print_timings:        eval time =    1659.23 ms /    42 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    3643.10 ms /   405 tokens\n",
            "  7%|▋         | 10/150 [06:42<1:47:42, 46.16s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      50.15 ms /    88 runs   (    0.57 ms per token,  1754.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2074.64 ms /   394 tokens (    5.27 ms per token,   189.91 tokens per second)\n",
            "llama_print_timings:        eval time =    3432.67 ms /    87 runs   (   39.46 ms per token,    25.34 tokens per second)\n",
            "llama_print_timings:       total time =    5613.90 ms /   481 tokens\n",
            "  7%|▋         | 11/150 [06:57<1:24:47, 36.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (6060) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      51.55 ms /    88 runs   (    0.59 ms per token,  1707.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3461.84 ms /    88 runs   (   39.34 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    3568.58 ms /    88 tokens\n",
            "  8%|▊         | 12/150 [07:10<1:07:29, 29.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (7670) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      50.24 ms /    88 runs   (    0.57 ms per token,  1751.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3459.26 ms /    88 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    3566.16 ms /    88 tokens\n",
            "  9%|▊         | 13/150 [07:16<50:57, 22.32s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (5366) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      58.45 ms /    88 runs   (    0.66 ms per token,  1505.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3477.23 ms /    88 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    3606.03 ms /    88 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      28.01 ms /    49 runs   (    0.57 ms per token,  1749.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =   28547.13 ms /  4046 tokens (    7.06 ms per token,   141.73 tokens per second)\n",
            "llama_print_timings:        eval time =    2843.53 ms /    48 runs   (   59.24 ms per token,    16.88 tokens per second)\n",
            "llama_print_timings:       total time =   31524.75 ms /  4094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     127.73 ms /   203 runs   (    0.63 ms per token,  1589.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10743.48 ms /  1820 tokens (    5.90 ms per token,   169.41 tokens per second)\n",
            "llama_print_timings:        eval time =    8965.07 ms /   202 runs   (   44.38 ms per token,    22.53 tokens per second)\n",
            "llama_print_timings:       total time =   20062.81 ms /  2022 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     115.53 ms /   203 runs   (    0.57 ms per token,  1757.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11853.09 ms /  1992 tokens (    5.95 ms per token,   168.06 tokens per second)\n",
            "llama_print_timings:        eval time =    9003.12 ms /   202 runs   (   44.57 ms per token,    22.44 tokens per second)\n",
            "llama_print_timings:       total time =   21145.73 ms /  2194 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.48 ms /     2 runs   (    0.74 ms per token,  1355.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1430.72 ms /   278 tokens (    5.15 ms per token,   194.31 tokens per second)\n",
            "llama_print_timings:        eval time =      38.36 ms /     1 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    1475.86 ms /   279 tokens\n",
            "  9%|▉         | 14/150 [08:44<1:35:19, 42.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      56.72 ms /    88 runs   (    0.64 ms per token,  1551.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2080.48 ms /   394 tokens (    5.28 ms per token,   189.38 tokens per second)\n",
            "llama_print_timings:        eval time =    3423.97 ms /    87 runs   (   39.36 ms per token,    25.41 tokens per second)\n",
            "llama_print_timings:       total time =    5623.37 ms /   481 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     108.44 ms /   210 runs   (    0.52 ms per token,  1936.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26813.37 ms /  3885 tokens (    6.90 ms per token,   144.89 tokens per second)\n",
            "llama_print_timings:        eval time =   12250.44 ms /   209 runs   (   58.61 ms per token,    17.06 tokens per second)\n",
            "llama_print_timings:       total time =   39430.11 ms /  4094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.90 ms /    72 runs   (    0.55 ms per token,  1804.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9533.60 ms /  1640 tokens (    5.81 ms per token,   172.02 tokens per second)\n",
            "llama_print_timings:        eval time =    3097.04 ms /    71 runs   (   43.62 ms per token,    22.93 tokens per second)\n",
            "llama_print_timings:       total time =   12743.28 ms /  1711 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     101.07 ms /   186 runs   (    0.54 ms per token,  1840.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9912.41 ms /  1700 tokens (    5.83 ms per token,   171.50 tokens per second)\n",
            "llama_print_timings:        eval time =    8108.06 ms /   185 runs   (   43.83 ms per token,    22.82 tokens per second)\n",
            "llama_print_timings:       total time =   18267.21 ms /  1885 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     165.88 ms /   256 runs   (    0.65 ms per token,  1543.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     763.81 ms /   147 tokens (    5.20 ms per token,   192.46 tokens per second)\n",
            "llama_print_timings:        eval time =    9971.55 ms /   255 runs   (   39.10 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =   11095.84 ms /   402 tokens\n",
            " 10%|█         | 15/150 [10:19<2:10:29, 58.00s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     117.81 ms /   183 runs   (    0.64 ms per token,  1553.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2066.46 ms /   395 tokens (    5.23 ms per token,   191.15 tokens per second)\n",
            "llama_print_timings:        eval time =    7235.49 ms /   182 runs   (   39.76 ms per token,    25.15 tokens per second)\n",
            "llama_print_timings:       total time =    9574.07 ms /   577 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     145.96 ms /   256 runs   (    0.57 ms per token,  1753.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21434.36 ms /  3253 tokens (    6.59 ms per token,   151.77 tokens per second)\n",
            "llama_print_timings:        eval time =   14148.75 ms /   255 runs   (   55.49 ms per token,    18.02 tokens per second)\n",
            "llama_print_timings:       total time =   36061.06 ms /  3508 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      56.79 ms /    84 runs   (    0.68 ms per token,  1479.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10698.62 ms /  1821 tokens (    5.88 ms per token,   170.21 tokens per second)\n",
            "llama_print_timings:        eval time =    3683.82 ms /    83 runs   (   44.38 ms per token,    22.53 tokens per second)\n",
            "llama_print_timings:       total time =   14534.15 ms /  1904 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      51.44 ms /    76 runs   (    0.68 ms per token,  1477.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10994.28 ms /  1871 tokens (    5.88 ms per token,   170.18 tokens per second)\n",
            "llama_print_timings:        eval time =    3342.97 ms /    75 runs   (   44.57 ms per token,    22.44 tokens per second)\n",
            "llama_print_timings:       total time =   14476.09 ms /  1946 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      32.76 ms /    56 runs   (    0.58 ms per token,  1709.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     969.26 ms /   190 tokens (    5.10 ms per token,   196.03 tokens per second)\n",
            "llama_print_timings:        eval time =    2154.09 ms /    55 runs   (   39.17 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    3183.05 ms /   245 tokens\n",
            " 11%|█         | 16/150 [11:44<2:28:04, 66.31s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     115.98 ms /   183 runs   (    0.63 ms per token,  1577.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2066.55 ms /   395 tokens (    5.23 ms per token,   191.14 tokens per second)\n",
            "llama_print_timings:        eval time =    7227.54 ms /   182 runs   (   39.71 ms per token,    25.18 tokens per second)\n",
            "llama_print_timings:       total time =    9555.77 ms /   577 tokens\n",
            " 11%|█▏        | 17/150 [12:02<1:54:38, 51.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (7493) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     159.76 ms /   256 runs   (    0.62 ms per token,  1602.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =   10179.81 ms /   256 runs   (   39.76 ms per token,    25.15 tokens per second)\n",
            "llama_print_timings:       total time =   10550.48 ms /   256 tokens\n",
            " 12%|█▏        | 18/150 [12:18<1:29:54, 40.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8879) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      40.94 ms /    65 runs   (    0.63 ms per token,  1587.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =      38.78 ms /     3 tokens (   12.93 ms per token,    77.36 tokens per second)\n",
            "llama_print_timings:        eval time =    2546.60 ms /    64 runs   (   39.79 ms per token,    25.13 tokens per second)\n",
            "llama_print_timings:       total time =    2667.31 ms /    67 tokens\n",
            " 13%|█▎        | 19/150 [12:36<1:14:14, 34.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 15 due to error: Requested tokens (4404) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      38.39 ms /    65 runs   (    0.59 ms per token,  1693.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2557.26 ms /    65 runs   (   39.34 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    2633.28 ms /    65 tokens\n",
            " 13%|█▎        | 20/150 [12:44<57:00, 26.31s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8825) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.43 ms /    65 runs   (    0.64 ms per token,  1568.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2578.43 ms /    65 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    2669.93 ms /    65 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     140.68 ms /   256 runs   (    0.55 ms per token,  1819.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21696.31 ms /  3256 tokens (    6.66 ms per token,   150.07 tokens per second)\n",
            "llama_print_timings:        eval time =   14141.50 ms /   255 runs   (   55.46 ms per token,    18.03 tokens per second)\n",
            "llama_print_timings:       total time =   36314.16 ms /  3511 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      15.33 ms /    30 runs   (    0.51 ms per token,  1956.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12453.60 ms /  2074 tokens (    6.00 ms per token,   166.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1294.22 ms /    29 runs   (   44.63 ms per token,    22.41 tokens per second)\n",
            "llama_print_timings:       total time =   13814.91 ms /  2103 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      46.36 ms /    85 runs   (    0.55 ms per token,  1833.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12359.16 ms /  2072 tokens (    5.96 ms per token,   167.65 tokens per second)\n",
            "llama_print_timings:        eval time =    3766.04 ms /    84 runs   (   44.83 ms per token,    22.30 tokens per second)\n",
            "llama_print_timings:       total time =   16251.52 ms /  2156 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      43.89 ms /    68 runs   (    0.65 ms per token,  1549.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1067.49 ms /   203 tokens (    5.26 ms per token,   190.17 tokens per second)\n",
            "llama_print_timings:        eval time =    2629.73 ms /    67 runs   (   39.25 ms per token,    25.48 tokens per second)\n",
            "llama_print_timings:       total time =    3792.23 ms /   270 tokens\n",
            " 14%|█▍        | 21/150 [14:01<1:29:10, 41.48s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      30.26 ms /    55 runs   (    0.55 ms per token,  1817.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2067.61 ms /   394 tokens (    5.25 ms per token,   190.56 tokens per second)\n",
            "llama_print_timings:        eval time =    2136.91 ms /    54 runs   (   39.57 ms per token,    25.27 tokens per second)\n",
            "llama_print_timings:       total time =    4270.15 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      20.62 ms /    37 runs   (    0.56 ms per token,  1794.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =   28441.54 ms /  4058 tokens (    7.01 ms per token,   142.68 tokens per second)\n",
            "llama_print_timings:        eval time =    2130.15 ms /    36 runs   (   59.17 ms per token,    16.90 tokens per second)\n",
            "llama_print_timings:       total time =   30674.58 ms /  4094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      56.26 ms /   101 runs   (    0.56 ms per token,  1795.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10875.14 ms /  1832 tokens (    5.94 ms per token,   168.46 tokens per second)\n",
            "llama_print_timings:        eval time =    4505.46 ms /   101 runs   (   44.61 ms per token,    22.42 tokens per second)\n",
            "llama_print_timings:       total time =   15529.55 ms /  1933 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      60.53 ms /   100 runs   (    0.61 ms per token,  1652.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11214.48 ms /  1896 tokens (    5.91 ms per token,   169.07 tokens per second)\n",
            "llama_print_timings:        eval time =    4381.44 ms /    99 runs   (   44.26 ms per token,    22.60 tokens per second)\n",
            "llama_print_timings:       total time =   15752.99 ms /  1995 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.71 ms /    71 runs   (    0.63 ms per token,  1587.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1150.28 ms /   223 tokens (    5.16 ms per token,   193.87 tokens per second)\n",
            "llama_print_timings:        eval time =    2744.75 ms /    70 runs   (   39.21 ms per token,    25.50 tokens per second)\n",
            "llama_print_timings:       total time =    3990.06 ms /   293 tokens\n",
            " 15%|█▍        | 22/150 [15:26<1:56:02, 54.39s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      30.13 ms /    55 runs   (    0.55 ms per token,  1825.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2057.56 ms /   394 tokens (    5.22 ms per token,   191.49 tokens per second)\n",
            "llama_print_timings:        eval time =    2119.92 ms /    54 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    4241.33 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     145.93 ms /   256 runs   (    0.57 ms per token,  1754.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17555.08 ms /  2773 tokens (    6.33 ms per token,   157.96 tokens per second)\n",
            "llama_print_timings:        eval time =   13683.76 ms /   255 runs   (   53.66 ms per token,    18.64 tokens per second)\n",
            "llama_print_timings:       total time =   31665.10 ms /  3028 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.41 ms /    73 runs   (    0.57 ms per token,  1762.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7596.67 ms /  1326 tokens (    5.73 ms per token,   174.55 tokens per second)\n",
            "llama_print_timings:        eval time =    3120.17 ms /    72 runs   (   43.34 ms per token,    23.08 tokens per second)\n",
            "llama_print_timings:       total time =   10818.57 ms /  1398 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.53 ms /     2 runs   (    0.77 ms per token,  1307.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7855.08 ms /  1379 tokens (    5.70 ms per token,   175.56 tokens per second)\n",
            "llama_print_timings:        eval time =      42.14 ms /     1 runs   (   42.14 ms per token,    23.73 tokens per second)\n",
            "llama_print_timings:       total time =    7918.64 ms /  1380 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      27.60 ms /    45 runs   (    0.61 ms per token,  1630.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1032.12 ms /   200 tokens (    5.16 ms per token,   193.78 tokens per second)\n",
            "llama_print_timings:        eval time =    1726.67 ms /    44 runs   (   39.24 ms per token,    25.48 tokens per second)\n",
            "llama_print_timings:       total time =    2818.56 ms /   244 tokens\n",
            " 15%|█▌        | 23/150 [16:34<2:04:04, 58.62s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      30.43 ms /    55 runs   (    0.55 ms per token,  1807.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2062.43 ms /   394 tokens (    5.23 ms per token,   191.04 tokens per second)\n",
            "llama_print_timings:        eval time =    2136.43 ms /    54 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    4259.72 ms /   448 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     141.69 ms /   256 runs   (    0.55 ms per token,  1806.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21784.15 ms /  3328 tokens (    6.55 ms per token,   152.77 tokens per second)\n",
            "llama_print_timings:        eval time =   14335.46 ms /   256 runs   (   56.00 ms per token,    17.86 tokens per second)\n",
            "llama_print_timings:       total time =   36544.41 ms /  3584 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     121.39 ms /   201 runs   (    0.60 ms per token,  1655.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11381.57 ms /  1907 tokens (    5.97 ms per token,   167.55 tokens per second)\n",
            "llama_print_timings:        eval time =    9010.29 ms /   200 runs   (   45.05 ms per token,    22.20 tokens per second)\n",
            "llama_print_timings:       total time =   20716.65 ms /  2107 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     129.54 ms /   230 runs   (    0.56 ms per token,  1775.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12336.83 ms /  2063 tokens (    5.98 ms per token,   167.22 tokens per second)\n",
            "llama_print_timings:        eval time =   10343.05 ms /   229 runs   (   45.17 ms per token,    22.14 tokens per second)\n",
            "llama_print_timings:       total time =   23017.73 ms /  2292 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      62.27 ms /    76 runs   (    0.82 ms per token,  1220.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1693.76 ms /   328 tokens (    5.16 ms per token,   193.65 tokens per second)\n",
            "llama_print_timings:        eval time =    2960.29 ms /    75 runs   (   39.47 ms per token,    25.34 tokens per second)\n",
            "llama_print_timings:       total time =    4795.93 ms /   403 tokens\n",
            " 16%|█▌        | 24/150 [18:11<2:27:26, 70.21s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      37.08 ms /    55 runs   (    0.67 ms per token,  1483.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2107.45 ms /   394 tokens (    5.35 ms per token,   186.96 tokens per second)\n",
            "llama_print_timings:        eval time =    2128.61 ms /    54 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    4314.25 ms /   448 tokens\n",
            " 17%|█▋        | 25/150 [18:25<1:50:42, 53.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 17 due to error: Requested tokens (7482) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      38.36 ms /    55 runs   (    0.70 ms per token,  1433.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2171.64 ms /    55 runs   (   39.48 ms per token,    25.33 tokens per second)\n",
            "llama_print_timings:       total time =    2252.29 ms /    55 tokens\n",
            " 17%|█▋        | 26/150 [18:37<1:24:16, 40.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5440) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.49 ms /    55 runs   (    0.66 ms per token,  1507.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2155.87 ms /    55 runs   (   39.20 ms per token,    25.51 tokens per second)\n",
            "llama_print_timings:       total time =    2230.88 ms /    55 tokens\n",
            " 18%|█▊        | 27/150 [18:46<1:04:19, 31.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (6128) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      30.69 ms /    55 runs   (    0.56 ms per token,  1791.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2165.74 ms /    55 runs   (   39.38 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    2231.73 ms /    55 tokens\n",
            " 19%|█▊        | 28/150 [18:53<48:41, 23.95s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8831) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.25 ms /    55 runs   (    0.71 ms per token,  1401.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2181.13 ms /    55 runs   (   39.66 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    2264.76 ms /    55 tokens\n",
            " 19%|█▉        | 29/150 [18:58<37:11, 18.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (7302) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      31.22 ms /    55 runs   (    0.57 ms per token,  1761.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2186.61 ms /    55 runs   (   39.76 ms per token,    25.15 tokens per second)\n",
            "llama_print_timings:       total time =    2251.65 ms /    55 tokens\n",
            " 20%|██        | 30/150 [19:12<34:06, 17.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6928) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      57.82 ms /   100 runs   (    0.58 ms per token,  1729.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =      39.16 ms /     4 tokens (    9.79 ms per token,   102.14 tokens per second)\n",
            "llama_print_timings:        eval time =    3961.27 ms /    99 runs   (   40.01 ms per token,    24.99 tokens per second)\n",
            "llama_print_timings:       total time =    4111.02 ms /   103 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     155.01 ms /   256 runs   (    0.61 ms per token,  1651.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17817.97 ms /  2778 tokens (    6.41 ms per token,   155.91 tokens per second)\n",
            "llama_print_timings:        eval time =   13551.79 ms /   255 runs   (   53.14 ms per token,    18.82 tokens per second)\n",
            "llama_print_timings:       total time =   31790.87 ms /  3033 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     108.51 ms /   193 runs   (    0.56 ms per token,  1778.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9943.26 ms /  1701 tokens (    5.85 ms per token,   171.07 tokens per second)\n",
            "llama_print_timings:        eval time =    8428.40 ms /   192 runs   (   43.90 ms per token,    22.78 tokens per second)\n",
            "llama_print_timings:       total time =   18639.38 ms /  1893 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1926.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10697.03 ms /  1819 tokens (    5.88 ms per token,   170.05 tokens per second)\n",
            "llama_print_timings:        eval time =      42.85 ms /     1 runs   (   42.85 ms per token,    23.34 tokens per second)\n",
            "llama_print_timings:       total time =   10765.57 ms /  1820 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      53.41 ms /    85 runs   (    0.63 ms per token,  1591.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1844.44 ms /   355 tokens (    5.20 ms per token,   192.47 tokens per second)\n",
            "llama_print_timings:        eval time =    3308.64 ms /    84 runs   (   39.39 ms per token,    25.39 tokens per second)\n",
            "llama_print_timings:       total time =    5262.00 ms /   439 tokens\n",
            " 21%|██        | 31/150 [20:28<1:08:58, 34.78s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      63.98 ms /   100 runs   (    0.64 ms per token,  1563.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2088.41 ms /   394 tokens (    5.30 ms per token,   188.66 tokens per second)\n",
            "llama_print_timings:        eval time =    3893.30 ms /    99 runs   (   39.33 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    6108.61 ms /   493 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     137.40 ms /   256 runs   (    0.54 ms per token,  1863.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20149.59 ms /  3112 tokens (    6.47 ms per token,   154.44 tokens per second)\n",
            "llama_print_timings:        eval time =   14223.24 ms /   256 runs   (   55.56 ms per token,    18.00 tokens per second)\n",
            "llama_print_timings:       total time =   34781.12 ms /  3368 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      73.17 ms /   120 runs   (    0.61 ms per token,  1640.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4800.20 ms /   867 tokens (    5.54 ms per token,   180.62 tokens per second)\n",
            "llama_print_timings:        eval time =    4991.94 ms /   119 runs   (   41.95 ms per token,    23.84 tokens per second)\n",
            "llama_print_timings:       total time =    9960.05 ms /   986 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      50.69 ms /    74 runs   (    0.68 ms per token,  1459.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5250.57 ms /   954 tokens (    5.50 ms per token,   181.69 tokens per second)\n",
            "llama_print_timings:        eval time =    3046.76 ms /    73 runs   (   41.74 ms per token,    23.96 tokens per second)\n",
            "llama_print_timings:       total time =    8422.06 ms /  1027 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       5.55 ms /     2 runs   (    2.77 ms per token,   360.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1023.99 ms /   199 tokens (    5.15 ms per token,   194.34 tokens per second)\n",
            "llama_print_timings:        eval time =      37.93 ms /     1 runs   (   37.93 ms per token,    26.37 tokens per second)\n",
            "llama_print_timings:       total time =    1072.78 ms /   200 tokens\n",
            " 21%|██▏       | 32/150 [21:41<1:30:32, 46.04s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      56.62 ms /   100 runs   (    0.57 ms per token,  1766.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2067.59 ms /   394 tokens (    5.25 ms per token,   190.56 tokens per second)\n",
            "llama_print_timings:        eval time =    3909.89 ms /    99 runs   (   39.49 ms per token,    25.32 tokens per second)\n",
            "llama_print_timings:       total time =    6091.29 ms /   493 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     139.05 ms /   256 runs   (    0.54 ms per token,  1841.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =   23287.84 ms /  3512 tokens (    6.63 ms per token,   150.81 tokens per second)\n",
            "llama_print_timings:        eval time =   14624.14 ms /   255 runs   (   57.35 ms per token,    17.44 tokens per second)\n",
            "llama_print_timings:       total time =   38348.63 ms /  3767 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      84.00 ms /   134 runs   (    0.63 ms per token,  1595.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7139.90 ms /  1259 tokens (    5.67 ms per token,   176.33 tokens per second)\n",
            "llama_print_timings:        eval time =    5749.03 ms /   133 runs   (   43.23 ms per token,    23.13 tokens per second)\n",
            "llama_print_timings:       total time =   13101.84 ms /  1392 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     107.87 ms /   163 runs   (    0.66 ms per token,  1511.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7867.57 ms /  1383 tokens (    5.69 ms per token,   175.78 tokens per second)\n",
            "llama_print_timings:        eval time =    6991.75 ms /   162 runs   (   43.16 ms per token,    23.17 tokens per second)\n",
            "llama_print_timings:       total time =   15150.05 ms /  1545 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      26.38 ms /    39 runs   (    0.68 ms per token,  1478.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1319.09 ms /   256 tokens (    5.15 ms per token,   194.07 tokens per second)\n",
            "llama_print_timings:        eval time =    1495.09 ms /    38 runs   (   39.34 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    2875.82 ms /   294 tokens\n",
            " 22%|██▏       | 33/150 [23:07<1:53:43, 58.32s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      56.97 ms /   100 runs   (    0.57 ms per token,  1755.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2079.70 ms /   394 tokens (    5.28 ms per token,   189.45 tokens per second)\n",
            "llama_print_timings:        eval time =    3900.79 ms /    99 runs   (   39.40 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    6100.76 ms /   493 tokens\n",
            " 23%|██▎       | 34/150 [23:17<1:24:39, 43.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8015) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      66.22 ms /   100 runs   (    0.66 ms per token,  1510.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3931.71 ms /   100 runs   (   39.32 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    4072.48 ms /   100 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     153.93 ms /   256 runs   (    0.60 ms per token,  1663.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12984.63 ms /  2156 tokens (    6.02 ms per token,   166.04 tokens per second)\n",
            "llama_print_timings:        eval time =   11825.36 ms /   255 runs   (   46.37 ms per token,    21.56 tokens per second)\n",
            "llama_print_timings:       total time =   25275.30 ms /  2411 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      93.93 ms /   144 runs   (    0.65 ms per token,  1533.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3894.71 ms /   719 tokens (    5.42 ms per token,   184.61 tokens per second)\n",
            "llama_print_timings:        eval time =    5963.41 ms /   143 runs   (   41.70 ms per token,    23.98 tokens per second)\n",
            "llama_print_timings:       total time =   10088.71 ms /   862 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      54.13 ms /    97 runs   (    0.56 ms per token,  1791.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4565.98 ms /   834 tokens (    5.47 ms per token,   182.66 tokens per second)\n",
            "llama_print_timings:        eval time =    3981.08 ms /    96 runs   (   41.47 ms per token,    24.11 tokens per second)\n",
            "llama_print_timings:       total time =    8670.57 ms /   930 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      27.61 ms /    40 runs   (    0.69 ms per token,  1448.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1304.04 ms /   250 tokens (    5.22 ms per token,   191.71 tokens per second)\n",
            "llama_print_timings:        eval time =    1532.06 ms /    39 runs   (   39.28 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =    2889.08 ms /   289 tokens\n",
            " 23%|██▎       | 35/150 [24:14<1:31:19, 47.65s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      61.80 ms /   100 runs   (    0.62 ms per token,  1618.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2100.76 ms /   394 tokens (    5.33 ms per token,   187.55 tokens per second)\n",
            "llama_print_timings:        eval time =    3922.81 ms /    99 runs   (   39.62 ms per token,    25.24 tokens per second)\n",
            "llama_print_timings:       total time =    6152.21 ms /   493 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     149.79 ms /   256 runs   (    0.59 ms per token,  1709.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16674.90 ms /  2664 tokens (    6.26 ms per token,   159.76 tokens per second)\n",
            "llama_print_timings:        eval time =   13265.89 ms /   255 runs   (   52.02 ms per token,    19.22 tokens per second)\n",
            "llama_print_timings:       total time =   30357.12 ms /  2919 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.44 ms /    69 runs   (    0.57 ms per token,  1749.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6872.35 ms /  1223 tokens (    5.62 ms per token,   177.96 tokens per second)\n",
            "llama_print_timings:        eval time =    2878.26 ms /    68 runs   (   42.33 ms per token,    23.63 tokens per second)\n",
            "llama_print_timings:       total time =    9842.52 ms /  1291 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      51.24 ms /    77 runs   (    0.67 ms per token,  1502.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7122.86 ms /  1267 tokens (    5.62 ms per token,   177.88 tokens per second)\n",
            "llama_print_timings:        eval time =    3253.08 ms /    76 runs   (   42.80 ms per token,    23.36 tokens per second)\n",
            "llama_print_timings:       total time =   10502.61 ms /  1343 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      32.51 ms /    54 runs   (    0.60 ms per token,  1660.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     897.47 ms /   175 tokens (    5.13 ms per token,   194.99 tokens per second)\n",
            "llama_print_timings:        eval time =    2071.23 ms /    53 runs   (   39.08 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    3032.81 ms /   228 tokens\n",
            " 24%|██▍       | 36/150 [25:17<1:39:07, 52.17s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      57.87 ms /   100 runs   (    0.58 ms per token,  1728.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2068.21 ms /   394 tokens (    5.25 ms per token,   190.50 tokens per second)\n",
            "llama_print_timings:        eval time =    3892.27 ms /    99 runs   (   39.32 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    6083.44 ms /   493 tokens\n",
            " 25%|██▍       | 37/150 [25:31<1:16:45, 40.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6704) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      59.07 ms /   100 runs   (    0.59 ms per token,  1692.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3942.49 ms /   100 runs   (   39.42 ms per token,    25.36 tokens per second)\n",
            "llama_print_timings:       total time =    4060.80 ms /   100 tokens\n",
            " 25%|██▌       | 38/150 [25:38<57:30, 30.81s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (4734) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     113.32 ms /   188 runs   (    0.60 ms per token,  1659.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =      40.12 ms /     4 tokens (   10.03 ms per token,    99.70 tokens per second)\n",
            "llama_print_timings:        eval time =    7361.50 ms /   187 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    7637.00 ms /   191 tokens\n",
            " 26%|██▌       | 39/150 [25:55<48:57, 26.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (5059) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     108.66 ms /   188 runs   (    0.58 ms per token,  1730.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    7474.38 ms /   188 runs   (   39.76 ms per token,    25.15 tokens per second)\n",
            "llama_print_timings:       total time =    7693.86 ms /   188 tokens\n",
            " 27%|██▋       | 40/150 [26:11<42:50, 23.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5427) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     115.78 ms /   188 runs   (    0.62 ms per token,  1623.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    7501.45 ms /   188 runs   (   39.90 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =    7750.25 ms /   188 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     146.38 ms /   256 runs   (    0.57 ms per token,  1748.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13923.87 ms /  2269 tokens (    6.14 ms per token,   162.96 tokens per second)\n",
            "llama_print_timings:        eval time =   12179.33 ms /   255 runs   (   47.76 ms per token,    20.94 tokens per second)\n",
            "llama_print_timings:       total time =   26517.60 ms /  2524 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      60.02 ms /    89 runs   (    0.67 ms per token,  1482.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4539.68 ms /   834 tokens (    5.44 ms per token,   183.71 tokens per second)\n",
            "llama_print_timings:        eval time =    3637.56 ms /    88 runs   (   41.34 ms per token,    24.19 tokens per second)\n",
            "llama_print_timings:       total time =    8318.49 ms /   922 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     161.15 ms /   243 runs   (    0.66 ms per token,  1507.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4873.22 ms /   892 tokens (    5.46 ms per token,   183.04 tokens per second)\n",
            "llama_print_timings:        eval time =   10027.30 ms /   242 runs   (   41.44 ms per token,    24.13 tokens per second)\n",
            "llama_print_timings:       total time =   15301.58 ms /  1134 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      43.88 ms /    76 runs   (    0.58 ms per token,  1731.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1011.67 ms /   195 tokens (    5.19 ms per token,   192.75 tokens per second)\n",
            "llama_print_timings:        eval time =    2931.92 ms /    75 runs   (   39.09 ms per token,    25.58 tokens per second)\n",
            "llama_print_timings:       total time =    4032.72 ms /   270 tokens\n",
            " 27%|██▋       | 41/150 [27:21<1:08:08, 37.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     119.74 ms /   188 runs   (    0.64 ms per token,  1570.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2065.90 ms /   394 tokens (    5.24 ms per token,   190.72 tokens per second)\n",
            "llama_print_timings:        eval time =    7423.08 ms /   187 runs   (   39.70 ms per token,    25.19 tokens per second)\n",
            "llama_print_timings:       total time =    9775.32 ms /   581 tokens\n",
            " 28%|██▊       | 42/150 [27:38<56:07, 31.18s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5960) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      70.14 ms /   110 runs   (    0.64 ms per token,  1568.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    4313.39 ms /   110 runs   (   39.21 ms per token,    25.50 tokens per second)\n",
            "llama_print_timings:       total time =    4463.30 ms /   110 tokens\n",
            " 29%|██▊       | 43/150 [27:53<47:06, 26.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5975) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      75.28 ms /   110 runs   (    0.68 ms per token,  1461.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    4335.84 ms /   110 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    4518.77 ms /   110 tokens\n",
            " 29%|██▉       | 44/150 [28:01<37:01, 20.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5687) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      64.44 ms /   110 runs   (    0.59 ms per token,  1707.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    4363.26 ms /   110 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    4498.62 ms /   110 tokens\n",
            " 30%|███       | 45/150 [28:10<30:10, 17.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5587) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      54.86 ms /    80 runs   (    0.69 ms per token,  1458.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =      39.44 ms /     4 tokens (    9.86 ms per token,   101.42 tokens per second)\n",
            "llama_print_timings:        eval time =    3188.18 ms /    79 runs   (   40.36 ms per token,    24.78 tokens per second)\n",
            "llama_print_timings:       total time =    3344.81 ms /    83 tokens\n",
            " 31%|███       | 46/150 [28:24<28:09, 16.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (5543) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      55.76 ms /    80 runs   (    0.70 ms per token,  1434.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3210.90 ms /    80 runs   (   40.14 ms per token,    24.92 tokens per second)\n",
            "llama_print_timings:       total time =    3334.33 ms /    80 tokens\n",
            " 31%|███▏      | 47/150 [28:33<24:04, 14.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8952) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      55.92 ms /    80 runs   (    0.70 ms per token,  1430.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3193.07 ms /    80 runs   (   39.91 ms per token,    25.05 tokens per second)\n",
            "llama_print_timings:       total time =    3317.23 ms /    80 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     144.71 ms /   256 runs   (    0.57 ms per token,  1769.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19145.94 ms /  2962 tokens (    6.46 ms per token,   154.71 tokens per second)\n",
            "llama_print_timings:        eval time =   13735.49 ms /   255 runs   (   53.86 ms per token,    18.57 tokens per second)\n",
            "llama_print_timings:       total time =   33351.70 ms /  3217 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      48.11 ms /    85 runs   (    0.57 ms per token,  1766.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8828.65 ms /  1527 tokens (    5.78 ms per token,   172.96 tokens per second)\n",
            "llama_print_timings:        eval time =    3634.01 ms /    84 runs   (   43.26 ms per token,    23.11 tokens per second)\n",
            "llama_print_timings:       total time =   12604.24 ms /  1611 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      68.28 ms /   113 runs   (    0.60 ms per token,  1655.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9151.80 ms /  1581 tokens (    5.79 ms per token,   172.75 tokens per second)\n",
            "llama_print_timings:        eval time =    4862.16 ms /   112 runs   (   43.41 ms per token,    23.04 tokens per second)\n",
            "llama_print_timings:       total time =   14192.94 ms /  1693 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      50.42 ms /    74 runs   (    0.68 ms per token,  1467.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     972.82 ms /   191 tokens (    5.09 ms per token,   196.34 tokens per second)\n",
            "llama_print_timings:        eval time =    2848.31 ms /    73 runs   (   39.02 ms per token,    25.63 tokens per second)\n",
            "llama_print_timings:       total time =    3928.24 ms /   264 tokens\n",
            " 32%|███▏      | 48/150 [29:49<55:45, 32.80s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      19.55 ms /    28 runs   (    0.70 ms per token,  1431.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2090.84 ms /   395 tokens (    5.29 ms per token,   188.92 tokens per second)\n",
            "llama_print_timings:        eval time =    1071.19 ms /    27 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    3211.79 ms /   422 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     148.71 ms /   256 runs   (    0.58 ms per token,  1721.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18753.08 ms /  2922 tokens (    6.42 ms per token,   155.81 tokens per second)\n",
            "llama_print_timings:        eval time =   13820.22 ms /   255 runs   (   54.20 ms per token,    18.45 tokens per second)\n",
            "llama_print_timings:       total time =   33078.05 ms /  3177 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     150.77 ms /   256 runs   (    0.59 ms per token,  1698.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8651.25 ms /  1504 tokens (    5.75 ms per token,   173.85 tokens per second)\n",
            "llama_print_timings:        eval time =   11177.75 ms /   256 runs   (   43.66 ms per token,    22.90 tokens per second)\n",
            "llama_print_timings:       total time =   20271.75 ms /  1760 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      40.79 ms /    74 runs   (    0.55 ms per token,  1814.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10007.87 ms /  1712 tokens (    5.85 ms per token,   171.07 tokens per second)\n",
            "llama_print_timings:        eval time =    3238.58 ms /    74 runs   (   43.76 ms per token,    22.85 tokens per second)\n",
            "llama_print_timings:       total time =   13350.64 ms /  1786 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      29.01 ms /    48 runs   (    0.60 ms per token,  1654.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1889.66 ms /   363 tokens (    5.21 ms per token,   192.10 tokens per second)\n",
            "llama_print_timings:        eval time =    1843.02 ms /    47 runs   (   39.21 ms per token,    25.50 tokens per second)\n",
            "llama_print_timings:       total time =    3790.64 ms /   410 tokens\n",
            " 33%|███▎      | 49/150 [31:12<1:20:31, 47.84s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      19.01 ms /    28 runs   (    0.68 ms per token,  1472.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2067.41 ms /   395 tokens (    5.23 ms per token,   191.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1061.62 ms /    27 runs   (   39.32 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    3173.71 ms /   422 tokens\n",
            " 33%|███▎      | 50/150 [31:24<1:01:28, 36.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (6532) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      16.30 ms /    28 runs   (    0.58 ms per token,  1718.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1096.90 ms /    28 runs   (   39.17 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    1128.38 ms /    28 tokens\n",
            " 34%|███▍      | 51/150 [31:30<45:42, 27.70s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (7137) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      18.93 ms /    28 runs   (    0.68 ms per token,  1479.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1095.29 ms /    28 runs   (   39.12 ms per token,    25.56 tokens per second)\n",
            "llama_print_timings:       total time =    1135.48 ms /    28 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     136.13 ms /   256 runs   (    0.53 ms per token,  1880.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21082.14 ms /  3197 tokens (    6.59 ms per token,   151.64 tokens per second)\n",
            "llama_print_timings:        eval time =   14298.61 ms /   255 runs   (   56.07 ms per token,    17.83 tokens per second)\n",
            "llama_print_timings:       total time =   35811.24 ms /  3452 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      75.00 ms /   113 runs   (    0.66 ms per token,  1506.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5169.49 ms /   943 tokens (    5.48 ms per token,   182.42 tokens per second)\n",
            "llama_print_timings:        eval time =    4678.19 ms /   112 runs   (   41.77 ms per token,    23.94 tokens per second)\n",
            "llama_print_timings:       total time =   10023.34 ms /  1055 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     152.85 ms /   247 runs   (    0.62 ms per token,  1615.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5664.58 ms /  1031 tokens (    5.49 ms per token,   182.01 tokens per second)\n",
            "llama_print_timings:        eval time =   10262.39 ms /   246 runs   (   41.72 ms per token,    23.97 tokens per second)\n",
            "llama_print_timings:       total time =   16298.26 ms /  1277 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      37.80 ms /    69 runs   (    0.55 ms per token,  1825.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     971.02 ms /   192 tokens (    5.06 ms per token,   197.73 tokens per second)\n",
            "llama_print_timings:        eval time =    2635.68 ms /    68 runs   (   38.76 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =    3677.49 ms /   260 tokens\n",
            " 35%|███▍      | 52/150 [32:47<1:09:32, 42.58s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      16.10 ms /    28 runs   (    0.58 ms per token,  1738.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2062.03 ms /   395 tokens (    5.22 ms per token,   191.56 tokens per second)\n",
            "llama_print_timings:        eval time =    1061.23 ms /    27 runs   (   39.30 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    3158.54 ms /   422 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     137.77 ms /   256 runs   (    0.54 ms per token,  1858.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25579.37 ms /  3760 tokens (    6.80 ms per token,   146.99 tokens per second)\n",
            "llama_print_timings:        eval time =   14927.32 ms /   255 runs   (   58.54 ms per token,    17.08 tokens per second)\n",
            "llama_print_timings:       total time =   40963.48 ms /  4015 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      26.22 ms /    41 runs   (    0.64 ms per token,  1563.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8669.18 ms /  1509 tokens (    5.74 ms per token,   174.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1735.03 ms /    40 runs   (   43.38 ms per token,    23.05 tokens per second)\n",
            "llama_print_timings:       total time =   10478.51 ms /  1549 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      57.31 ms /   100 runs   (    0.57 ms per token,  1744.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8767.28 ms /  1522 tokens (    5.76 ms per token,   173.60 tokens per second)\n",
            "llama_print_timings:        eval time =    4270.62 ms /    99 runs   (   43.14 ms per token,    23.18 tokens per second)\n",
            "llama_print_timings:       total time =   13176.14 ms /  1621 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      43.38 ms /    63 runs   (    0.69 ms per token,  1452.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     600.47 ms /   120 tokens (    5.00 ms per token,   199.84 tokens per second)\n",
            "llama_print_timings:        eval time =    2423.34 ms /    62 runs   (   39.09 ms per token,    25.58 tokens per second)\n",
            "llama_print_timings:       total time =    3113.61 ms /   182 tokens\n",
            " 35%|███▌      | 53/150 [34:06<1:26:22, 53.43s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      16.01 ms /    28 runs   (    0.57 ms per token,  1748.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2081.68 ms /   395 tokens (    5.27 ms per token,   189.75 tokens per second)\n",
            "llama_print_timings:        eval time =    1064.62 ms /    27 runs   (   39.43 ms per token,    25.36 tokens per second)\n",
            "llama_print_timings:       total time =    3180.33 ms /   422 tokens\n",
            " 36%|███▌      | 54/150 [34:19<1:05:57, 41.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8247) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      19.90 ms /    28 runs   (    0.71 ms per token,  1407.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1102.99 ms /    28 runs   (   39.39 ms per token,    25.39 tokens per second)\n",
            "llama_print_timings:       total time =    1141.01 ms /    28 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     135.84 ms /   256 runs   (    0.53 ms per token,  1884.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25375.23 ms /  3723 tokens (    6.82 ms per token,   146.72 tokens per second)\n",
            "llama_print_timings:        eval time =   14938.52 ms /   255 runs   (   58.58 ms per token,    17.07 tokens per second)\n",
            "llama_print_timings:       total time =   40745.95 ms /  3978 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.57 ms /    83 runs   (    0.54 ms per token,  1862.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8413.14 ms /  1464 tokens (    5.75 ms per token,   174.01 tokens per second)\n",
            "llama_print_timings:        eval time =    3587.74 ms /    83 runs   (   43.23 ms per token,    23.13 tokens per second)\n",
            "llama_print_timings:       total time =   12110.91 ms /  1547 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     117.89 ms /   200 runs   (    0.59 ms per token,  1696.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8748.11 ms /  1527 tokens (    5.73 ms per token,   174.55 tokens per second)\n",
            "llama_print_timings:        eval time =    8607.18 ms /   199 runs   (   43.25 ms per token,    23.12 tokens per second)\n",
            "llama_print_timings:       total time =   17652.28 ms /  1726 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.90 ms /    67 runs   (    0.67 ms per token,  1492.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     848.21 ms /   162 tokens (    5.24 ms per token,   190.99 tokens per second)\n",
            "llama_print_timings:        eval time =    2584.89 ms /    66 runs   (   39.16 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    3532.26 ms /   228 tokens\n",
            " 37%|███▋      | 55/150 [35:43<1:25:51, 54.23s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      18.66 ms /    28 runs   (    0.67 ms per token,  1500.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2087.99 ms /   395 tokens (    5.29 ms per token,   189.18 tokens per second)\n",
            "llama_print_timings:        eval time =    1066.69 ms /    27 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    3199.38 ms /   422 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     139.39 ms /   256 runs   (    0.54 ms per token,  1836.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19692.00 ms /  3052 tokens (    6.45 ms per token,   154.99 tokens per second)\n",
            "llama_print_timings:        eval time =   14007.42 ms /   255 runs   (   54.93 ms per token,    18.20 tokens per second)\n",
            "llama_print_timings:       total time =   34114.93 ms /  3307 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      46.69 ms /    78 runs   (    0.60 ms per token,  1670.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9337.47 ms /  1608 tokens (    5.81 ms per token,   172.21 tokens per second)\n",
            "llama_print_timings:        eval time =    3380.67 ms /    77 runs   (   43.90 ms per token,    22.78 tokens per second)\n",
            "llama_print_timings:       total time =   12836.01 ms /  1685 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     126.83 ms /   193 runs   (    0.66 ms per token,  1521.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9653.65 ms /  1663 tokens (    5.80 ms per token,   172.27 tokens per second)\n",
            "llama_print_timings:        eval time =    8412.67 ms /   192 runs   (   43.82 ms per token,    22.82 tokens per second)\n",
            "llama_print_timings:       total time =   18393.75 ms /  1855 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      25.55 ms /    47 runs   (    0.54 ms per token,  1839.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1053.90 ms /   205 tokens (    5.14 ms per token,   194.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1791.19 ms /    46 runs   (   38.94 ms per token,    25.68 tokens per second)\n",
            "llama_print_timings:       total time =    2906.68 ms /   251 tokens\n",
            " 37%|███▋      | 56/150 [37:02<1:36:25, 61.54s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      43.23 ms /    73 runs   (    0.59 ms per token,  1688.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2027.32 ms /   392 tokens (    5.17 ms per token,   193.36 tokens per second)\n",
            "llama_print_timings:        eval time =    2861.09 ms /    73 runs   (   39.19 ms per token,    25.51 tokens per second)\n",
            "llama_print_timings:       total time =    4986.09 ms /   465 tokens\n",
            " 38%|███▊      | 57/150 [37:08<1:09:46, 45.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 56 due to error: Texts list is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      51.88 ms /    73 runs   (    0.71 ms per token,  1407.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2899.30 ms /    73 runs   (   39.72 ms per token,    25.18 tokens per second)\n",
            "llama_print_timings:       total time =    3017.83 ms /    73 tokens\n",
            " 39%|███▊      | 58/150 [37:13<50:28, 32.91s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 57 due to error: Texts list is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      42.95 ms /    73 runs   (    0.59 ms per token,  1699.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2884.74 ms /    73 runs   (   39.52 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    2970.16 ms /    73 tokens\n",
            " 39%|███▉      | 59/150 [37:17<36:52, 24.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 58 due to error: Texts list is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      34.01 ms /    58 runs   (    0.59 ms per token,  1705.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =      38.43 ms /     4 tokens (    9.61 ms per token,   104.09 tokens per second)\n",
            "llama_print_timings:        eval time =    2255.35 ms /    57 runs   (   39.57 ms per token,    25.27 tokens per second)\n",
            "llama_print_timings:       total time =    2364.81 ms /    61 tokens\n",
            " 40%|████      | 60/150 [37:29<30:38, 20.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (5820) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      34.27 ms /    58 runs   (    0.59 ms per token,  1692.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2280.34 ms /    58 runs   (   39.32 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    2347.69 ms /    58 tokens\n",
            " 41%|████      | 61/150 [37:37<25:06, 16.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8566) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      33.01 ms /    58 runs   (    0.57 ms per token,  1756.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2297.89 ms /    58 runs   (   39.62 ms per token,    25.24 tokens per second)\n",
            "llama_print_timings:       total time =    2358.85 ms /    58 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     152.47 ms /   256 runs   (    0.60 ms per token,  1678.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15898.66 ms /  2530 tokens (    6.28 ms per token,   159.13 tokens per second)\n",
            "llama_print_timings:        eval time =   13457.58 ms /   255 runs   (   52.77 ms per token,    18.95 tokens per second)\n",
            "llama_print_timings:       total time =   29809.40 ms /  2785 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      18.48 ms /    32 runs   (    0.58 ms per token,  1731.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6052.04 ms /  1076 tokens (    5.62 ms per token,   177.79 tokens per second)\n",
            "llama_print_timings:        eval time =    1314.91 ms /    31 runs   (   42.42 ms per token,    23.58 tokens per second)\n",
            "llama_print_timings:       total time =    7417.78 ms /  1107 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     159.20 ms /   256 runs   (    0.62 ms per token,  1608.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6071.49 ms /  1096 tokens (    5.54 ms per token,   180.52 tokens per second)\n",
            "llama_print_timings:        eval time =   10737.99 ms /   255 runs   (   42.11 ms per token,    23.75 tokens per second)\n",
            "llama_print_timings:       total time =   17195.88 ms /  1351 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      85.21 ms /   140 runs   (    0.61 ms per token,  1642.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     721.03 ms /   138 tokens (    5.22 ms per token,   191.39 tokens per second)\n",
            "llama_print_timings:        eval time =    5395.21 ms /   139 runs   (   38.81 ms per token,    25.76 tokens per second)\n",
            "llama_print_timings:       total time =    6286.79 ms /   277 tokens\n",
            " 41%|████▏     | 62/150 [38:48<48:18, 32.93s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.30 ms /    58 runs   (    0.63 ms per token,  1597.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2080.10 ms /   394 tokens (    5.28 ms per token,   189.41 tokens per second)\n",
            "llama_print_timings:        eval time =    2237.93 ms /    57 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    4393.11 ms /   451 tokens\n",
            " 42%|████▏     | 63/150 [38:56<37:16, 25.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 11 due to error: Requested tokens (4317) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      33.91 ms /    58 runs   (    0.58 ms per token,  1710.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2275.10 ms /    58 runs   (   39.23 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    2342.86 ms /    58 tokens\n",
            " 43%|████▎     | 64/150 [39:07<30:20, 21.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 15 due to error: Requested tokens (4269) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      34.26 ms /    58 runs   (    0.59 ms per token,  1693.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2269.83 ms /    58 runs   (   39.13 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    2335.24 ms /    58 tokens\n",
            " 43%|████▎     | 65/150 [39:15<24:23, 17.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (4483) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.79 ms /    58 runs   (    0.63 ms per token,  1576.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2282.01 ms /    58 runs   (   39.35 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    2358.04 ms /    58 tokens\n",
            " 44%|████▍     | 66/150 [39:25<20:56, 14.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6626) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      34.75 ms /    58 runs   (    0.60 ms per token,  1669.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2285.57 ms /    58 runs   (   39.41 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    2353.91 ms /    58 tokens\n",
            " 45%|████▍     | 67/150 [39:33<17:47, 12.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5825) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      40.28 ms /    69 runs   (    0.58 ms per token,  1713.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =      41.91 ms /     7 tokens (    5.99 ms per token,   167.01 tokens per second)\n",
            "llama_print_timings:        eval time =    2672.99 ms /    68 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    2791.78 ms /    75 tokens\n",
            " 45%|████▌     | 68/150 [39:47<18:11, 13.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (9047) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.77 ms /    69 runs   (    0.58 ms per token,  1734.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2741.59 ms /    69 runs   (   39.73 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =    2818.62 ms /    69 tokens\n",
            " 46%|████▌     | 69/150 [39:55<15:44, 11.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (5917) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.51 ms /    69 runs   (    0.60 ms per token,  1662.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2766.11 ms /    69 runs   (   40.09 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =    2848.13 ms /    69 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      60.13 ms /   111 runs   (    0.54 ms per token,  1846.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =   27821.55 ms /  3984 tokens (    6.98 ms per token,   143.20 tokens per second)\n",
            "llama_print_timings:        eval time =    6440.66 ms /   110 runs   (   58.55 ms per token,    17.08 tokens per second)\n",
            "llama_print_timings:       total time =   34492.07 ms /  4094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     106.98 ms /   180 runs   (    0.59 ms per token,  1682.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10290.73 ms /  1766 tokens (    5.83 ms per token,   171.61 tokens per second)\n",
            "llama_print_timings:        eval time =    7887.46 ms /   179 runs   (   44.06 ms per token,    22.69 tokens per second)\n",
            "llama_print_timings:       total time =   18448.91 ms /  1945 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      56.16 ms /   100 runs   (    0.56 ms per token,  1780.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11100.87 ms /  1885 tokens (    5.89 ms per token,   169.81 tokens per second)\n",
            "llama_print_timings:        eval time =    4382.09 ms /    99 runs   (   44.26 ms per token,    22.59 tokens per second)\n",
            "llama_print_timings:       total time =   15621.88 ms /  1984 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1816.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1340.39 ms /   259 tokens (    5.18 ms per token,   193.23 tokens per second)\n",
            "llama_print_timings:        eval time =      38.51 ms /     1 runs   (   38.51 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    1384.34 ms /   260 tokens\n",
            " 47%|████▋     | 70/150 [41:15<42:58, 32.23s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      25.50 ms /    41 runs   (    0.62 ms per token,  1607.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2059.93 ms /   394 tokens (    5.23 ms per token,   191.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1580.35 ms /    40 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    3692.91 ms /   434 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     146.75 ms /   256 runs   (    0.57 ms per token,  1744.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24709.64 ms /  3655 tokens (    6.76 ms per token,   147.92 tokens per second)\n",
            "llama_print_timings:        eval time =   14925.86 ms /   255 runs   (   58.53 ms per token,    17.08 tokens per second)\n",
            "llama_print_timings:       total time =   40106.48 ms /  3910 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      62.67 ms /   120 runs   (    0.52 ms per token,  1914.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8646.79 ms /  1496 tokens (    5.78 ms per token,   173.01 tokens per second)\n",
            "llama_print_timings:        eval time =    5167.35 ms /   119 runs   (   43.42 ms per token,    23.03 tokens per second)\n",
            "llama_print_timings:       total time =   13989.41 ms /  1615 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      58.07 ms /    99 runs   (    0.59 ms per token,  1704.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8745.92 ms /  1518 tokens (    5.76 ms per token,   173.57 tokens per second)\n",
            "llama_print_timings:        eval time =    4214.53 ms /    98 runs   (   43.01 ms per token,    23.25 tokens per second)\n",
            "llama_print_timings:       total time =   13126.09 ms /  1616 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      84.99 ms /   115 runs   (    0.74 ms per token,  1353.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1011.59 ms /   195 tokens (    5.19 ms per token,   192.77 tokens per second)\n",
            "llama_print_timings:        eval time =    4465.77 ms /   114 runs   (   39.17 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    5686.03 ms /   309 tokens\n",
            " 47%|████▋     | 71/150 [42:47<1:06:06, 50.21s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      34.16 ms /    41 runs   (    0.83 ms per token,  1200.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2103.91 ms /   394 tokens (    5.34 ms per token,   187.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1582.59 ms /    40 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    3768.78 ms /   434 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     138.64 ms /   256 runs   (    0.54 ms per token,  1846.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21234.17 ms /  3243 tokens (    6.55 ms per token,   152.73 tokens per second)\n",
            "llama_print_timings:        eval time =   14377.18 ms /   255 runs   (   56.38 ms per token,    17.74 tokens per second)\n",
            "llama_print_timings:       total time =   36094.74 ms /  3498 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     146.56 ms /   256 runs   (    0.57 ms per token,  1746.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10900.34 ms /  1826 tokens (    5.97 ms per token,   167.52 tokens per second)\n",
            "llama_print_timings:        eval time =   11401.81 ms /   255 runs   (   44.71 ms per token,    22.36 tokens per second)\n",
            "llama_print_timings:       total time =   22729.43 ms /  2081 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     155.13 ms /   256 runs   (    0.61 ms per token,  1650.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12102.00 ms /  2032 tokens (    5.96 ms per token,   167.91 tokens per second)\n",
            "llama_print_timings:        eval time =   11472.14 ms /   256 runs   (   44.81 ms per token,    22.31 tokens per second)\n",
            "llama_print_timings:       total time =   24024.09 ms /  2288 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.28 ms /     2 runs   (    0.64 ms per token,  1562.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2003.60 ms /   384 tokens (    5.22 ms per token,   191.65 tokens per second)\n",
            "llama_print_timings:        eval time =      40.40 ms /     1 runs   (   40.40 ms per token,    24.75 tokens per second)\n",
            "llama_print_timings:       total time =    2053.36 ms /   385 tokens\n",
            " 48%|████▊     | 72/150 [44:28<1:24:56, 65.34s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      24.59 ms /    41 runs   (    0.60 ms per token,  1667.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2075.42 ms /   394 tokens (    5.27 ms per token,   189.84 tokens per second)\n",
            "llama_print_timings:        eval time =    1573.47 ms /    40 runs   (   39.34 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    3712.69 ms /   434 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     152.04 ms /   256 runs   (    0.59 ms per token,  1683.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24685.54 ms /  3635 tokens (    6.79 ms per token,   147.25 tokens per second)\n",
            "llama_print_timings:        eval time =   14785.22 ms /   255 runs   (   57.98 ms per token,    17.25 tokens per second)\n",
            "llama_print_timings:       total time =   39957.64 ms /  3890 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      20.67 ms /    39 runs   (    0.53 ms per token,  1886.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7946.02 ms /  1389 tokens (    5.72 ms per token,   174.80 tokens per second)\n",
            "llama_print_timings:        eval time =    1628.35 ms /    38 runs   (   42.85 ms per token,    23.34 tokens per second)\n",
            "llama_print_timings:       total time =    9635.27 ms /  1427 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.34 ms /    66 runs   (    0.55 ms per token,  1816.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8045.05 ms /  1411 tokens (    5.70 ms per token,   175.39 tokens per second)\n",
            "llama_print_timings:        eval time =    2777.06 ms /    65 runs   (   42.72 ms per token,    23.41 tokens per second)\n",
            "llama_print_timings:       total time =   10918.47 ms /  1476 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      45.66 ms /    82 runs   (    0.56 ms per token,  1795.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =     803.91 ms /   160 tokens (    5.02 ms per token,   199.03 tokens per second)\n",
            "llama_print_timings:        eval time =    3179.64 ms /    82 runs   (   38.78 ms per token,    25.79 tokens per second)\n",
            "llama_print_timings:       total time =    4070.29 ms /   242 tokens\n",
            " 49%|████▊     | 73/150 [45:47<1:29:07, 69.45s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      24.25 ms /    41 runs   (    0.59 ms per token,  1690.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2057.68 ms /   394 tokens (    5.22 ms per token,   191.48 tokens per second)\n",
            "llama_print_timings:        eval time =    1564.26 ms /    40 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    3674.06 ms /   434 tokens\n",
            " 49%|████▉     | 74/150 [46:05<1:08:20, 53.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 16 due to error: Requested tokens (4616) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      34.02 ms /    45 runs   (    0.76 ms per token,  1322.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =      40.39 ms /     4 tokens (   10.10 ms per token,    99.03 tokens per second)\n",
            "llama_print_timings:        eval time =    1730.02 ms /    44 runs   (   39.32 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    1835.37 ms /    48 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     145.97 ms /   256 runs   (    0.57 ms per token,  1753.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26099.93 ms /  3794 tokens (    6.88 ms per token,   145.36 tokens per second)\n",
            "llama_print_timings:        eval time =   15011.73 ms /   255 runs   (   58.87 ms per token,    16.99 tokens per second)\n",
            "llama_print_timings:       total time =   41594.92 ms /  4049 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      11.48 ms /    23 runs   (    0.50 ms per token,  2003.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11960.16 ms /  1998 tokens (    5.99 ms per token,   167.05 tokens per second)\n",
            "llama_print_timings:        eval time =     982.95 ms /    22 runs   (   44.68 ms per token,    22.38 tokens per second)\n",
            "llama_print_timings:       total time =   12998.92 ms /  2020 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     153.06 ms /   256 runs   (    0.60 ms per token,  1672.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11866.36 ms /  1996 tokens (    5.95 ms per token,   168.21 tokens per second)\n",
            "llama_print_timings:        eval time =   11429.05 ms /   255 runs   (   44.82 ms per token,    22.31 tokens per second)\n",
            "llama_print_timings:       total time =   23762.91 ms /  2251 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      43.77 ms /    66 runs   (    0.66 ms per token,  1507.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     691.17 ms /   132 tokens (    5.24 ms per token,   190.98 tokens per second)\n",
            "llama_print_timings:        eval time =    2531.91 ms /    65 runs   (   38.95 ms per token,    25.67 tokens per second)\n",
            "llama_print_timings:       total time =    3315.36 ms /   197 tokens\n",
            " 50%|█████     | 75/150 [47:36<1:21:29, 65.19s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      54.92 ms /    93 runs   (    0.59 ms per token,  1693.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2073.75 ms /   395 tokens (    5.25 ms per token,   190.48 tokens per second)\n",
            "llama_print_timings:        eval time =    3622.27 ms /    92 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    5818.35 ms /   487 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     132.81 ms /   239 runs   (    0.56 ms per token,  1799.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =   26615.19 ms /  3856 tokens (    6.90 ms per token,   144.88 tokens per second)\n",
            "llama_print_timings:        eval time =   14033.18 ms /   238 runs   (   58.96 ms per token,    16.96 tokens per second)\n",
            "llama_print_timings:       total time =   41095.22 ms /  4094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     145.11 ms /   256 runs   (    0.57 ms per token,  1764.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9800.08 ms /  1676 tokens (    5.85 ms per token,   171.02 tokens per second)\n",
            "llama_print_timings:        eval time =   11210.29 ms /   255 runs   (   43.96 ms per token,    22.75 tokens per second)\n",
            "llama_print_timings:       total time =   21442.87 ms /  1931 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     153.79 ms /   256 runs   (    0.60 ms per token,  1664.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10868.99 ms /  1850 tokens (    5.88 ms per token,   170.21 tokens per second)\n",
            "llama_print_timings:        eval time =   11341.92 ms /   255 runs   (   44.48 ms per token,    22.48 tokens per second)\n",
            "llama_print_timings:       total time =   22648.37 ms /  2105 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.64 ms per token,  1569.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1983.81 ms /   379 tokens (    5.23 ms per token,   191.05 tokens per second)\n",
            "llama_print_timings:        eval time =      39.43 ms /     1 runs   (   39.43 ms per token,    25.36 tokens per second)\n",
            "llama_print_timings:       total time =    2030.70 ms /   380 tokens\n",
            " 51%|█████     | 76/150 [49:24<1:36:02, 77.87s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      62.55 ms /    93 runs   (    0.67 ms per token,  1486.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2095.34 ms /   395 tokens (    5.30 ms per token,   188.51 tokens per second)\n",
            "llama_print_timings:        eval time =    3619.63 ms /    92 runs   (   39.34 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    5848.63 ms /   487 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     148.62 ms /   256 runs   (    0.58 ms per token,  1722.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =   22415.10 ms /  3363 tokens (    6.67 ms per token,   150.03 tokens per second)\n",
            "llama_print_timings:        eval time =   14299.68 ms /   255 runs   (   56.08 ms per token,    17.83 tokens per second)\n",
            "llama_print_timings:       total time =   37200.64 ms /  3618 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      92.93 ms /   164 runs   (    0.57 ms per token,  1764.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11342.99 ms /  1908 tokens (    5.94 ms per token,   168.21 tokens per second)\n",
            "llama_print_timings:        eval time =    7253.73 ms /   163 runs   (   44.50 ms per token,    22.47 tokens per second)\n",
            "llama_print_timings:       total time =   18865.76 ms /  2071 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      45.19 ms /    76 runs   (    0.59 ms per token,  1681.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12350.98 ms /  2061 tokens (    5.99 ms per token,   166.87 tokens per second)\n",
            "llama_print_timings:        eval time =    3365.63 ms /    75 runs   (   44.88 ms per token,    22.28 tokens per second)\n",
            "llama_print_timings:       total time =   15857.59 ms /  2136 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      46.48 ms /    65 runs   (    0.72 ms per token,  1398.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1404.29 ms /   270 tokens (    5.20 ms per token,   192.27 tokens per second)\n",
            "llama_print_timings:        eval time =    2519.78 ms /    64 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    4031.11 ms /   334 tokens\n",
            " 51%|█████▏    | 77/150 [50:54<1:39:09, 81.50s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      55.25 ms /    93 runs   (    0.59 ms per token,  1683.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2067.49 ms /   395 tokens (    5.23 ms per token,   191.05 tokens per second)\n",
            "llama_print_timings:        eval time =    3624.16 ms /    92 runs   (   39.39 ms per token,    25.39 tokens per second)\n",
            "llama_print_timings:       total time =    5810.02 ms /   487 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     145.54 ms /   256 runs   (    0.57 ms per token,  1758.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14735.56 ms /  2392 tokens (    6.16 ms per token,   162.33 tokens per second)\n",
            "llama_print_timings:        eval time =   12794.30 ms /   255 runs   (   50.17 ms per token,    19.93 tokens per second)\n",
            "llama_print_timings:       total time =   28002.81 ms /  2647 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      71.04 ms /   120 runs   (    0.59 ms per token,  1689.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5216.20 ms /   950 tokens (    5.49 ms per token,   182.13 tokens per second)\n",
            "llama_print_timings:        eval time =    4955.21 ms /   119 runs   (   41.64 ms per token,    24.02 tokens per second)\n",
            "llama_print_timings:       total time =   10349.81 ms /  1069 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.65 ms /    67 runs   (    0.59 ms per token,  1689.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5815.23 ms /  1046 tokens (    5.56 ms per token,   179.87 tokens per second)\n",
            "llama_print_timings:        eval time =    2750.23 ms /    66 runs   (   41.67 ms per token,    24.00 tokens per second)\n",
            "llama_print_timings:       total time =    8672.28 ms /  1112 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     104.40 ms /   157 runs   (    0.66 ms per token,  1503.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1178.98 ms /   226 tokens (    5.22 ms per token,   191.69 tokens per second)\n",
            "llama_print_timings:        eval time =    6128.41 ms /   156 runs   (   39.28 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =    7557.57 ms /   382 tokens\n",
            " 52%|█████▏    | 78/150 [52:01<1:32:45, 77.30s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      74.47 ms /    93 runs   (    0.80 ms per token,  1248.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2108.43 ms /   395 tokens (    5.34 ms per token,   187.34 tokens per second)\n",
            "llama_print_timings:        eval time =    3686.66 ms /    92 runs   (   40.07 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    5979.42 ms /   487 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     155.15 ms /   256 runs   (    0.61 ms per token,  1649.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13996.69 ms /  2291 tokens (    6.11 ms per token,   163.68 tokens per second)\n",
            "llama_print_timings:        eval time =   12243.02 ms /   255 runs   (   48.01 ms per token,    20.83 tokens per second)\n",
            "llama_print_timings:       total time =   26714.97 ms /  2546 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      30.76 ms /    44 runs   (    0.70 ms per token,  1430.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6087.38 ms /  1091 tokens (    5.58 ms per token,   179.22 tokens per second)\n",
            "llama_print_timings:        eval time =    1822.07 ms /    43 runs   (   42.37 ms per token,    23.60 tokens per second)\n",
            "llama_print_timings:       total time =    7994.23 ms /  1134 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1787.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6267.16 ms /  1120 tokens (    5.60 ms per token,   178.71 tokens per second)\n",
            "llama_print_timings:        eval time =      82.62 ms /     2 runs   (   41.31 ms per token,    24.21 tokens per second)\n",
            "llama_print_timings:       total time =    6372.63 ms /  1122 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1766.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1103.88 ms /   216 tokens (    5.11 ms per token,   195.67 tokens per second)\n",
            "llama_print_timings:        eval time =      77.76 ms /     2 runs   (   38.88 ms per token,    25.72 tokens per second)\n",
            "llama_print_timings:       total time =    1187.01 ms /   218 tokens\n",
            " 53%|█████▎    | 79/150 [52:54<1:22:55, 70.08s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      16.56 ms /    28 runs   (    0.59 ms per token,  1690.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2085.56 ms /   395 tokens (    5.28 ms per token,   189.40 tokens per second)\n",
            "llama_print_timings:        eval time =    1059.71 ms /    27 runs   (   39.25 ms per token,    25.48 tokens per second)\n",
            "llama_print_timings:       total time =    3185.30 ms /   422 tokens\n",
            " 53%|█████▎    | 80/150 [52:59<58:49, 50.42s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 79 due to error: Texts list is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      18.05 ms /    28 runs   (    0.64 ms per token,  1550.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1121.59 ms /    28 runs   (   40.06 ms per token,    24.96 tokens per second)\n",
            "llama_print_timings:       total time =    1163.27 ms /    28 tokens\n",
            " 54%|█████▍    | 81/150 [53:02<41:46, 36.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 80 due to error: Texts list is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      53.52 ms /    81 runs   (    0.66 ms per token,  1513.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =      48.91 ms /     5 tokens (    9.78 ms per token,   102.22 tokens per second)\n",
            "llama_print_timings:        eval time =    3173.32 ms /    80 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    3343.84 ms /    85 tokens\n",
            " 55%|█████▍    | 82/150 [53:18<34:10, 30.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 11 due to error: Requested tokens (4416) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      47.91 ms /    81 runs   (    0.59 ms per token,  1690.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3178.29 ms /    81 runs   (   39.24 ms per token,    25.49 tokens per second)\n",
            "llama_print_timings:       total time =    3279.06 ms /    81 tokens\n",
            " 55%|█████▌    | 83/150 [53:26<26:09, 23.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (6763) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      54.03 ms /    81 runs   (    0.67 ms per token,  1499.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3187.22 ms /    81 runs   (   39.35 ms per token,    25.41 tokens per second)\n",
            "llama_print_timings:       total time =    3305.08 ms /    81 tokens\n",
            " 56%|█████▌    | 84/150 [53:34<20:54, 19.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (5861) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      50.39 ms /    81 runs   (    0.62 ms per token,  1607.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3187.38 ms /    81 runs   (   39.35 ms per token,    25.41 tokens per second)\n",
            "llama_print_timings:       total time =    3299.29 ms /    81 tokens\n",
            " 57%|█████▋    | 85/150 [53:42<16:56, 15.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (6778) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      25.61 ms /    30 runs   (    0.85 ms per token,  1171.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =      40.08 ms /     5 tokens (    8.02 ms per token,   124.76 tokens per second)\n",
            "llama_print_timings:        eval time =    1156.69 ms /    29 runs   (   39.89 ms per token,    25.07 tokens per second)\n",
            "llama_print_timings:       total time =    1250.04 ms /    34 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     155.05 ms /   256 runs   (    0.61 ms per token,  1651.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24658.49 ms /  3588 tokens (    6.87 ms per token,   145.51 tokens per second)\n",
            "llama_print_timings:        eval time =   14492.83 ms /   255 runs   (   56.83 ms per token,    17.59 tokens per second)\n",
            "llama_print_timings:       total time =   39684.22 ms /  3843 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      32.59 ms /    56 runs   (    0.58 ms per token,  1718.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7445.58 ms /  1320 tokens (    5.64 ms per token,   177.29 tokens per second)\n",
            "llama_print_timings:        eval time =    2363.60 ms /    56 runs   (   42.21 ms per token,    23.69 tokens per second)\n",
            "llama_print_timings:       total time =    9899.70 ms /  1376 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.05 ms /    64 runs   (    0.69 ms per token,  1453.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7674.76 ms /  1365 tokens (    5.62 ms per token,   177.86 tokens per second)\n",
            "llama_print_timings:        eval time =    2688.75 ms /    63 runs   (   42.68 ms per token,    23.43 tokens per second)\n",
            "llama_print_timings:       total time =   10481.63 ms /  1428 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      37.89 ms /    59 runs   (    0.64 ms per token,  1557.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     687.64 ms /   135 tokens (    5.09 ms per token,   196.32 tokens per second)\n",
            "llama_print_timings:        eval time =    2243.92 ms /    58 runs   (   38.69 ms per token,    25.85 tokens per second)\n",
            "llama_print_timings:       total time =    3009.72 ms /   193 tokens\n",
            " 57%|█████▋    | 86/150 [54:59<36:15, 33.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      17.42 ms /    30 runs   (    0.58 ms per token,  1721.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2068.80 ms /   395 tokens (    5.24 ms per token,   190.93 tokens per second)\n",
            "llama_print_timings:        eval time =    1133.23 ms /    29 runs   (   39.08 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    3241.35 ms /   424 tokens\n",
            " 58%|█████▊    | 87/150 [55:11<28:47, 27.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8515) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      22.96 ms /    30 runs   (    0.77 ms per token,  1306.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1164.24 ms /    30 runs   (   38.81 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    1209.81 ms /    30 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     160.33 ms /   256 runs   (    0.63 ms per token,  1596.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18120.43 ms /  2836 tokens (    6.39 ms per token,   156.51 tokens per second)\n",
            "llama_print_timings:        eval time =   13822.09 ms /   255 runs   (   54.20 ms per token,    18.45 tokens per second)\n",
            "llama_print_timings:       total time =   32444.61 ms /  3091 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      61.22 ms /   108 runs   (    0.57 ms per token,  1764.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8183.43 ms /  1416 tokens (    5.78 ms per token,   173.03 tokens per second)\n",
            "llama_print_timings:        eval time =    4642.54 ms /   107 runs   (   43.39 ms per token,    23.05 tokens per second)\n",
            "llama_print_timings:       total time =   12995.94 ms /  1523 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.52 ms /    74 runs   (    0.60 ms per token,  1662.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8509.13 ms /  1477 tokens (    5.76 ms per token,   173.58 tokens per second)\n",
            "llama_print_timings:        eval time =    3138.44 ms /    73 runs   (   42.99 ms per token,    23.26 tokens per second)\n",
            "llama_print_timings:       total time =   11771.82 ms /  1550 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      26.05 ms /    45 runs   (    0.58 ms per token,  1727.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1217.28 ms /   235 tokens (    5.18 ms per token,   193.05 tokens per second)\n",
            "llama_print_timings:        eval time =    1720.17 ms /    44 runs   (   39.09 ms per token,    25.58 tokens per second)\n",
            "llama_print_timings:       total time =    2995.09 ms /   279 tokens\n",
            " 59%|█████▊    | 88/150 [56:19<40:43, 39.41s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      20.69 ms /    30 runs   (    0.69 ms per token,  1450.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2098.85 ms /   395 tokens (    5.31 ms per token,   188.20 tokens per second)\n",
            "llama_print_timings:        eval time =    1147.32 ms /    29 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    3302.26 ms /   424 tokens\n",
            " 59%|█████▉    | 89/150 [56:29<31:12, 30.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (5749) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      17.77 ms /    30 runs   (    0.59 ms per token,  1688.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1182.14 ms /    30 runs   (   39.40 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    1219.70 ms /    30 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     141.43 ms /   256 runs   (    0.55 ms per token,  1810.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13874.49 ms /  2274 tokens (    6.10 ms per token,   163.90 tokens per second)\n",
            "llama_print_timings:        eval time =   12209.16 ms /   255 runs   (   47.88 ms per token,    20.89 tokens per second)\n",
            "llama_print_timings:       total time =   26530.97 ms /  2529 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.18 ms /    74 runs   (    0.60 ms per token,  1674.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6085.19 ms /  1074 tokens (    5.67 ms per token,   176.49 tokens per second)\n",
            "llama_print_timings:        eval time =    3156.31 ms /    73 runs   (   43.24 ms per token,    23.13 tokens per second)\n",
            "llama_print_timings:       total time =    9362.81 ms /  1147 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.42 ms /     2 runs   (    0.71 ms per token,  1411.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6378.94 ms /  1134 tokens (    5.63 ms per token,   177.77 tokens per second)\n",
            "llama_print_timings:        eval time =      41.82 ms /     1 runs   (   41.82 ms per token,    23.91 tokens per second)\n",
            "llama_print_timings:       total time =    6445.08 ms /  1135 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      32.01 ms /    55 runs   (    0.58 ms per token,  1718.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1285.60 ms /   247 tokens (    5.20 ms per token,   192.13 tokens per second)\n",
            "llama_print_timings:        eval time =    2115.43 ms /    54 runs   (   39.17 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    3477.76 ms /   301 tokens\n",
            " 60%|██████    | 90/150 [57:20<36:54, 36.91s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      16.44 ms /    30 runs   (    0.55 ms per token,  1824.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2079.43 ms /   395 tokens (    5.26 ms per token,   189.96 tokens per second)\n",
            "llama_print_timings:        eval time =    1138.80 ms /    29 runs   (   39.27 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    3256.01 ms /   424 tokens\n",
            " 61%|██████    | 91/150 [57:31<28:32, 29.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (4512) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      20.41 ms /    30 runs   (    0.68 ms per token,  1469.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1187.04 ms /    30 runs   (   39.57 ms per token,    25.27 tokens per second)\n",
            "llama_print_timings:       total time =    1235.64 ms /    30 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     135.49 ms /   256 runs   (    0.53 ms per token,  1889.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24674.78 ms /  3656 tokens (    6.75 ms per token,   148.17 tokens per second)\n",
            "llama_print_timings:        eval time =   14828.03 ms /   255 runs   (   58.15 ms per token,    17.20 tokens per second)\n",
            "llama_print_timings:       total time =   39989.35 ms /  3911 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      93.38 ms /   151 runs   (    0.62 ms per token,  1617.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8072.45 ms /  1408 tokens (    5.73 ms per token,   174.42 tokens per second)\n",
            "llama_print_timings:        eval time =    6520.01 ms /   151 runs   (   43.18 ms per token,    23.16 tokens per second)\n",
            "llama_print_timings:       total time =   14843.48 ms /  1559 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.46 ms /     2 runs   (    0.73 ms per token,  1366.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8795.20 ms /  1528 tokens (    5.76 ms per token,   173.73 tokens per second)\n",
            "llama_print_timings:        eval time =      42.72 ms /     1 runs   (   42.72 ms per token,    23.41 tokens per second)\n",
            "llama_print_timings:       total time =    8869.23 ms /  1529 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      47.51 ms /    82 runs   (    0.58 ms per token,  1725.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1189.01 ms /   230 tokens (    5.17 ms per token,   193.44 tokens per second)\n",
            "llama_print_timings:        eval time =    3182.26 ms /    81 runs   (   39.29 ms per token,    25.45 tokens per second)\n",
            "llama_print_timings:       total time =    4474.94 ms /   311 tokens\n",
            " 61%|██████▏   | 92/150 [58:49<42:09, 43.61s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      17.13 ms /    30 runs   (    0.57 ms per token,  1751.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2067.15 ms /   395 tokens (    5.23 ms per token,   191.08 tokens per second)\n",
            "llama_print_timings:        eval time =    1135.61 ms /    29 runs   (   39.16 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    3242.93 ms /   424 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      69.50 ms /    95 runs   (    0.73 ms per token,  1366.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12753.83 ms /  2128 tokens (    5.99 ms per token,   166.85 tokens per second)\n",
            "llama_print_timings:        eval time =    4275.11 ms /    94 runs   (   45.48 ms per token,    21.99 tokens per second)\n",
            "llama_print_timings:       total time =   17222.41 ms /  2222 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      33.92 ms /    55 runs   (    0.62 ms per token,  1621.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5216.99 ms /   950 tokens (    5.49 ms per token,   182.10 tokens per second)\n",
            "llama_print_timings:        eval time =    2262.54 ms /    54 runs   (   41.90 ms per token,    23.87 tokens per second)\n",
            "llama_print_timings:       total time =    7565.13 ms /  1004 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      62.00 ms /   107 runs   (    0.58 ms per token,  1725.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5399.42 ms /   968 tokens (    5.58 ms per token,   179.28 tokens per second)\n",
            "llama_print_timings:        eval time =    4529.15 ms /   107 runs   (   42.33 ms per token,    23.62 tokens per second)\n",
            "llama_print_timings:       total time =   10085.33 ms /  1075 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      37.39 ms /    65 runs   (    0.58 ms per token,  1738.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1185.44 ms /   228 tokens (    5.20 ms per token,   192.33 tokens per second)\n",
            "llama_print_timings:        eval time =    2520.06 ms /    64 runs   (   39.38 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    3784.79 ms /   292 tokens\n",
            " 62%|██████▏   | 93/150 [59:37<42:45, 45.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      20.88 ms /    30 runs   (    0.70 ms per token,  1436.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2096.03 ms /   395 tokens (    5.31 ms per token,   188.45 tokens per second)\n",
            "llama_print_timings:        eval time =    1166.36 ms /    29 runs   (   40.22 ms per token,    24.86 tokens per second)\n",
            "llama_print_timings:       total time =    3317.42 ms /   424 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     143.80 ms /   256 runs   (    0.56 ms per token,  1780.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15635.83 ms /  2519 tokens (    6.21 ms per token,   161.10 tokens per second)\n",
            "llama_print_timings:        eval time =   13162.96 ms /   255 runs   (   51.62 ms per token,    19.37 tokens per second)\n",
            "llama_print_timings:       total time =   29256.67 ms /  2774 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.32 ms /    63 runs   (    0.66 ms per token,  1524.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6005.02 ms /  1080 tokens (    5.56 ms per token,   179.85 tokens per second)\n",
            "llama_print_timings:        eval time =    2663.23 ms /    63 runs   (   42.27 ms per token,    23.66 tokens per second)\n",
            "llama_print_timings:       total time =    8780.38 ms /  1143 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     163.41 ms /   256 runs   (    0.64 ms per token,  1566.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6218.58 ms /  1116 tokens (    5.57 ms per token,   179.46 tokens per second)\n",
            "llama_print_timings:        eval time =   10820.63 ms /   255 runs   (   42.43 ms per token,    23.57 tokens per second)\n",
            "llama_print_timings:       total time =   17486.59 ms /  1371 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      38.72 ms /    67 runs   (    0.58 ms per token,  1730.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =     855.62 ms /   168 tokens (    5.09 ms per token,   196.35 tokens per second)\n",
            "llama_print_timings:        eval time =    2623.49 ms /    67 runs   (   39.16 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    3557.07 ms /   235 tokens\n",
            " 63%|██████▎   | 94/150 [1:00:48<49:24, 52.94s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.95 ms /    59 runs   (    0.68 ms per token,  1476.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2076.46 ms /   395 tokens (    5.26 ms per token,   190.23 tokens per second)\n",
            "llama_print_timings:        eval time =    2300.89 ms /    58 runs   (   39.67 ms per token,    25.21 tokens per second)\n",
            "llama_print_timings:       total time =    4475.98 ms /   453 tokens\n",
            " 63%|██████▎   | 95/150 [1:01:11<40:13, 43.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 16 due to error: Requested tokens (4981) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      42.38 ms /    59 runs   (    0.72 ms per token,  1392.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2321.02 ms /    59 runs   (   39.34 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    2414.25 ms /    59 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     149.99 ms /   256 runs   (    0.59 ms per token,  1706.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21781.39 ms /  3285 tokens (    6.63 ms per token,   150.82 tokens per second)\n",
            "llama_print_timings:        eval time =   14407.72 ms /   255 runs   (   56.50 ms per token,    17.70 tokens per second)\n",
            "llama_print_timings:       total time =   36685.34 ms /  3540 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.70 ms /    63 runs   (    0.58 ms per token,  1716.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5872.94 ms /  1052 tokens (    5.58 ms per token,   179.13 tokens per second)\n",
            "llama_print_timings:        eval time =    2600.33 ms /    62 runs   (   41.94 ms per token,    23.84 tokens per second)\n",
            "llama_print_timings:       total time =    8586.93 ms /  1114 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.48 ms /     2 runs   (    0.74 ms per token,  1352.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6050.90 ms /  1085 tokens (    5.58 ms per token,   179.31 tokens per second)\n",
            "llama_print_timings:        eval time =      41.18 ms /     1 runs   (   41.18 ms per token,    24.29 tokens per second)\n",
            "llama_print_timings:       total time =    6112.85 ms /  1086 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      19.07 ms /    33 runs   (    0.58 ms per token,  1730.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =     952.54 ms /   184 tokens (    5.18 ms per token,   193.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1284.26 ms /    33 runs   (   38.92 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    2279.60 ms /   217 tokens\n",
            " 64%|██████▍   | 96/150 [1:02:20<46:11, 51.33s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      33.30 ms /    59 runs   (    0.56 ms per token,  1771.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2070.24 ms /   395 tokens (    5.24 ms per token,   190.80 tokens per second)\n",
            "llama_print_timings:        eval time =    2286.66 ms /    58 runs   (   39.43 ms per token,    25.36 tokens per second)\n",
            "llama_print_timings:       total time =    4430.07 ms /   453 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     136.83 ms /   256 runs   (    0.53 ms per token,  1870.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =   24701.30 ms /  3679 tokens (    6.71 ms per token,   148.94 tokens per second)\n",
            "llama_print_timings:        eval time =   14832.58 ms /   255 runs   (   58.17 ms per token,    17.19 tokens per second)\n",
            "llama_print_timings:       total time =   40027.31 ms /  3934 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     177.03 ms /   256 runs   (    0.69 ms per token,  1446.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8520.30 ms /  1462 tokens (    5.83 ms per token,   171.59 tokens per second)\n",
            "llama_print_timings:        eval time =   11205.02 ms /   255 runs   (   43.94 ms per token,    22.76 tokens per second)\n",
            "llama_print_timings:       total time =   20230.95 ms /  1717 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      70.89 ms /   119 runs   (    0.60 ms per token,  1678.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9810.24 ms /  1679 tokens (    5.84 ms per token,   171.15 tokens per second)\n",
            "llama_print_timings:        eval time =    5147.34 ms /   118 runs   (   43.62 ms per token,    22.92 tokens per second)\n",
            "llama_print_timings:       total time =   15167.93 ms /  1797 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      89.56 ms /   118 runs   (    0.76 ms per token,  1317.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1739.63 ms /   332 tokens (    5.24 ms per token,   190.85 tokens per second)\n",
            "llama_print_timings:        eval time =    4607.97 ms /   117 runs   (   39.38 ms per token,    25.39 tokens per second)\n",
            "llama_print_timings:       total time =    6560.80 ms /   449 tokens\n",
            " 65%|██████▍   | 97/150 [1:04:01<58:27, 66.17s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      33.40 ms /    59 runs   (    0.57 ms per token,  1766.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2064.55 ms /   395 tokens (    5.23 ms per token,   191.33 tokens per second)\n",
            "llama_print_timings:        eval time =    2284.73 ms /    58 runs   (   39.39 ms per token,    25.39 tokens per second)\n",
            "llama_print_timings:       total time =    4429.45 ms /   453 tokens\n",
            " 65%|██████▌   | 98/150 [1:04:16<44:03, 50.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 17 due to error: Requested tokens (7473) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      35.55 ms /    59 runs   (    0.60 ms per token,  1659.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2309.56 ms /    59 runs   (   39.15 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    2382.44 ms /    59 tokens\n",
            " 66%|██████▌   | 99/150 [1:04:25<32:45, 38.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6573) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.77 ms /    64 runs   (    0.57 ms per token,  1740.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =      40.90 ms /     5 tokens (    8.18 ms per token,   122.25 tokens per second)\n",
            "llama_print_timings:        eval time =    2471.01 ms /    63 runs   (   39.22 ms per token,    25.50 tokens per second)\n",
            "llama_print_timings:       total time =    2589.32 ms /    68 tokens\n",
            " 67%|██████▋   | 100/150 [1:04:38<25:43, 30.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 9 due to error: Requested tokens (6646) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.76 ms /    70 runs   (    0.60 ms per token,  1676.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =      43.20 ms /     6 tokens (    7.20 ms per token,   138.89 tokens per second)\n",
            "llama_print_timings:        eval time =    2707.95 ms /    69 runs   (   39.25 ms per token,    25.48 tokens per second)\n",
            "llama_print_timings:       total time =    2842.29 ms /    75 tokens\n",
            " 67%|██████▋   | 101/150 [1:04:53<21:12, 25.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 11 due to error: Requested tokens (4209) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      40.61 ms /    70 runs   (    0.58 ms per token,  1723.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2772.40 ms /    70 runs   (   39.61 ms per token,    25.25 tokens per second)\n",
            "llama_print_timings:       total time =    2858.96 ms /    70 tokens\n",
            " 68%|██████▊   | 102/150 [1:05:01<16:33, 20.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (7601) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.45 ms /    70 runs   (    0.59 ms per token,  1688.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2806.09 ms /    70 runs   (   40.09 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    2897.63 ms /    70 tokens\n",
            " 69%|██████▊   | 103/150 [1:05:08<13:00, 16.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (7584) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     104.40 ms /   154 runs   (    0.68 ms per token,  1475.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =      41.23 ms /     7 tokens (    5.89 ms per token,   169.78 tokens per second)\n",
            "llama_print_timings:        eval time =    6257.70 ms /   153 runs   (   40.90 ms per token,    24.45 tokens per second)\n",
            "llama_print_timings:       total time =    6564.03 ms /   160 tokens\n",
            " 69%|██████▉   | 104/150 [1:05:30<13:56, 18.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 15 due to error: Requested tokens (4323) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      86.92 ms /   154 runs   (    0.56 ms per token,  1771.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    6056.85 ms /   154 runs   (   39.33 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    6259.17 ms /   154 tokens\n",
            " 70%|███████   | 105/150 [1:05:49<13:52, 18.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 16 due to error: Requested tokens (4787) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     100.07 ms /   154 runs   (    0.65 ms per token,  1538.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    6116.58 ms /   154 runs   (   39.72 ms per token,    25.18 tokens per second)\n",
            "llama_print_timings:       total time =    6358.57 ms /   154 tokens\n",
            " 71%|███████   | 106/150 [1:06:10<13:59, 19.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 14 due to error: Requested tokens (4365) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      88.06 ms /   154 runs   (    0.57 ms per token,  1748.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    6155.90 ms /   154 runs   (   39.97 ms per token,    25.02 tokens per second)\n",
            "llama_print_timings:       total time =    6364.21 ms /   154 tokens\n",
            " 71%|███████▏  | 107/150 [1:06:29<13:40, 19.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 14 due to error: Requested tokens (4334) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     100.53 ms /   154 runs   (    0.65 ms per token,  1531.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    6194.47 ms /   154 runs   (   40.22 ms per token,    24.86 tokens per second)\n",
            "llama_print_timings:       total time =    6442.56 ms /   154 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     160.48 ms /   256 runs   (    0.63 ms per token,  1595.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18701.75 ms /  2904 tokens (    6.44 ms per token,   155.28 tokens per second)\n",
            "llama_print_timings:        eval time =   13689.22 ms /   256 runs   (   53.47 ms per token,    18.70 tokens per second)\n",
            "llama_print_timings:       total time =   32928.37 ms /  3160 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.51 ms /    62 runs   (    0.59 ms per token,  1697.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8387.06 ms /  1464 tokens (    5.73 ms per token,   174.55 tokens per second)\n",
            "llama_print_timings:        eval time =    2624.95 ms /    61 runs   (   43.03 ms per token,    23.24 tokens per second)\n",
            "llama_print_timings:       total time =   11128.62 ms /  1525 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      88.70 ms /   153 runs   (    0.58 ms per token,  1724.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8634.44 ms /  1500 tokens (    5.76 ms per token,   173.72 tokens per second)\n",
            "llama_print_timings:        eval time =    6569.10 ms /   152 runs   (   43.22 ms per token,    23.14 tokens per second)\n",
            "llama_print_timings:       total time =   15449.50 ms /  1652 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      72.20 ms /   105 runs   (    0.69 ms per token,  1454.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =     978.33 ms /   189 tokens (    5.18 ms per token,   193.19 tokens per second)\n",
            "llama_print_timings:        eval time =    4064.38 ms /   104 runs   (   39.08 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    5211.17 ms /   293 tokens\n",
            " 72%|███████▏  | 108/150 [1:07:47<25:48, 36.86s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      86.37 ms /   154 runs   (    0.56 ms per token,  1782.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2077.01 ms /   397 tokens (    5.23 ms per token,   191.14 tokens per second)\n",
            "llama_print_timings:        eval time =    6018.03 ms /   153 runs   (   39.33 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    8306.92 ms /   550 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     155.29 ms /   256 runs   (    0.61 ms per token,  1648.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =   18084.64 ms /  2830 tokens (    6.39 ms per token,   156.49 tokens per second)\n",
            "llama_print_timings:        eval time =   13804.84 ms /   255 runs   (   54.14 ms per token,    18.47 tokens per second)\n",
            "llama_print_timings:       total time =   32401.65 ms /  3085 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      27.23 ms /    46 runs   (    0.59 ms per token,  1689.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7050.29 ms /  1246 tokens (    5.66 ms per token,   176.73 tokens per second)\n",
            "llama_print_timings:        eval time =    1927.03 ms /    45 runs   (   42.82 ms per token,    23.35 tokens per second)\n",
            "llama_print_timings:       total time =    9057.11 ms /  1291 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      43.13 ms /    78 runs   (    0.55 ms per token,  1808.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7296.02 ms /  1282 tokens (    5.69 ms per token,   175.71 tokens per second)\n",
            "llama_print_timings:        eval time =    3272.76 ms /    77 runs   (   42.50 ms per token,    23.53 tokens per second)\n",
            "llama_print_timings:       total time =   10695.53 ms /  1359 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      37.50 ms /    65 runs   (    0.58 ms per token,  1733.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     645.84 ms /   126 tokens (    5.13 ms per token,   195.09 tokens per second)\n",
            "llama_print_timings:        eval time =    2490.38 ms /    64 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    3215.01 ms /   190 tokens\n",
            " 73%|███████▎  | 109/150 [1:09:07<34:02, 49.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      96.71 ms /   154 runs   (    0.63 ms per token,  1592.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2077.30 ms /   397 tokens (    5.23 ms per token,   191.11 tokens per second)\n",
            "llama_print_timings:        eval time =    6040.04 ms /   153 runs   (   39.48 ms per token,    25.33 tokens per second)\n",
            "llama_print_timings:       total time =    8357.81 ms /   550 tokens\n",
            " 73%|███████▎  | 110/150 [1:09:27<27:05, 40.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8886) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.66 ms /    64 runs   (    0.57 ms per token,  1745.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =      38.83 ms /     3 tokens (   12.94 ms per token,    77.26 tokens per second)\n",
            "llama_print_timings:        eval time =    2451.62 ms /    63 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    2574.46 ms /    66 tokens\n",
            " 74%|███████▍  | 111/150 [1:09:40<21:02, 32.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (9034) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      36.94 ms /    64 runs   (    0.58 ms per token,  1732.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2494.31 ms /    64 runs   (   38.97 ms per token,    25.66 tokens per second)\n",
            "llama_print_timings:       total time =    2576.76 ms /    64 tokens\n",
            " 75%|███████▍  | 112/150 [1:09:52<16:36, 26.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6766) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      56.81 ms /    99 runs   (    0.57 ms per token,  1742.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =      43.16 ms /     5 tokens (    8.63 ms per token,   115.84 tokens per second)\n",
            "llama_print_timings:        eval time =    3854.06 ms /    98 runs   (   39.33 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    4025.19 ms /   103 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     146.26 ms /   256 runs   (    0.57 ms per token,  1750.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25543.17 ms /  3698 tokens (    6.91 ms per token,   144.77 tokens per second)\n",
            "llama_print_timings:        eval time =   14724.42 ms /   255 runs   (   57.74 ms per token,    17.32 tokens per second)\n",
            "llama_print_timings:       total time =   40786.29 ms /  3953 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     143.48 ms /   256 runs   (    0.56 ms per token,  1784.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14073.33 ms /  2311 tokens (    6.09 ms per token,   164.21 tokens per second)\n",
            "llama_print_timings:        eval time =   12105.45 ms /   255 runs   (   47.47 ms per token,    21.06 tokens per second)\n",
            "llama_print_timings:       total time =   26628.11 ms /  2566 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     150.19 ms /   256 runs   (    0.59 ms per token,  1704.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15382.23 ms /  2488 tokens (    6.18 ms per token,   161.75 tokens per second)\n",
            "llama_print_timings:        eval time =   13124.11 ms /   256 runs   (   51.27 ms per token,    19.51 tokens per second)\n",
            "llama_print_timings:       total time =   29002.32 ms /  2744 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.22 ms /     2 runs   (    0.61 ms per token,  1643.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1945.32 ms /   363 tokens (    5.36 ms per token,   186.60 tokens per second)\n",
            "llama_print_timings:        eval time =      39.60 ms /     1 runs   (   39.60 ms per token,    25.25 tokens per second)\n",
            "llama_print_timings:       total time =    2001.04 ms /   364 tokens\n",
            " 75%|███████▌  | 113/150 [1:11:42<31:45, 51.49s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      56.72 ms /    99 runs   (    0.57 ms per token,  1745.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2084.08 ms /   395 tokens (    5.28 ms per token,   189.53 tokens per second)\n",
            "llama_print_timings:        eval time =    3875.22 ms /    98 runs   (   39.54 ms per token,    25.29 tokens per second)\n",
            "llama_print_timings:       total time =    6092.30 ms /   493 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     143.16 ms /   256 runs   (    0.56 ms per token,  1788.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25469.72 ms /  3752 tokens (    6.79 ms per token,   147.31 tokens per second)\n",
            "llama_print_timings:        eval time =   14747.82 ms /   256 runs   (   57.61 ms per token,    17.36 tokens per second)\n",
            "llama_print_timings:       total time =   40723.42 ms /  4008 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     141.92 ms /   256 runs   (    0.55 ms per token,  1803.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14287.93 ms /  2318 tokens (    6.16 ms per token,   162.23 tokens per second)\n",
            "llama_print_timings:        eval time =   12258.04 ms /   255 runs   (   48.07 ms per token,    20.80 tokens per second)\n",
            "llama_print_timings:       total time =   27019.59 ms /  2573 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     138.24 ms /   256 runs   (    0.54 ms per token,  1851.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15823.01 ms /  2544 tokens (    6.22 ms per token,   160.78 tokens per second)\n",
            "llama_print_timings:        eval time =   13114.24 ms /   255 runs   (   51.43 ms per token,    19.44 tokens per second)\n",
            "llama_print_timings:       total time =   29398.40 ms /  2799 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      20.92 ms /    35 runs   (    0.60 ms per token,  1673.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1900.65 ms /   363 tokens (    5.24 ms per token,   190.99 tokens per second)\n",
            "llama_print_timings:        eval time =    1330.59 ms /    34 runs   (   39.13 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    3283.13 ms /   397 tokens\n",
            " 76%|███████▌  | 114/150 [1:13:33<41:36, 69.36s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      54.05 ms /    79 runs   (    0.68 ms per token,  1461.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2077.72 ms /   394 tokens (    5.27 ms per token,   189.63 tokens per second)\n",
            "llama_print_timings:        eval time =    3096.64 ms /    78 runs   (   39.70 ms per token,    25.19 tokens per second)\n",
            "llama_print_timings:       total time =    5304.76 ms /   472 tokens\n",
            " 77%|███████▋  | 115/150 [1:13:52<31:41, 54.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 15 due to error: Requested tokens (4342) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.23 ms /    79 runs   (    0.56 ms per token,  1786.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3101.76 ms /    79 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    3197.88 ms /    79 tokens\n",
            " 77%|███████▋  | 116/150 [1:14:04<23:30, 41.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8963) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.47 ms /    79 runs   (    0.56 ms per token,  1776.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3098.48 ms /    79 runs   (   39.22 ms per token,    25.50 tokens per second)\n",
            "llama_print_timings:       total time =    3198.14 ms /    79 tokens\n",
            " 78%|███████▊  | 117/150 [1:14:11<17:04, 31.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (5726) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      46.59 ms /    79 runs   (    0.59 ms per token,  1695.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3112.65 ms /    79 runs   (   39.40 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    3223.96 ms /    79 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     161.56 ms /   256 runs   (    0.63 ms per token,  1584.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =   21852.62 ms /  3272 tokens (    6.68 ms per token,   149.73 tokens per second)\n",
            "llama_print_timings:        eval time =   14274.42 ms /   256 runs   (   55.76 ms per token,    17.93 tokens per second)\n",
            "llama_print_timings:       total time =   36678.57 ms /  3528 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      68.99 ms /   118 runs   (    0.58 ms per token,  1710.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10786.99 ms /  1832 tokens (    5.89 ms per token,   169.83 tokens per second)\n",
            "llama_print_timings:        eval time =    5212.63 ms /   118 runs   (   44.17 ms per token,    22.64 tokens per second)\n",
            "llama_print_timings:       total time =   16213.77 ms /  1950 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     163.75 ms /   256 runs   (    0.64 ms per token,  1563.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11359.72 ms /  1924 tokens (    5.90 ms per token,   169.37 tokens per second)\n",
            "llama_print_timings:        eval time =   11372.49 ms /   255 runs   (   44.60 ms per token,    22.42 tokens per second)\n",
            "llama_print_timings:       total time =   23193.69 ms /  2179 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      46.88 ms /    77 runs   (    0.61 ms per token,  1642.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1258.34 ms /   245 tokens (    5.14 ms per token,   194.70 tokens per second)\n",
            "llama_print_timings:        eval time =    2975.57 ms /    76 runs   (   39.15 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    4350.11 ms /   321 tokens\n",
            " 79%|███████▊  | 118/150 [1:15:41<25:59, 48.73s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.40 ms /    69 runs   (    0.60 ms per token,  1666.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2098.99 ms /   394 tokens (    5.33 ms per token,   187.71 tokens per second)\n",
            "llama_print_timings:        eval time =    2674.09 ms /    68 runs   (   39.32 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    4873.43 ms /   462 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     147.44 ms /   256 runs   (    0.58 ms per token,  1736.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15399.88 ms /  2476 tokens (    6.22 ms per token,   160.78 tokens per second)\n",
            "llama_print_timings:        eval time =   13242.94 ms /   255 runs   (   51.93 ms per token,    19.26 tokens per second)\n",
            "llama_print_timings:       total time =   29115.92 ms /  2731 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      71.64 ms /   119 runs   (    0.60 ms per token,  1661.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5791.27 ms /  1040 tokens (    5.57 ms per token,   179.58 tokens per second)\n",
            "llama_print_timings:        eval time =    4951.26 ms /   118 runs   (   41.96 ms per token,    23.83 tokens per second)\n",
            "llama_print_timings:       total time =   10921.55 ms /  1158 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     151.58 ms /   256 runs   (    0.59 ms per token,  1688.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6301.76 ms /  1128 tokens (    5.59 ms per token,   179.00 tokens per second)\n",
            "llama_print_timings:        eval time =   10785.09 ms /   256 runs   (   42.13 ms per token,    23.74 tokens per second)\n",
            "llama_print_timings:       total time =   17496.45 ms /  1384 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      81.94 ms /   127 runs   (    0.65 ms per token,  1550.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1155.96 ms /   224 tokens (    5.16 ms per token,   193.78 tokens per second)\n",
            "llama_print_timings:        eval time =    4974.19 ms /   127 runs   (   39.17 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    6324.88 ms /   351 tokens\n",
            " 79%|███████▉  | 119/150 [1:16:58<29:36, 57.30s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      42.43 ms /    71 runs   (    0.60 ms per token,  1673.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2071.11 ms /   395 tokens (    5.24 ms per token,   190.72 tokens per second)\n",
            "llama_print_timings:        eval time =    2752.92 ms /    70 runs   (   39.33 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    4924.63 ms /   465 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     147.13 ms /   256 runs   (    0.57 ms per token,  1740.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19300.04 ms /  3000 tokens (    6.43 ms per token,   155.44 tokens per second)\n",
            "llama_print_timings:        eval time =   13995.82 ms /   255 runs   (   54.89 ms per token,    18.22 tokens per second)\n",
            "llama_print_timings:       total time =   33799.32 ms /  3255 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      91.83 ms /   143 runs   (    0.64 ms per token,  1557.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9211.45 ms /  1580 tokens (    5.83 ms per token,   171.53 tokens per second)\n",
            "llama_print_timings:        eval time =    6235.41 ms /   142 runs   (   43.91 ms per token,    22.77 tokens per second)\n",
            "llama_print_timings:       total time =   15721.27 ms /  1722 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      96.12 ms /   143 runs   (    0.67 ms per token,  1487.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9733.59 ms /  1676 tokens (    5.81 ms per token,   172.19 tokens per second)\n",
            "llama_print_timings:        eval time =    6213.53 ms /   142 runs   (   43.76 ms per token,    22.85 tokens per second)\n",
            "llama_print_timings:       total time =   16226.72 ms /  1818 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      80.26 ms /   142 runs   (    0.57 ms per token,  1769.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1384.62 ms /   270 tokens (    5.13 ms per token,   195.00 tokens per second)\n",
            "llama_print_timings:        eval time =    5520.23 ms /   141 runs   (   39.15 ms per token,    25.54 tokens per second)\n",
            "llama_print_timings:       total time =    7095.50 ms /   411 tokens\n",
            " 80%|████████  | 120/150 [1:18:29<33:39, 67.32s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      51.32 ms /    71 runs   (    0.72 ms per token,  1383.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2074.90 ms /   395 tokens (    5.25 ms per token,   190.37 tokens per second)\n",
            "llama_print_timings:        eval time =    2771.57 ms /    70 runs   (   39.59 ms per token,    25.26 tokens per second)\n",
            "llama_print_timings:       total time =    4968.94 ms /   465 tokens\n",
            " 81%|████████  | 121/150 [1:18:45<25:10, 52.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8073) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      51.12 ms /    71 runs   (    0.72 ms per token,  1388.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2795.72 ms /    71 runs   (   39.38 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    2916.61 ms /    71 tokens\n",
            " 81%|████████▏ | 122/150 [1:19:01<19:10, 41.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6954) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      42.86 ms /    71 runs   (    0.60 ms per token,  1656.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2774.01 ms /    71 runs   (   39.07 ms per token,    25.59 tokens per second)\n",
            "llama_print_timings:       total time =    2868.05 ms /    71 tokens\n",
            " 82%|████████▏ | 123/150 [1:19:11<14:21, 31.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (7053) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      54.14 ms /    71 runs   (    0.76 ms per token,  1311.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2830.16 ms /    71 runs   (   39.86 ms per token,    25.09 tokens per second)\n",
            "llama_print_timings:       total time =    2952.24 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      95.51 ms /   167 runs   (    0.57 ms per token,  1748.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =   27488.51 ms /  3928 tokens (    7.00 ms per token,   142.90 tokens per second)\n",
            "llama_print_timings:        eval time =    9684.69 ms /   166 runs   (   58.34 ms per token,    17.14 tokens per second)\n",
            "llama_print_timings:       total time =   37532.83 ms /  4094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     156.65 ms /   256 runs   (    0.61 ms per token,  1634.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10250.85 ms /  1751 tokens (    5.85 ms per token,   170.82 tokens per second)\n",
            "llama_print_timings:        eval time =   11202.80 ms /   255 runs   (   43.93 ms per token,    22.76 tokens per second)\n",
            "llama_print_timings:       total time =   21950.73 ms /  2006 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     124.27 ms /   200 runs   (    0.62 ms per token,  1609.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11345.84 ms /  1922 tokens (    5.90 ms per token,   169.40 tokens per second)\n",
            "llama_print_timings:        eval time =    8877.87 ms /   199 runs   (   44.61 ms per token,    22.42 tokens per second)\n",
            "llama_print_timings:       total time =   20584.30 ms /  2121 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.53 ms per token,  1904.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1984.49 ms /   379 tokens (    5.24 ms per token,   190.98 tokens per second)\n",
            "llama_print_timings:        eval time =      38.81 ms /     1 runs   (   38.81 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    2033.22 ms /   380 tokens\n",
            " 83%|████████▎ | 124/150 [1:20:49<22:22, 51.64s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      65.17 ms /    71 runs   (    0.92 ms per token,  1089.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2074.22 ms /   395 tokens (    5.25 ms per token,   190.43 tokens per second)\n",
            "llama_print_timings:        eval time =    2792.50 ms /    70 runs   (   39.89 ms per token,    25.07 tokens per second)\n",
            "llama_print_timings:       total time =    5000.40 ms /   465 tokens\n",
            " 83%|████████▎ | 125/150 [1:21:03<16:51, 40.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 16 due to error: Requested tokens (4818) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      47.94 ms /    71 runs   (    0.68 ms per token,  1480.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2796.40 ms /    71 runs   (   39.39 ms per token,    25.39 tokens per second)\n",
            "llama_print_timings:       total time =    2904.92 ms /    71 tokens\n",
            " 84%|████████▍ | 126/150 [1:21:18<13:09, 32.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 16 due to error: Requested tokens (5250) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      50.48 ms /    71 runs   (    0.71 ms per token,  1406.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2814.71 ms /    71 runs   (   39.64 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    2932.76 ms /    71 tokens\n",
            " 85%|████████▍ | 127/150 [1:21:32<10:25, 27.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6895) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      48.81 ms /    71 runs   (    0.69 ms per token,  1454.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2820.79 ms /    71 runs   (   39.73 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =    2938.15 ms /    71 tokens\n",
            " 85%|████████▌ | 128/150 [1:21:43<08:11, 22.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6922) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.64 ms /    71 runs   (    0.59 ms per token,  1704.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2841.18 ms /    71 runs   (   40.02 ms per token,    24.99 tokens per second)\n",
            "llama_print_timings:       total time =    2934.44 ms /    71 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     154.47 ms /   256 runs   (    0.60 ms per token,  1657.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =   17401.01 ms /  2716 tokens (    6.41 ms per token,   156.08 tokens per second)\n",
            "llama_print_timings:        eval time =   13427.36 ms /   255 runs   (   52.66 ms per token,    18.99 tokens per second)\n",
            "llama_print_timings:       total time =   31350.67 ms /  2971 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.86 ms /    70 runs   (    0.57 ms per token,  1756.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7168.59 ms /  1269 tokens (    5.65 ms per token,   177.02 tokens per second)\n",
            "llama_print_timings:        eval time =    2926.14 ms /    69 runs   (   42.41 ms per token,    23.58 tokens per second)\n",
            "llama_print_timings:       total time =   10207.92 ms /  1338 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      58.18 ms /    85 runs   (    0.68 ms per token,  1461.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7403.18 ms /  1320 tokens (    5.61 ms per token,   178.30 tokens per second)\n",
            "llama_print_timings:        eval time =    3572.65 ms /    84 runs   (   42.53 ms per token,    23.51 tokens per second)\n",
            "llama_print_timings:       total time =   11125.30 ms /  1404 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      31.78 ms /    52 runs   (    0.61 ms per token,  1636.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     899.61 ms /   176 tokens (    5.11 ms per token,   195.64 tokens per second)\n",
            "llama_print_timings:        eval time =    1984.54 ms /    51 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    2954.33 ms /   227 tokens\n",
            " 86%|████████▌ | 129/150 [1:22:49<12:24, 35.47s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      41.66 ms /    71 runs   (    0.59 ms per token,  1704.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2061.35 ms /   395 tokens (    5.22 ms per token,   191.62 tokens per second)\n",
            "llama_print_timings:        eval time =    2751.53 ms /    70 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    4905.34 ms /   465 tokens\n",
            " 87%|████████▋ | 130/150 [1:23:00<09:22, 28.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6884) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      53.07 ms /    93 runs   (    0.57 ms per token,  1752.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =      39.56 ms /     4 tokens (    9.89 ms per token,   101.11 tokens per second)\n",
            "llama_print_timings:        eval time =    3611.74 ms /    92 runs   (   39.26 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    3762.21 ms /    96 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     146.06 ms /   256 runs   (    0.57 ms per token,  1752.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15521.32 ms /  2491 tokens (    6.23 ms per token,   160.49 tokens per second)\n",
            "llama_print_timings:        eval time =   13365.00 ms /   255 runs   (   52.41 ms per token,    19.08 tokens per second)\n",
            "llama_print_timings:       total time =   29395.00 ms /  2746 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      29.60 ms /    41 runs   (    0.72 ms per token,  1384.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5799.74 ms /  1034 tokens (    5.61 ms per token,   178.28 tokens per second)\n",
            "llama_print_timings:        eval time =    1693.87 ms /    40 runs   (   42.35 ms per token,    23.61 tokens per second)\n",
            "llama_print_timings:       total time =    7585.79 ms /  1074 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      38.69 ms /    61 runs   (    0.63 ms per token,  1576.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5940.67 ms /  1064 tokens (    5.58 ms per token,   179.10 tokens per second)\n",
            "llama_print_timings:        eval time =    2567.05 ms /    61 runs   (   42.08 ms per token,    23.76 tokens per second)\n",
            "llama_print_timings:       total time =    8606.01 ms /  1125 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      26.67 ms /    47 runs   (    0.57 ms per token,  1762.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     848.50 ms /   168 tokens (    5.05 ms per token,   198.00 tokens per second)\n",
            "llama_print_timings:        eval time =    1795.68 ms /    46 runs   (   39.04 ms per token,    25.62 tokens per second)\n",
            "llama_print_timings:       total time =    2710.12 ms /   214 tokens\n",
            " 87%|████████▋ | 131/150 [1:24:06<12:26, 39.28s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      57.76 ms /    93 runs   (    0.62 ms per token,  1610.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2099.81 ms /   394 tokens (    5.33 ms per token,   187.64 tokens per second)\n",
            "llama_print_timings:        eval time =    3618.72 ms /    92 runs   (   39.33 ms per token,    25.42 tokens per second)\n",
            "llama_print_timings:       total time =    5851.16 ms /   486 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     138.13 ms /   256 runs   (    0.54 ms per token,  1853.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =   23687.40 ms /  3549 tokens (    6.67 ms per token,   149.83 tokens per second)\n",
            "llama_print_timings:        eval time =   14765.96 ms /   255 runs   (   57.91 ms per token,    17.27 tokens per second)\n",
            "llama_print_timings:       total time =   38962.50 ms /  3804 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      38.57 ms /    67 runs   (    0.58 ms per token,  1737.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7485.31 ms /  1307 tokens (    5.73 ms per token,   174.61 tokens per second)\n",
            "llama_print_timings:        eval time =    2852.44 ms /    66 runs   (   43.22 ms per token,    23.14 tokens per second)\n",
            "llama_print_timings:       total time =   10451.35 ms /  1373 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      76.21 ms /   130 runs   (    0.59 ms per token,  1705.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7710.53 ms /  1352 tokens (    5.70 ms per token,   175.34 tokens per second)\n",
            "llama_print_timings:        eval time =    5547.97 ms /   130 runs   (   42.68 ms per token,    23.43 tokens per second)\n",
            "llama_print_timings:       total time =   13459.19 ms /  1482 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1734.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =     969.01 ms /   189 tokens (    5.13 ms per token,   195.04 tokens per second)\n",
            "llama_print_timings:        eval time =      38.39 ms /     1 runs   (   38.39 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    1012.73 ms /   190 tokens\n",
            " 88%|████████▊ | 132/150 [1:25:29<15:45, 52.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      59.79 ms /    93 runs   (    0.64 ms per token,  1555.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2085.66 ms /   394 tokens (    5.29 ms per token,   188.91 tokens per second)\n",
            "llama_print_timings:        eval time =    3620.35 ms /    92 runs   (   39.35 ms per token,    25.41 tokens per second)\n",
            "llama_print_timings:       total time =    5856.03 ms /   486 tokens\n",
            " 89%|████████▊ | 133/150 [1:25:49<12:06, 42.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 18 due to error: Requested tokens (4253) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      53.09 ms /    93 runs   (    0.57 ms per token,  1751.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3629.12 ms /    93 runs   (   39.02 ms per token,    25.63 tokens per second)\n",
            "llama_print_timings:       total time =    3744.67 ms /    93 tokens\n",
            " 89%|████████▉ | 134/150 [1:25:57<08:39, 32.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (4552) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      59.52 ms /    93 runs   (    0.64 ms per token,  1562.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    3645.05 ms /    93 runs   (   39.19 ms per token,    25.51 tokens per second)\n",
            "llama_print_timings:       total time =    3777.93 ms /    93 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      71.24 ms /   138 runs   (    0.52 ms per token,  1937.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =   27720.60 ms /  3957 tokens (    7.01 ms per token,   142.75 tokens per second)\n",
            "llama_print_timings:        eval time =    8102.91 ms /   137 runs   (   59.15 ms per token,    16.91 tokens per second)\n",
            "llama_print_timings:       total time =   36103.86 ms /  4094 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      26.65 ms /    48 runs   (    0.56 ms per token,  1800.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10124.59 ms /  1722 tokens (    5.88 ms per token,   170.08 tokens per second)\n",
            "llama_print_timings:        eval time =    2059.17 ms /    47 runs   (   43.81 ms per token,    22.82 tokens per second)\n",
            "llama_print_timings:       total time =   12270.76 ms /  1769 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      50.93 ms /    86 runs   (    0.59 ms per token,  1688.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10242.39 ms /  1748 tokens (    5.86 ms per token,   170.66 tokens per second)\n",
            "llama_print_timings:        eval time =    3709.54 ms /    85 runs   (   43.64 ms per token,    22.91 tokens per second)\n",
            "llama_print_timings:       total time =   14095.97 ms /  1833 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      44.84 ms /    73 runs   (    0.61 ms per token,  1627.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     638.81 ms /   123 tokens (    5.19 ms per token,   192.54 tokens per second)\n",
            "llama_print_timings:        eval time =    2782.83 ms /    72 runs   (   38.65 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    3514.56 ms /   195 tokens\n",
            " 90%|█████████ | 135/150 [1:27:20<11:50, 47.37s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      57.22 ms /    93 runs   (    0.62 ms per token,  1625.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2086.08 ms /   394 tokens (    5.29 ms per token,   188.87 tokens per second)\n",
            "llama_print_timings:        eval time =    3613.99 ms /    92 runs   (   39.28 ms per token,    25.46 tokens per second)\n",
            "llama_print_timings:       total time =    5839.30 ms /   486 tokens\n",
            " 91%|█████████ | 136/150 [1:27:34<08:43, 37.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 18 due to error: Requested tokens (4689) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      53.30 ms /    75 runs   (    0.71 ms per token,  1407.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =      41.51 ms /     6 tokens (    6.92 ms per token,   144.53 tokens per second)\n",
            "llama_print_timings:        eval time =    2913.25 ms /    74 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    3077.82 ms /    80 tokens\n",
            " 91%|█████████▏| 137/150 [1:27:47<06:31, 30.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8648) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      48.46 ms /    75 runs   (    0.65 ms per token,  1547.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2955.32 ms /    75 runs   (   39.40 ms per token,    25.38 tokens per second)\n",
            "llama_print_timings:       total time =    3072.58 ms /    75 tokens\n",
            " 92%|█████████▏| 138/150 [1:28:00<04:59, 24.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8410) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      46.36 ms /    75 runs   (    0.62 ms per token,  1617.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2969.75 ms /    75 runs   (   39.60 ms per token,    25.25 tokens per second)\n",
            "llama_print_timings:       total time =    3076.35 ms /    75 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     157.72 ms /   256 runs   (    0.62 ms per token,  1623.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =   19834.98 ms /  3018 tokens (    6.57 ms per token,   152.16 tokens per second)\n",
            "llama_print_timings:        eval time =   13961.43 ms /   255 runs   (   54.75 ms per token,    18.26 tokens per second)\n",
            "llama_print_timings:       total time =   34307.74 ms /  3273 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      60.58 ms /   112 runs   (    0.54 ms per token,  1848.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6856.92 ms /  1213 tokens (    5.65 ms per token,   176.90 tokens per second)\n",
            "llama_print_timings:        eval time =    4675.73 ms /   111 runs   (   42.12 ms per token,    23.74 tokens per second)\n",
            "llama_print_timings:       total time =   11704.16 ms /  1324 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.54 ms /    66 runs   (    0.60 ms per token,  1669.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7403.29 ms /  1309 tokens (    5.66 ms per token,   176.81 tokens per second)\n",
            "llama_print_timings:        eval time =    2745.11 ms /    65 runs   (   42.23 ms per token,    23.68 tokens per second)\n",
            "llama_print_timings:       total time =   10261.93 ms /  1374 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.20 ms /    68 runs   (    0.58 ms per token,  1734.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1135.49 ms /   221 tokens (    5.14 ms per token,   194.63 tokens per second)\n",
            "llama_print_timings:        eval time =    2603.55 ms /    67 runs   (   38.86 ms per token,    25.73 tokens per second)\n",
            "llama_print_timings:       total time =    3825.34 ms /   288 tokens\n",
            " 93%|█████████▎| 139/150 [1:29:11<07:07, 38.86s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      48.24 ms /    75 runs   (    0.64 ms per token,  1554.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2064.67 ms /   396 tokens (    5.21 ms per token,   191.80 tokens per second)\n",
            "llama_print_timings:        eval time =    2911.77 ms /    74 runs   (   39.35 ms per token,    25.41 tokens per second)\n",
            "llama_print_timings:       total time =    5096.68 ms /   470 tokens\n",
            " 93%|█████████▎| 140/150 [1:29:19<04:57, 29.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8101) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      42.07 ms /    75 runs   (    0.56 ms per token,  1782.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2966.40 ms /    75 runs   (   39.55 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    3057.80 ms /    75 tokens\n",
            " 94%|█████████▍| 141/150 [1:29:25<03:22, 22.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (5062) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      42.95 ms /    75 runs   (    0.57 ms per token,  1746.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2948.58 ms /    75 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    3040.11 ms /    75 tokens\n",
            " 95%|█████████▍| 142/150 [1:29:33<02:25, 18.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 10 due to error: Requested tokens (8039) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      28.30 ms /    43 runs   (    0.66 ms per token,  1519.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =      39.09 ms /     4 tokens (    9.77 ms per token,   102.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1640.39 ms /    42 runs   (   39.06 ms per token,    25.60 tokens per second)\n",
            "llama_print_timings:       total time =    1734.58 ms /    46 tokens\n",
            " 95%|█████████▌| 143/150 [1:29:44<01:51, 15.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 8 due to error: Requested tokens (4607) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      25.11 ms /    43 runs   (    0.58 ms per token,  1712.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1700.08 ms /    43 runs   (   39.54 ms per token,    25.29 tokens per second)\n",
            "llama_print_timings:       total time =    1749.24 ms /    43 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =     140.06 ms /   256 runs   (    0.55 ms per token,  1827.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25594.49 ms /  3720 tokens (    6.88 ms per token,   145.34 tokens per second)\n",
            "llama_print_timings:        eval time =   14862.69 ms /   256 runs   (   58.06 ms per token,    17.22 tokens per second)\n",
            "llama_print_timings:       total time =   40949.45 ms /  3976 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      27.21 ms /    48 runs   (    0.57 ms per token,  1763.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8397.69 ms /  1463 tokens (    5.74 ms per token,   174.21 tokens per second)\n",
            "llama_print_timings:        eval time =    2014.56 ms /    47 runs   (   42.86 ms per token,    23.33 tokens per second)\n",
            "llama_print_timings:       total time =   10504.64 ms /  1510 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      69.30 ms /    94 runs   (    0.74 ms per token,  1356.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8558.94 ms /  1490 tokens (    5.74 ms per token,   174.09 tokens per second)\n",
            "llama_print_timings:        eval time =    4015.20 ms /    93 runs   (   43.17 ms per token,    23.16 tokens per second)\n",
            "llama_print_timings:       total time =   12772.57 ms /  1583 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      40.61 ms /    70 runs   (    0.58 ms per token,  1723.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     642.64 ms /   127 tokens (    5.06 ms per token,   197.62 tokens per second)\n",
            "llama_print_timings:        eval time =    2680.32 ms /    69 runs   (   38.85 ms per token,    25.74 tokens per second)\n",
            "llama_print_timings:       total time =    3413.76 ms /   196 tokens\n",
            " 96%|█████████▌| 144/150 [1:31:03<03:28, 34.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      25.60 ms /    43 runs   (    0.60 ms per token,  1679.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2061.03 ms /   394 tokens (    5.23 ms per token,   191.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1653.35 ms /    42 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    3770.65 ms /   436 tokens\n",
            " 97%|█████████▋| 145/150 [1:31:19<02:26, 29.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 14 due to error: Requested tokens (4284) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      25.99 ms /    43 runs   (    0.60 ms per token,  1654.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1682.67 ms /    43 runs   (   39.13 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    1737.54 ms /    43 tokens\n",
            " 97%|█████████▋| 146/150 [1:31:31<01:35, 23.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 14 due to error: Requested tokens (4142) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      25.63 ms /    43 runs   (    0.60 ms per token,  1677.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1683.25 ms /    43 runs   (   39.15 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    1737.20 ms /    43 tokens\n",
            " 98%|█████████▊| 147/150 [1:31:41<00:59, 19.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 12 due to error: Requested tokens (6786) exceed context window of 4096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      35.46 ms /    62 runs   (    0.57 ms per token,  1748.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =      40.03 ms /     4 tokens (   10.01 ms per token,    99.93 tokens per second)\n",
            "llama_print_timings:        eval time =    2386.96 ms /    61 runs   (   39.13 ms per token,    25.56 tokens per second)\n",
            "llama_print_timings:       total time =    2499.70 ms /    65 tokens\n",
            " 99%|█████████▊| 148/150 [1:31:45<00:30, 15.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 147 due to error: Texts list is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.33 ms /    62 runs   (    0.63 ms per token,  1576.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2494.84 ms /    62 runs   (   40.24 ms per token,    24.85 tokens per second)\n",
            "llama_print_timings:       total time =    2582.23 ms /    62 tokens\n",
            " 99%|█████████▉| 149/150 [1:31:50<00:12, 12.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 148 due to error: Texts list is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     410.95 ms\n",
            "llama_print_timings:      sample time =      39.64 ms /    62 runs   (    0.64 ms per token,  1564.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    2497.62 ms /    62 runs   (   40.28 ms per token,    24.82 tokens per second)\n",
            "llama_print_timings:       total time =    2591.59 ms /    62 tokens\n",
            "100%|██████████| 150/150 [1:31:54<00:00, 36.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping question 149 due to error: Texts list is empty\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tFyFBfr-Vk91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}