{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f1c17fe69ee423daae2055d82120512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e8094047b4c4ca6af0fb6ba9aa130f5",
              "IPY_MODEL_6ad4d6bb354943dca9c626aae1ccc886",
              "IPY_MODEL_d3960bf2e86343abbeae5c1bda3dc4e4"
            ],
            "layout": "IPY_MODEL_7fece29ccfb44ee686384bc547be2a0c"
          }
        },
        "1e8094047b4c4ca6af0fb6ba9aa130f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b14f94226d847cca1c338df5facfc72",
            "placeholder": "​",
            "style": "IPY_MODEL_983a0015f2cc4ae5bcb24ee4493d150e",
            "value": "modules.json: 100%"
          }
        },
        "6ad4d6bb354943dca9c626aae1ccc886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24f791deb6334c8d8f07eb6bf66e6869",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b4aa4162a2b4c20a4ae9a2cd9b58158",
            "value": 349
          }
        },
        "d3960bf2e86343abbeae5c1bda3dc4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cecbd61bbce848e1a65c8349b516204e",
            "placeholder": "​",
            "style": "IPY_MODEL_f83b4db75e13418fad6929b3adeea250",
            "value": " 349/349 [00:00&lt;00:00, 25.8kB/s]"
          }
        },
        "7fece29ccfb44ee686384bc547be2a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b14f94226d847cca1c338df5facfc72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "983a0015f2cc4ae5bcb24ee4493d150e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24f791deb6334c8d8f07eb6bf66e6869": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b4aa4162a2b4c20a4ae9a2cd9b58158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cecbd61bbce848e1a65c8349b516204e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f83b4db75e13418fad6929b3adeea250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "537d6955966140c4b000165090f00092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_053653309a96447aade8e5e21a690a2a",
              "IPY_MODEL_bf6418e9e1f74fc9953c9df3e118cded",
              "IPY_MODEL_1c843aeda9b048a1be7a946ede94fb5e"
            ],
            "layout": "IPY_MODEL_6cc59ac62c77483db5c616ca77da7535"
          }
        },
        "053653309a96447aade8e5e21a690a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_805e4f2422234a9fadfad5da66465b59",
            "placeholder": "​",
            "style": "IPY_MODEL_c2a7f3b05f8a476db302c3b330004e8e",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "bf6418e9e1f74fc9953c9df3e118cded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_634e53a4d5bf462981eed6f1d0aad49d",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66302f63c4ad4b6aa99d125f5cf15b87",
            "value": 124
          }
        },
        "1c843aeda9b048a1be7a946ede94fb5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ab2a3f7a01c43a3a642b8ea314bb1d7",
            "placeholder": "​",
            "style": "IPY_MODEL_9a7d9214e5e443039f1faa56c2d6b957",
            "value": " 124/124 [00:00&lt;00:00, 10.4kB/s]"
          }
        },
        "6cc59ac62c77483db5c616ca77da7535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805e4f2422234a9fadfad5da66465b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2a7f3b05f8a476db302c3b330004e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "634e53a4d5bf462981eed6f1d0aad49d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66302f63c4ad4b6aa99d125f5cf15b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ab2a3f7a01c43a3a642b8ea314bb1d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a7d9214e5e443039f1faa56c2d6b957": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4ba54e8135e459fbf3f863c313ae6d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b56983006dc4bf3b1a89097c9c521e5",
              "IPY_MODEL_4dc112471f0f447a91eb8d07087f9d08",
              "IPY_MODEL_af2f8f501276400e83993880fedb143d"
            ],
            "layout": "IPY_MODEL_b5d76c14e80f4c129e42d605e95cf086"
          }
        },
        "3b56983006dc4bf3b1a89097c9c521e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c5f0d8f8293411f9fdd34ff13b7f0b3",
            "placeholder": "​",
            "style": "IPY_MODEL_3fa97e1559ba4294a850d83b265b11b9",
            "value": "README.md: 100%"
          }
        },
        "4dc112471f0f447a91eb8d07087f9d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55d31675e8544ecd85ee2a2f7b0853fa",
            "max": 94783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42ad3b107a6a4aa18f181d6f5e31a35a",
            "value": 94783
          }
        },
        "af2f8f501276400e83993880fedb143d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3415e0857ba417e800cf37ca67f326d",
            "placeholder": "​",
            "style": "IPY_MODEL_ce76f585ea6640c3ae5d23dbcfd4016d",
            "value": " 94.8k/94.8k [00:00&lt;00:00, 6.19MB/s]"
          }
        },
        "b5d76c14e80f4c129e42d605e95cf086": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c5f0d8f8293411f9fdd34ff13b7f0b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fa97e1559ba4294a850d83b265b11b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55d31675e8544ecd85ee2a2f7b0853fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ad3b107a6a4aa18f181d6f5e31a35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3415e0857ba417e800cf37ca67f326d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce76f585ea6640c3ae5d23dbcfd4016d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24c754d293214b458a72ca858e1a2610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67ed23bdd6454754a4861917ce0e014a",
              "IPY_MODEL_eace34bfb1674a0b962f6c6b587ee2ff",
              "IPY_MODEL_9f1385323bc949c9a148391239376bb9"
            ],
            "layout": "IPY_MODEL_6ef943ef950744bb84a88d30221d0459"
          }
        },
        "67ed23bdd6454754a4861917ce0e014a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ba75d6dbe74888a88fcf89f6e48331",
            "placeholder": "​",
            "style": "IPY_MODEL_d7498f63cbac4da79a193665213b5583",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "eace34bfb1674a0b962f6c6b587ee2ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2acd0d920884b7ba21caa51db3ad64c",
            "max": 52,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68859e2d4a3b40a5bf37fb6e0ae454f4",
            "value": 52
          }
        },
        "9f1385323bc949c9a148391239376bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e480bb9236d2496a8212c4739539d108",
            "placeholder": "​",
            "style": "IPY_MODEL_caa7ae8cf9324b5b92f3325845740d70",
            "value": " 52.0/52.0 [00:00&lt;00:00, 3.69kB/s]"
          }
        },
        "6ef943ef950744bb84a88d30221d0459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8ba75d6dbe74888a88fcf89f6e48331": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7498f63cbac4da79a193665213b5583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2acd0d920884b7ba21caa51db3ad64c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68859e2d4a3b40a5bf37fb6e0ae454f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e480bb9236d2496a8212c4739539d108": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caa7ae8cf9324b5b92f3325845740d70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46dffa7f6d1c4727b0172cff1f36a7bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a80a69261b34e689da8b5912a061312",
              "IPY_MODEL_ea22d1e9c8824160879fd9e9cc9f96d0",
              "IPY_MODEL_cc710dad1352410eb16b951bce3f1dce"
            ],
            "layout": "IPY_MODEL_0ddfe409101c4e76a13f5d209ae82db5"
          }
        },
        "4a80a69261b34e689da8b5912a061312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb57c01da294ad8af1829379d2f0c2a",
            "placeholder": "​",
            "style": "IPY_MODEL_76a1b3459fca44bbb95b009c716d7dbf",
            "value": "config.json: 100%"
          }
        },
        "ea22d1e9c8824160879fd9e9cc9f96d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6057c45c52b54ae9bb4bbad08c761954",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77f4aa460a1d4992801e472cf580aec9",
            "value": 743
          }
        },
        "cc710dad1352410eb16b951bce3f1dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6635a7d5d5a249d1b5ac3bc6be3a4d55",
            "placeholder": "​",
            "style": "IPY_MODEL_6b109e7baa4e4074a722308acc98c800",
            "value": " 743/743 [00:00&lt;00:00, 45.9kB/s]"
          }
        },
        "0ddfe409101c4e76a13f5d209ae82db5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb57c01da294ad8af1829379d2f0c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76a1b3459fca44bbb95b009c716d7dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6057c45c52b54ae9bb4bbad08c761954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f4aa460a1d4992801e472cf580aec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6635a7d5d5a249d1b5ac3bc6be3a4d55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b109e7baa4e4074a722308acc98c800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6894756d01d344a694c3e4fbecb65cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb88523ff6714baaa5a0686f2c8806a7",
              "IPY_MODEL_d28e4c2b524b4b5a979ab094de2cfbb8",
              "IPY_MODEL_52a2e5d895484281904df8fbfdb79ed9"
            ],
            "layout": "IPY_MODEL_d00f09a5553b4679b246bb10f2b46ecd"
          }
        },
        "cb88523ff6714baaa5a0686f2c8806a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff9b2b1e4be14f41996dab3ed34325d0",
            "placeholder": "​",
            "style": "IPY_MODEL_79b8471513fd40ee8d93853f35f30488",
            "value": "model.safetensors: 100%"
          }
        },
        "d28e4c2b524b4b5a979ab094de2cfbb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac2e74ee2e634dc89b6238e8b3de81d9",
            "max": 133466304,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_359f07ab1ede439c99a3b910f7d93086",
            "value": 133466304
          }
        },
        "52a2e5d895484281904df8fbfdb79ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_210e5b324ad44b35b2a8f174ceb61a03",
            "placeholder": "​",
            "style": "IPY_MODEL_9cad002c09dc4fc29796e0439d109a8b",
            "value": " 133M/133M [00:00&lt;00:00, 249MB/s]"
          }
        },
        "d00f09a5553b4679b246bb10f2b46ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9b2b1e4be14f41996dab3ed34325d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79b8471513fd40ee8d93853f35f30488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac2e74ee2e634dc89b6238e8b3de81d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "359f07ab1ede439c99a3b910f7d93086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "210e5b324ad44b35b2a8f174ceb61a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cad002c09dc4fc29796e0439d109a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3017d9ce9fee48f192106743711cfd6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f53daf867cb489fb66a3e01e64a2a53",
              "IPY_MODEL_132c94e72dd143a6b51c00658e0c9712",
              "IPY_MODEL_38f2c2db6e6249978a8823ea8f8810ed"
            ],
            "layout": "IPY_MODEL_a7b88a40d0e1449b8a34e314153e6cb0"
          }
        },
        "3f53daf867cb489fb66a3e01e64a2a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90b682641e5c4b6ba3e6301aa810db55",
            "placeholder": "​",
            "style": "IPY_MODEL_514cad5b2a5a4ed8b887879c7df2291c",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "132c94e72dd143a6b51c00658e0c9712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f41b1065c46e42a69369dd4150307fa6",
            "max": 366,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f22d9f77e4964533a42753779f7c63f4",
            "value": 366
          }
        },
        "38f2c2db6e6249978a8823ea8f8810ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db339921e0b940659e61c3aa1ee2297c",
            "placeholder": "​",
            "style": "IPY_MODEL_5b9b111eb5c645a2b5a120b961d27aad",
            "value": " 366/366 [00:00&lt;00:00, 16.8kB/s]"
          }
        },
        "a7b88a40d0e1449b8a34e314153e6cb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b682641e5c4b6ba3e6301aa810db55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "514cad5b2a5a4ed8b887879c7df2291c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f41b1065c46e42a69369dd4150307fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f22d9f77e4964533a42753779f7c63f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db339921e0b940659e61c3aa1ee2297c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9b111eb5c645a2b5a120b961d27aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "337f7f186ff342d299ede6eda7fe191c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_120f6cf2a86649f5b16cf106b3257f59",
              "IPY_MODEL_debc827bdd1047978c476351765f497e",
              "IPY_MODEL_51ae1e98fb7546afba9e265e6cb3ba7d"
            ],
            "layout": "IPY_MODEL_931d171e9d0e4e93bc028e3852bfce3d"
          }
        },
        "120f6cf2a86649f5b16cf106b3257f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_273fa20711444ddda92492719562056d",
            "placeholder": "​",
            "style": "IPY_MODEL_9c5b24d267dc446aafb9a25d95bfc3a0",
            "value": "vocab.txt: 100%"
          }
        },
        "debc827bdd1047978c476351765f497e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ef327b7bebc4090afedbd23de3cb354",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bacabcf5cc6415fa15627be441f2964",
            "value": 231508
          }
        },
        "51ae1e98fb7546afba9e265e6cb3ba7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed4e5ab076df4558b1e57b02e15ed65e",
            "placeholder": "​",
            "style": "IPY_MODEL_79062662cabb4931a3c39af500669f75",
            "value": " 232k/232k [00:00&lt;00:00, 12.3MB/s]"
          }
        },
        "931d171e9d0e4e93bc028e3852bfce3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273fa20711444ddda92492719562056d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c5b24d267dc446aafb9a25d95bfc3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ef327b7bebc4090afedbd23de3cb354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bacabcf5cc6415fa15627be441f2964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed4e5ab076df4558b1e57b02e15ed65e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79062662cabb4931a3c39af500669f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c325aa5774da4ae38dd6e9f08e7e615f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02e0f42a9b0e41f399a424921dae3456",
              "IPY_MODEL_279859eac6a946839a222c1de3b9798c",
              "IPY_MODEL_cbcb7189a4bc490dadc236c70bd1c236"
            ],
            "layout": "IPY_MODEL_028eed067a7548749a40ece83a94ee37"
          }
        },
        "02e0f42a9b0e41f399a424921dae3456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_622a074e736d4e848ae7b5d2b82fde33",
            "placeholder": "​",
            "style": "IPY_MODEL_263fc5e72d084e6c8d2fe8f3d93ea0ff",
            "value": "tokenizer.json: 100%"
          }
        },
        "279859eac6a946839a222c1de3b9798c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eba211414764924a59727fd61ad6ee8",
            "max": 711396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b6350b5643ac447998ef67cc04c0de23",
            "value": 711396
          }
        },
        "cbcb7189a4bc490dadc236c70bd1c236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b1395b2510a4d3f822750b810b85422",
            "placeholder": "​",
            "style": "IPY_MODEL_d62c2c0881e74300864f14981793a5b6",
            "value": " 711k/711k [00:00&lt;00:00, 21.9MB/s]"
          }
        },
        "028eed067a7548749a40ece83a94ee37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "622a074e736d4e848ae7b5d2b82fde33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "263fc5e72d084e6c8d2fe8f3d93ea0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eba211414764924a59727fd61ad6ee8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6350b5643ac447998ef67cc04c0de23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b1395b2510a4d3f822750b810b85422": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d62c2c0881e74300864f14981793a5b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f5a453434d04f37a9ef7ca218c9f414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e81e3ccba55420c89743a15c2e9e7bc",
              "IPY_MODEL_f279055373114a98a7d7e93a121b4a03",
              "IPY_MODEL_f12cbc56bb394d8cae5d5e65189b1ebf"
            ],
            "layout": "IPY_MODEL_40132398b10c4444840625cecf2454ba"
          }
        },
        "6e81e3ccba55420c89743a15c2e9e7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0063e7cb1a6e40ad9af404d9265c2ea3",
            "placeholder": "​",
            "style": "IPY_MODEL_1085aecf322e441eaea71c1fc14330ad",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f279055373114a98a7d7e93a121b4a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e50c437afaf84da29c9c6441ff29e2d0",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f64a3f53f5a4b0b86bfa617de64cc4d",
            "value": 125
          }
        },
        "f12cbc56bb394d8cae5d5e65189b1ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fd17e91187c4af0b0c65286103e9a45",
            "placeholder": "​",
            "style": "IPY_MODEL_e3f0cda6038541afbcc1f69ad7ade879",
            "value": " 125/125 [00:00&lt;00:00, 9.61kB/s]"
          }
        },
        "40132398b10c4444840625cecf2454ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0063e7cb1a6e40ad9af404d9265c2ea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1085aecf322e441eaea71c1fc14330ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e50c437afaf84da29c9c6441ff29e2d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f64a3f53f5a4b0b86bfa617de64cc4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fd17e91187c4af0b0c65286103e9a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f0cda6038541afbcc1f69ad7ade879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "030c05d39d6f4c7d8c0bd3d314c7f846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7be1bff160df4fc4ae14e4e408c6a3ea",
              "IPY_MODEL_d4afb332943c4441a7f7c46c91776481",
              "IPY_MODEL_b2ba496ae8d34a6298f8b8a34ca75a84"
            ],
            "layout": "IPY_MODEL_fa27ddccf32e436e908a9e253eb794e3"
          }
        },
        "7be1bff160df4fc4ae14e4e408c6a3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f62b1f1ddbd9489496f6b36f2fd7d56b",
            "placeholder": "​",
            "style": "IPY_MODEL_724b961b446b47b7857baa72e66025a7",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "d4afb332943c4441a7f7c46c91776481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_161d75fffcb0420582cdf2b280ce28fd",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71ede720aead46d7ac711449eb522290",
            "value": 190
          }
        },
        "b2ba496ae8d34a6298f8b8a34ca75a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3948edfdcfd548ccaf082849049b1161",
            "placeholder": "​",
            "style": "IPY_MODEL_dc5d3a78cbd14434b5cf906df7b976e0",
            "value": " 190/190 [00:00&lt;00:00, 7.99kB/s]"
          }
        },
        "fa27ddccf32e436e908a9e253eb794e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f62b1f1ddbd9489496f6b36f2fd7d56b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "724b961b446b47b7857baa72e66025a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "161d75fffcb0420582cdf2b280ce28fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71ede720aead46d7ac711449eb522290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3948edfdcfd548ccaf082849049b1161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc5d3a78cbd14434b5cf906df7b976e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b13293b73a4a4005b772282348a00650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c972d0d9bcb84a4eb4667bec5ac83bfa",
              "IPY_MODEL_91ea50d2f7034b0d99945cf23b6943aa",
              "IPY_MODEL_648c697326084f38b83b34d53cc85c7c"
            ],
            "layout": "IPY_MODEL_6952d90c9005469b9d6536f83639e048"
          }
        },
        "c972d0d9bcb84a4eb4667bec5ac83bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f306f479ae254082a294338f79c3b01f",
            "placeholder": "​",
            "style": "IPY_MODEL_e07f87b716604a21a4138087e70f8e30",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "91ea50d2f7034b0d99945cf23b6943aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ff657783d434fd9880c1294de15d99f",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89dd999192824135b685bf961ca483e0",
            "value": 443
          }
        },
        "648c697326084f38b83b34d53cc85c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_409200f7340f4e2f99cd732be52a2b87",
            "placeholder": "​",
            "style": "IPY_MODEL_bcc8a40d83c4436e9a006b91eb7aff5a",
            "value": " 443/443 [00:00&lt;00:00, 27.5kB/s]"
          }
        },
        "6952d90c9005469b9d6536f83639e048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f306f479ae254082a294338f79c3b01f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e07f87b716604a21a4138087e70f8e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ff657783d434fd9880c1294de15d99f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89dd999192824135b685bf961ca483e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "409200f7340f4e2f99cd732be52a2b87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcc8a40d83c4436e9a006b91eb7aff5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3012d404ac934ff98ea752396efe793d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eccbbeaf8a5c4a0fa86a4c2c2371b246",
              "IPY_MODEL_9ff67b7e240b4553a280639a3bdc69bd",
              "IPY_MODEL_82bac068abf14d6888dbd97ef11f6e8c"
            ],
            "layout": "IPY_MODEL_15c4e7d957fe4c6eaaffe0f094203a27"
          }
        },
        "eccbbeaf8a5c4a0fa86a4c2c2371b246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fbb357c25bd4e448bec970418b2dda4",
            "placeholder": "​",
            "style": "IPY_MODEL_fade3011358544278ae69699e73d1f80",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "9ff67b7e240b4553a280639a3bdc69bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25965da087d04023bd94eab088352b88",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ab2336c604f4dd789e2e5ef7205881f",
            "value": 5069051
          }
        },
        "82bac068abf14d6888dbd97ef11f6e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2887d75c5bfa4518bcc9247c544b58aa",
            "placeholder": "​",
            "style": "IPY_MODEL_eabc074fa29a4361baaea52d6a4fa423",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 94.8MB/s]"
          }
        },
        "15c4e7d957fe4c6eaaffe0f094203a27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fbb357c25bd4e448bec970418b2dda4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fade3011358544278ae69699e73d1f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25965da087d04023bd94eab088352b88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ab2336c604f4dd789e2e5ef7205881f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2887d75c5bfa4518bcc9247c544b58aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eabc074fa29a4361baaea52d6a4fa423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e7c95a2750246c58135cfecb4a9023f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3ac8d63a30b44edbd332c127c03dfc1",
              "IPY_MODEL_6f8c2753f34c4411b72d61fa13c3f202",
              "IPY_MODEL_fece0f29f4854d7b987cf08a2412359a"
            ],
            "layout": "IPY_MODEL_03e40930e48a4729b8fe0d18c760044c"
          }
        },
        "f3ac8d63a30b44edbd332c127c03dfc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20c0b79b63c64617ad249941c009c7f3",
            "placeholder": "​",
            "style": "IPY_MODEL_d5a90a07953b4112a17ac54f4e86837e",
            "value": "tokenizer.json: 100%"
          }
        },
        "6f8c2753f34c4411b72d61fa13c3f202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25d277a989fa4bbf816e49a71339e08a",
            "max": 17098107,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1df629db6d584dcebf7b977e61f39819",
            "value": 17098107
          }
        },
        "fece0f29f4854d7b987cf08a2412359a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_278c4567c1804134812443188593559b",
            "placeholder": "​",
            "style": "IPY_MODEL_0acefdd806764846a7f74b93776438d8",
            "value": " 17.1M/17.1M [00:00&lt;00:00, 202MB/s]"
          }
        },
        "03e40930e48a4729b8fe0d18c760044c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20c0b79b63c64617ad249941c009c7f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5a90a07953b4112a17ac54f4e86837e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25d277a989fa4bbf816e49a71339e08a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1df629db6d584dcebf7b977e61f39819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "278c4567c1804134812443188593559b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0acefdd806764846a7f74b93776438d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2418599bcd43412fbdf7bb7336fff780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c298742e3ec4a5a9a8f916eeb1a061b",
              "IPY_MODEL_e3a187a830d146f8bd45664c776b6886",
              "IPY_MODEL_0f150aafffff445b86153b5e06577a41"
            ],
            "layout": "IPY_MODEL_29f107e7311140a180cd333cbdbbe01a"
          }
        },
        "0c298742e3ec4a5a9a8f916eeb1a061b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeddd58108004dcc95dacf5d3e9ab2cb",
            "placeholder": "​",
            "style": "IPY_MODEL_b90633fc879d4d66b9780aae567b46b2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "e3a187a830d146f8bd45664c776b6886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e44b466ba2054f909bffba13a0fe6632",
            "max": 279,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ea332ca0a294509a31f750b338b14ce",
            "value": 279
          }
        },
        "0f150aafffff445b86153b5e06577a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_175cbd13fea44680b81ecd9b8130fc7f",
            "placeholder": "​",
            "style": "IPY_MODEL_53a4f1e9a7874e4c8c2d4f95373adc05",
            "value": " 279/279 [00:00&lt;00:00, 19.4kB/s]"
          }
        },
        "29f107e7311140a180cd333cbdbbe01a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeddd58108004dcc95dacf5d3e9ab2cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b90633fc879d4d66b9780aae567b46b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e44b466ba2054f909bffba13a0fe6632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ea332ca0a294509a31f750b338b14ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "175cbd13fea44680b81ecd9b8130fc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53a4f1e9a7874e4c8c2d4f95373adc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54651e986f734afe89457e75a1e3da92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85a323d164764a69ad916e63741b4d7a",
              "IPY_MODEL_5bcd156306a54c10882697c9a87670fe",
              "IPY_MODEL_6ebf8d1b969445b8a5a4741ec6105a56"
            ],
            "layout": "IPY_MODEL_f2085041b48c4bad8d4521443727d390"
          }
        },
        "85a323d164764a69ad916e63741b4d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_037ad8a010c947238cfca5465e582f8d",
            "placeholder": "​",
            "style": "IPY_MODEL_dccd9b34c02e4a6fa26b2388433729b9",
            "value": "config.json: 100%"
          }
        },
        "5bcd156306a54c10882697c9a87670fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8fd761b6d544750b867f66e44d64224",
            "max": 801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07197d46cb7948f788568a26bed44210",
            "value": 801
          }
        },
        "6ebf8d1b969445b8a5a4741ec6105a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ecd8d8ba7e14dbead2947019ddd7f52",
            "placeholder": "​",
            "style": "IPY_MODEL_174ba495bfbf43d98aff6e2cf3039aad",
            "value": " 801/801 [00:00&lt;00:00, 54.6kB/s]"
          }
        },
        "f2085041b48c4bad8d4521443727d390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "037ad8a010c947238cfca5465e582f8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dccd9b34c02e4a6fa26b2388433729b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8fd761b6d544750b867f66e44d64224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07197d46cb7948f788568a26bed44210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ecd8d8ba7e14dbead2947019ddd7f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "174ba495bfbf43d98aff6e2cf3039aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aaa7e48c23da4ce7ab238f531bd23b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a12649f9a41944ea90966cbd74341667",
              "IPY_MODEL_6c1fd01e5cfe47ff8a1564b0d016ebd3",
              "IPY_MODEL_cbd7f35402e54a859d2a1ad5c72b88e0"
            ],
            "layout": "IPY_MODEL_e129f30b78804454ad8d695b509aaa11"
          }
        },
        "a12649f9a41944ea90966cbd74341667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c86cbca4d5f24ee6ba67d25cd822cd28",
            "placeholder": "​",
            "style": "IPY_MODEL_8b9653d6f7164dd2a5a31b7b52da8e66",
            "value": "model.safetensors: 100%"
          }
        },
        "6c1fd01e5cfe47ff8a1564b0d016ebd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d0dccb2342b452faf3c715e95182ca5",
            "max": 2239618772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac336a195e70448988a586c56d743e3d",
            "value": 2239618772
          }
        },
        "cbd7f35402e54a859d2a1ad5c72b88e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187cb96eca62494d9f71d8abfd0b004c",
            "placeholder": "​",
            "style": "IPY_MODEL_cd3703afa3fb4082a065481902dc5005",
            "value": " 2.24G/2.24G [00:10&lt;00:00, 210MB/s]"
          }
        },
        "e129f30b78804454ad8d695b509aaa11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c86cbca4d5f24ee6ba67d25cd822cd28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b9653d6f7164dd2a5a31b7b52da8e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d0dccb2342b452faf3c715e95182ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac336a195e70448988a586c56d743e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "187cb96eca62494d9f71d8abfd0b004c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd3703afa3fb4082a065481902dc5005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/winterForestStump/thesis/blob/main/notebooks/rag_x_phi3_financebenchQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-nomic langchain langchain-core langchain-community chromadb --quiet\n",
        "%pip install sentence_transformers FlagEmbedding --quiet"
      ],
      "metadata": {
        "id": "FvVmzL2j9VE2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LlamaCpp x GPU usage\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUlkK-AQ9VE_",
        "outputId": "d4f1998e-5a0a-4630-eb36-fb27fc7c11db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.78.tar.gz (50.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp310-cp310-linux_x86_64.whl size=169130810 sha256=172100963ace4c30c4e8ac939346a319299a8f682788c03a1dbf91fc644f7cc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/c5/bd/3b1c20081bd71ce9d28b562573c97915c790bf1ef231879a61\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IfHIpuz9VFB",
        "outputId": "d2ca6876-1e44-4e9f-82f3-fc44240d7906"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "import chromadb\n",
        "from langchain.storage.file_system import LocalFileStore\n",
        "from langchain.storage._lc_store import create_kv_docstore\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "4chIcfH79VFC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-fp16.gguf --local-dir ./models --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFSdx4MY9VFD",
        "outputId": "0839cc99-8cf5-4879-ac58-4af47a8456ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/commands/download.py:132: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "Downloading 'Phi-3-mini-4k-instruct-fp16.gguf' to 'models/.huggingface/download/Phi-3-mini-4k-instruct-fp16.gguf.5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3.incomplete'\n",
            "Phi-3-mini-4k-instruct-fp16.gguf: 100% 7.64G/7.64G [01:00<00:00, 127MB/s]\n",
            "Download complete. Moving file to models/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "models/Phi-3-mini-4k-instruct-fp16.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEMP = 0\n",
        "N_CTX = 4096\n",
        "N_GPU_L = -1\n",
        "\n",
        "llm_phi3 = LlamaCpp(\n",
        "    model_path=\"/content/models/Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    temperature=TEMP,\n",
        "    n_ctx=N_CTX,\n",
        "    n_gpu_layers = N_GPU_L,\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UWQXosB9VFE",
        "outputId": "ce1746d4-a993-4242-f8ec-4aa7cde23264"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from /content/models/Phi-3-mini-4k-instruct-fp16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
            "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  130 tensors\n",
            "llm_load_vocab: special tokens cache size = 323\n",
            "llm_load_vocab: token to piece cache size = 0.1687 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = phi3\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32064\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 96\n",
            "llm_load_print_meta: n_embd_head_k    = 96\n",
            "llm_load_print_meta: n_embd_head_v    = 96\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
            "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 2\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = F16\n",
            "llm_load_print_meta: model params     = 3.82 B\n",
            "llm_load_print_meta: model size       = 7.12 GiB (16.00 BPW) \n",
            "llm_load_print_meta: general.name     = Phi3\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =   187.88 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  7100.64 MiB\n",
            "....................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 4096\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 32\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1536.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =    18.75 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.88 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1286\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.model': 'llama', 'general.file_type': '1'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
            "' + message['content'] + '<|end|>' + '\n",
            "' + '<|assistant|>' + '\n",
            "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
            "'}}{% endif %}{% endfor %}\n",
            "Using chat eos_token: <|endoftext|>\n",
            "Using chat bos_token: <s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = pd.read_json('https://raw.githubusercontent.com/patronus-ai/financebench/main/data/financebench_open_source.jsonl', lines=True)\n",
        "questions.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jA87mjiJ9VFE",
        "outputId": "8947dc0b-edcf-469d-a01f-1743a06805ef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 11 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   financebench_id       150 non-null    object\n",
            " 1   company               150 non-null    object\n",
            " 2   doc_name              150 non-null    object\n",
            " 3   question_type         150 non-null    object\n",
            " 4   question_reasoning    100 non-null    object\n",
            " 5   domain_question_num   50 non-null     object\n",
            " 6   question              150 non-null    object\n",
            " 7   answer                150 non-null    object\n",
            " 8   justification         100 non-null    object\n",
            " 9   dataset_subset_label  150 non-null    object\n",
            " 10  evidence              150 non-null    object\n",
            "dtypes: object(11)\n",
            "memory usage: 13.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"BAAI/bge-small-en-v1.5\"\n",
        "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
        "\n",
        "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'}, #gpu\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "8f1c17fe69ee423daae2055d82120512",
            "1e8094047b4c4ca6af0fb6ba9aa130f5",
            "6ad4d6bb354943dca9c626aae1ccc886",
            "d3960bf2e86343abbeae5c1bda3dc4e4",
            "7fece29ccfb44ee686384bc547be2a0c",
            "9b14f94226d847cca1c338df5facfc72",
            "983a0015f2cc4ae5bcb24ee4493d150e",
            "24f791deb6334c8d8f07eb6bf66e6869",
            "0b4aa4162a2b4c20a4ae9a2cd9b58158",
            "cecbd61bbce848e1a65c8349b516204e",
            "f83b4db75e13418fad6929b3adeea250",
            "537d6955966140c4b000165090f00092",
            "053653309a96447aade8e5e21a690a2a",
            "bf6418e9e1f74fc9953c9df3e118cded",
            "1c843aeda9b048a1be7a946ede94fb5e",
            "6cc59ac62c77483db5c616ca77da7535",
            "805e4f2422234a9fadfad5da66465b59",
            "c2a7f3b05f8a476db302c3b330004e8e",
            "634e53a4d5bf462981eed6f1d0aad49d",
            "66302f63c4ad4b6aa99d125f5cf15b87",
            "3ab2a3f7a01c43a3a642b8ea314bb1d7",
            "9a7d9214e5e443039f1faa56c2d6b957",
            "d4ba54e8135e459fbf3f863c313ae6d3",
            "3b56983006dc4bf3b1a89097c9c521e5",
            "4dc112471f0f447a91eb8d07087f9d08",
            "af2f8f501276400e83993880fedb143d",
            "b5d76c14e80f4c129e42d605e95cf086",
            "0c5f0d8f8293411f9fdd34ff13b7f0b3",
            "3fa97e1559ba4294a850d83b265b11b9",
            "55d31675e8544ecd85ee2a2f7b0853fa",
            "42ad3b107a6a4aa18f181d6f5e31a35a",
            "f3415e0857ba417e800cf37ca67f326d",
            "ce76f585ea6640c3ae5d23dbcfd4016d",
            "24c754d293214b458a72ca858e1a2610",
            "67ed23bdd6454754a4861917ce0e014a",
            "eace34bfb1674a0b962f6c6b587ee2ff",
            "9f1385323bc949c9a148391239376bb9",
            "6ef943ef950744bb84a88d30221d0459",
            "f8ba75d6dbe74888a88fcf89f6e48331",
            "d7498f63cbac4da79a193665213b5583",
            "c2acd0d920884b7ba21caa51db3ad64c",
            "68859e2d4a3b40a5bf37fb6e0ae454f4",
            "e480bb9236d2496a8212c4739539d108",
            "caa7ae8cf9324b5b92f3325845740d70",
            "46dffa7f6d1c4727b0172cff1f36a7bd",
            "4a80a69261b34e689da8b5912a061312",
            "ea22d1e9c8824160879fd9e9cc9f96d0",
            "cc710dad1352410eb16b951bce3f1dce",
            "0ddfe409101c4e76a13f5d209ae82db5",
            "8cb57c01da294ad8af1829379d2f0c2a",
            "76a1b3459fca44bbb95b009c716d7dbf",
            "6057c45c52b54ae9bb4bbad08c761954",
            "77f4aa460a1d4992801e472cf580aec9",
            "6635a7d5d5a249d1b5ac3bc6be3a4d55",
            "6b109e7baa4e4074a722308acc98c800",
            "6894756d01d344a694c3e4fbecb65cc6",
            "cb88523ff6714baaa5a0686f2c8806a7",
            "d28e4c2b524b4b5a979ab094de2cfbb8",
            "52a2e5d895484281904df8fbfdb79ed9",
            "d00f09a5553b4679b246bb10f2b46ecd",
            "ff9b2b1e4be14f41996dab3ed34325d0",
            "79b8471513fd40ee8d93853f35f30488",
            "ac2e74ee2e634dc89b6238e8b3de81d9",
            "359f07ab1ede439c99a3b910f7d93086",
            "210e5b324ad44b35b2a8f174ceb61a03",
            "9cad002c09dc4fc29796e0439d109a8b",
            "3017d9ce9fee48f192106743711cfd6f",
            "3f53daf867cb489fb66a3e01e64a2a53",
            "132c94e72dd143a6b51c00658e0c9712",
            "38f2c2db6e6249978a8823ea8f8810ed",
            "a7b88a40d0e1449b8a34e314153e6cb0",
            "90b682641e5c4b6ba3e6301aa810db55",
            "514cad5b2a5a4ed8b887879c7df2291c",
            "f41b1065c46e42a69369dd4150307fa6",
            "f22d9f77e4964533a42753779f7c63f4",
            "db339921e0b940659e61c3aa1ee2297c",
            "5b9b111eb5c645a2b5a120b961d27aad",
            "337f7f186ff342d299ede6eda7fe191c",
            "120f6cf2a86649f5b16cf106b3257f59",
            "debc827bdd1047978c476351765f497e",
            "51ae1e98fb7546afba9e265e6cb3ba7d",
            "931d171e9d0e4e93bc028e3852bfce3d",
            "273fa20711444ddda92492719562056d",
            "9c5b24d267dc446aafb9a25d95bfc3a0",
            "5ef327b7bebc4090afedbd23de3cb354",
            "9bacabcf5cc6415fa15627be441f2964",
            "ed4e5ab076df4558b1e57b02e15ed65e",
            "79062662cabb4931a3c39af500669f75",
            "c325aa5774da4ae38dd6e9f08e7e615f",
            "02e0f42a9b0e41f399a424921dae3456",
            "279859eac6a946839a222c1de3b9798c",
            "cbcb7189a4bc490dadc236c70bd1c236",
            "028eed067a7548749a40ece83a94ee37",
            "622a074e736d4e848ae7b5d2b82fde33",
            "263fc5e72d084e6c8d2fe8f3d93ea0ff",
            "9eba211414764924a59727fd61ad6ee8",
            "b6350b5643ac447998ef67cc04c0de23",
            "2b1395b2510a4d3f822750b810b85422",
            "d62c2c0881e74300864f14981793a5b6",
            "9f5a453434d04f37a9ef7ca218c9f414",
            "6e81e3ccba55420c89743a15c2e9e7bc",
            "f279055373114a98a7d7e93a121b4a03",
            "f12cbc56bb394d8cae5d5e65189b1ebf",
            "40132398b10c4444840625cecf2454ba",
            "0063e7cb1a6e40ad9af404d9265c2ea3",
            "1085aecf322e441eaea71c1fc14330ad",
            "e50c437afaf84da29c9c6441ff29e2d0",
            "6f64a3f53f5a4b0b86bfa617de64cc4d",
            "2fd17e91187c4af0b0c65286103e9a45",
            "e3f0cda6038541afbcc1f69ad7ade879",
            "030c05d39d6f4c7d8c0bd3d314c7f846",
            "7be1bff160df4fc4ae14e4e408c6a3ea",
            "d4afb332943c4441a7f7c46c91776481",
            "b2ba496ae8d34a6298f8b8a34ca75a84",
            "fa27ddccf32e436e908a9e253eb794e3",
            "f62b1f1ddbd9489496f6b36f2fd7d56b",
            "724b961b446b47b7857baa72e66025a7",
            "161d75fffcb0420582cdf2b280ce28fd",
            "71ede720aead46d7ac711449eb522290",
            "3948edfdcfd548ccaf082849049b1161",
            "dc5d3a78cbd14434b5cf906df7b976e0"
          ]
        },
        "id": "QCpr-Zyg9VFG",
        "outputId": "6649ad46-d148-4035-acc6-37304102ce3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f1c17fe69ee423daae2055d82120512"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "537d6955966140c4b000165090f00092"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4ba54e8135e459fbf3f863c313ae6d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24c754d293214b458a72ca858e1a2610"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46dffa7f6d1c4727b0172cff1f36a7bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6894756d01d344a694c3e4fbecb65cc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3017d9ce9fee48f192106743711cfd6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "337f7f186ff342d299ede6eda7fe191c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c325aa5774da4ae38dd6e9f08e7e615f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f5a453434d04f37a9ef7ca218c9f414"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "030c05d39d6f4c7d8c0bd3d314c7f846"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reranker = FlagReranker('BAAI/bge-reranker-large', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "b13293b73a4a4005b772282348a00650",
            "c972d0d9bcb84a4eb4667bec5ac83bfa",
            "91ea50d2f7034b0d99945cf23b6943aa",
            "648c697326084f38b83b34d53cc85c7c",
            "6952d90c9005469b9d6536f83639e048",
            "f306f479ae254082a294338f79c3b01f",
            "e07f87b716604a21a4138087e70f8e30",
            "7ff657783d434fd9880c1294de15d99f",
            "89dd999192824135b685bf961ca483e0",
            "409200f7340f4e2f99cd732be52a2b87",
            "bcc8a40d83c4436e9a006b91eb7aff5a",
            "3012d404ac934ff98ea752396efe793d",
            "eccbbeaf8a5c4a0fa86a4c2c2371b246",
            "9ff67b7e240b4553a280639a3bdc69bd",
            "82bac068abf14d6888dbd97ef11f6e8c",
            "15c4e7d957fe4c6eaaffe0f094203a27",
            "6fbb357c25bd4e448bec970418b2dda4",
            "fade3011358544278ae69699e73d1f80",
            "25965da087d04023bd94eab088352b88",
            "0ab2336c604f4dd789e2e5ef7205881f",
            "2887d75c5bfa4518bcc9247c544b58aa",
            "eabc074fa29a4361baaea52d6a4fa423",
            "9e7c95a2750246c58135cfecb4a9023f",
            "f3ac8d63a30b44edbd332c127c03dfc1",
            "6f8c2753f34c4411b72d61fa13c3f202",
            "fece0f29f4854d7b987cf08a2412359a",
            "03e40930e48a4729b8fe0d18c760044c",
            "20c0b79b63c64617ad249941c009c7f3",
            "d5a90a07953b4112a17ac54f4e86837e",
            "25d277a989fa4bbf816e49a71339e08a",
            "1df629db6d584dcebf7b977e61f39819",
            "278c4567c1804134812443188593559b",
            "0acefdd806764846a7f74b93776438d8",
            "2418599bcd43412fbdf7bb7336fff780",
            "0c298742e3ec4a5a9a8f916eeb1a061b",
            "e3a187a830d146f8bd45664c776b6886",
            "0f150aafffff445b86153b5e06577a41",
            "29f107e7311140a180cd333cbdbbe01a",
            "eeddd58108004dcc95dacf5d3e9ab2cb",
            "b90633fc879d4d66b9780aae567b46b2",
            "e44b466ba2054f909bffba13a0fe6632",
            "1ea332ca0a294509a31f750b338b14ce",
            "175cbd13fea44680b81ecd9b8130fc7f",
            "53a4f1e9a7874e4c8c2d4f95373adc05",
            "54651e986f734afe89457e75a1e3da92",
            "85a323d164764a69ad916e63741b4d7a",
            "5bcd156306a54c10882697c9a87670fe",
            "6ebf8d1b969445b8a5a4741ec6105a56",
            "f2085041b48c4bad8d4521443727d390",
            "037ad8a010c947238cfca5465e582f8d",
            "dccd9b34c02e4a6fa26b2388433729b9",
            "a8fd761b6d544750b867f66e44d64224",
            "07197d46cb7948f788568a26bed44210",
            "9ecd8d8ba7e14dbead2947019ddd7f52",
            "174ba495bfbf43d98aff6e2cf3039aad",
            "aaa7e48c23da4ce7ab238f531bd23b5e",
            "a12649f9a41944ea90966cbd74341667",
            "6c1fd01e5cfe47ff8a1564b0d016ebd3",
            "cbd7f35402e54a859d2a1ad5c72b88e0",
            "e129f30b78804454ad8d695b509aaa11",
            "c86cbca4d5f24ee6ba67d25cd822cd28",
            "8b9653d6f7164dd2a5a31b7b52da8e66",
            "7d0dccb2342b452faf3c715e95182ca5",
            "ac336a195e70448988a586c56d743e3d",
            "187cb96eca62494d9f71d8abfd0b004c",
            "cd3703afa3fb4082a065481902dc5005"
          ]
        },
        "id": "Dt9-e49TRv0n",
        "outputId": "2f30e7d6-8886-4ecf-c499-1dd0d333bb80"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b13293b73a4a4005b772282348a00650"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3012d404ac934ff98ea752396efe793d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e7c95a2750246c58135cfecb4a9023f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2418599bcd43412fbdf7bb7336fff780"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54651e986f734afe89457e75a1e3da92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaa7e48c23da4ce7ab238f531bd23b5e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "persistent_client = chromadb.PersistentClient('/content/drive/MyDrive/Thesis/chromadb')\n",
        "collection = persistent_client.get_or_create_collection(\"reports_l2\")\n",
        "fs = LocalFileStore('/content/drive/MyDrive/Thesis/reports_store_location')\n",
        "store = create_kv_docstore(fs)\n",
        "vectorstore = Chroma(client = persistent_client,\n",
        "                     collection_name=\"reports_l2\",\n",
        "                     embedding_function=bge_embeddings,\n",
        "                     persist_directory='/content/drive/MyDrive/Thesis/chromadb')\n",
        "vectorstore.persist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncJiWt939VFI",
        "outputId": "44e7c154-2e97-4647-c0ba-0e93be949e0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = vectorstore.get()['metadatas']\n",
        "metadata_list = []\n",
        "for i in range(len(metadata)):\n",
        "  metadata_list.append(metadata[i]['company'])\n",
        "metadata_list = list(set(metadata_list))"
      ],
      "metadata": {
        "id": "Xcq5E9K19VFI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct metadata Company name invoke:"
      ],
      "metadata": {
        "id": "fyOVy6MWuSDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Metadata company name\n",
        "prompt_metadata = PromptTemplate(\n",
        "template=\"\"\"\n",
        "  <|assistant|> You need to identify the correct spelling of companies from the metadata list which are mentioned in the users input.\n",
        "  Format your response as a JSON object with only a single key 'company', without any additional commentary or explanations. Do not try to answer the question itself.<|end|>\n",
        "  <|user|>Database metadata list with company names: {metadata_list}. Users input: {input}.<|end|>\n",
        "  <|assistant|>\n",
        "\"\"\",\n",
        "input_variables=[\"input\", \"metadata_list\"])\n",
        "\n",
        "retrieval_metadata = prompt_metadata | llm_phi3 | JsonOutputParser()"
      ],
      "metadata": {
        "id": "BomYo9X1tIgs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from json import JSONDecodeError\n",
        "from langchain_core.output_parsers.json import OutputParserException\n",
        "\n",
        "if 'correct_name' not in questions.columns:\n",
        "    questions['correct_name'] = None\n",
        "\n",
        "for i in tqdm(range(len(questions))):\n",
        "    try:\n",
        "        company = retrieval_metadata.invoke({\"input\": questions['question'][i], \"metadata_list\": metadata_list})\n",
        "        questions.at[i, 'correct_name'] = company\n",
        "    except JSONDecodeError as e:\n",
        "        questions.at[i, 'correct_name'] = \"JSONDecodeError\"\n",
        "    except OutputParserException as e:\n",
        "        questions.at[i, 'correct_name'] = \"OutputParserException\"\n",
        "    except Exception as e:\n",
        "        questions.at[i, 'correct_name'] = f\"Exception: {str(e)}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO1sHXOVtyqF",
        "outputId": "8f4a8969-7d75-4956-9412-efaa9d65e47c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/150 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      29.95 ms /    52 runs   (    0.58 ms per token,  1736.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     283.08 ms /    47 tokens (    6.02 ms per token,   166.03 tokens per second)\n",
            "llama_print_timings:        eval time =    1897.67 ms /    51 runs   (   37.21 ms per token,    26.87 tokens per second)\n",
            "llama_print_timings:       total time =    2254.82 ms /    98 tokens\n",
            "  1%|          | 1/150 [00:02<05:38,  2.27s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      54.22 ms /    91 runs   (    0.60 ms per token,  1678.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     284.42 ms /    56 tokens (    5.08 ms per token,   196.89 tokens per second)\n",
            "llama_print_timings:        eval time =    3360.72 ms /    90 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
            "llama_print_timings:       total time =    3777.88 ms /   146 tokens\n",
            "  1%|▏         | 2/150 [00:06<07:49,  3.17s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       4.75 ms /     8 runs   (    0.59 ms per token,  1685.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =     119.75 ms /    22 tokens (    5.44 ms per token,   183.72 tokens per second)\n",
            "llama_print_timings:        eval time =     260.00 ms /     7 runs   (   37.14 ms per token,    26.92 tokens per second)\n",
            "llama_print_timings:       total time =     391.02 ms /    29 tokens\n",
            "  2%|▏         | 3/150 [00:06<04:40,  1.91s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      34.10 ms /    52 runs   (    0.66 ms per token,  1524.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =     243.46 ms /    42 tokens (    5.80 ms per token,   172.51 tokens per second)\n",
            "llama_print_timings:        eval time =    1918.65 ms /    51 runs   (   37.62 ms per token,    26.58 tokens per second)\n",
            "llama_print_timings:       total time =    2250.46 ms /    93 tokens\n",
            "  3%|▎         | 4/150 [00:08<05:00,  2.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       5.50 ms /     8 runs   (    0.69 ms per token,  1455.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     164.23 ms /    32 tokens (    5.13 ms per token,   194.85 tokens per second)\n",
            "llama_print_timings:        eval time =     264.78 ms /     7 runs   (   37.83 ms per token,    26.44 tokens per second)\n",
            "llama_print_timings:       total time =     443.59 ms /    39 tokens\n",
            "  3%|▎         | 5/150 [00:09<03:35,  1.48s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      45.82 ms /    66 runs   (    0.69 ms per token,  1440.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     285.13 ms /    50 tokens (    5.70 ms per token,   175.36 tokens per second)\n",
            "llama_print_timings:        eval time =    2464.68 ms /    65 runs   (   37.92 ms per token,    26.37 tokens per second)\n",
            "llama_print_timings:       total time =    2868.13 ms /   115 tokens\n",
            "  4%|▍         | 6/150 [00:12<04:42,  1.96s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      29.45 ms /    51 runs   (    0.58 ms per token,  1731.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     201.06 ms /    37 tokens (    5.43 ms per token,   184.03 tokens per second)\n",
            "llama_print_timings:        eval time =    1869.60 ms /    50 runs   (   37.39 ms per token,    26.74 tokens per second)\n",
            "llama_print_timings:       total time =    2143.71 ms /    87 tokens\n",
            "  5%|▍         | 7/150 [00:14<04:50,  2.03s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      28.98 ms /    49 runs   (    0.59 ms per token,  1690.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =      80.16 ms /    16 tokens (    5.01 ms per token,   199.61 tokens per second)\n",
            "llama_print_timings:        eval time =    1787.45 ms /    48 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
            "llama_print_timings:       total time =    1936.55 ms /    64 tokens\n",
            "  5%|▌         | 8/150 [00:16<04:44,  2.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      57.15 ms /    95 runs   (    0.60 ms per token,  1662.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     487.86 ms /    94 tokens (    5.19 ms per token,   192.68 tokens per second)\n",
            "llama_print_timings:        eval time =    3530.41 ms /    94 runs   (   37.56 ms per token,    26.63 tokens per second)\n",
            "llama_print_timings:       total time =    4159.28 ms /   188 tokens\n",
            "  6%|▌         | 9/150 [00:20<06:19,  2.69s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      36.11 ms /    55 runs   (    0.66 ms per token,  1523.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     325.88 ms /    64 tokens (    5.09 ms per token,   196.39 tokens per second)\n",
            "llama_print_timings:        eval time =    2079.01 ms /    55 runs   (   37.80 ms per token,    26.45 tokens per second)\n",
            "llama_print_timings:       total time =    2493.08 ms /   119 tokens\n",
            "  7%|▋         | 10/150 [00:22<06:09,  2.64s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      17.98 ms /    26 runs   (    0.69 ms per token,  1445.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     456.37 ms /    88 tokens (    5.19 ms per token,   192.83 tokens per second)\n",
            "llama_print_timings:        eval time =     990.29 ms /    26 runs   (   38.09 ms per token,    26.25 tokens per second)\n",
            "llama_print_timings:       total time =    1495.31 ms /   114 tokens\n",
            "  7%|▋         | 11/150 [00:24<05:19,  2.30s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.96 ms /    75 runs   (    0.65 ms per token,  1531.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     328.12 ms /    61 tokens (    5.38 ms per token,   185.91 tokens per second)\n",
            "llama_print_timings:        eval time =    2790.68 ms /    74 runs   (   37.71 ms per token,    26.52 tokens per second)\n",
            "llama_print_timings:       total time =    3237.40 ms /   135 tokens\n",
            "  8%|▊         | 12/150 [00:27<05:57,  2.59s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      57.77 ms /   101 runs   (    0.57 ms per token,  1748.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     326.20 ms /    63 tokens (    5.18 ms per token,   193.14 tokens per second)\n",
            "llama_print_timings:        eval time =    3757.72 ms /   100 runs   (   37.58 ms per token,    26.61 tokens per second)\n",
            "llama_print_timings:       total time =    4230.14 ms /   163 tokens\n",
            "  9%|▊         | 13/150 [00:32<07:04,  3.10s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      66.42 ms /   108 runs   (    0.62 ms per token,  1625.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     241.22 ms /    42 tokens (    5.74 ms per token,   174.11 tokens per second)\n",
            "llama_print_timings:        eval time =    4021.77 ms /   107 runs   (   37.59 ms per token,    26.61 tokens per second)\n",
            "llama_print_timings:       total time =    4427.52 ms /   149 tokens\n",
            "  9%|▉         | 14/150 [00:36<07:57,  3.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      77.77 ms /   106 runs   (    0.73 ms per token,  1363.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =      82.08 ms /    16 tokens (    5.13 ms per token,   194.94 tokens per second)\n",
            "llama_print_timings:        eval time =    3993.68 ms /   105 runs   (   38.04 ms per token,    26.29 tokens per second)\n",
            "llama_print_timings:       total time =    4281.16 ms /   121 tokens\n",
            " 10%|█         | 15/150 [00:40<08:26,  3.75s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       9.76 ms /    17 runs   (    0.57 ms per token,  1742.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     243.17 ms /    45 tokens (    5.40 ms per token,   185.06 tokens per second)\n",
            "llama_print_timings:        eval time =     597.49 ms /    16 runs   (   37.34 ms per token,    26.78 tokens per second)\n",
            "llama_print_timings:       total time =     864.23 ms /    61 tokens\n",
            " 11%|█         | 16/150 [00:41<06:27,  2.89s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      78.64 ms /   134 runs   (    0.59 ms per token,  1703.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =     324.58 ms /    59 tokens (    5.50 ms per token,   181.77 tokens per second)\n",
            "llama_print_timings:        eval time =    5005.61 ms /   133 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
            "llama_print_timings:       total time =    5517.01 ms /   192 tokens\n",
            " 11%|█▏        | 17/150 [00:47<08:10,  3.69s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      13.92 ms /    26 runs   (    0.54 ms per token,  1868.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =     445.92 ms /    82 tokens (    5.44 ms per token,   183.89 tokens per second)\n",
            "llama_print_timings:        eval time =     941.04 ms /    25 runs   (   37.64 ms per token,    26.57 tokens per second)\n",
            "llama_print_timings:       total time =    1421.42 ms /   107 tokens\n",
            " 12%|█▏        | 18/150 [00:48<06:37,  3.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      65.63 ms /    94 runs   (    0.70 ms per token,  1432.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     611.36 ms /   114 tokens (    5.36 ms per token,   186.47 tokens per second)\n",
            "llama_print_timings:        eval time =    3546.36 ms /    93 runs   (   38.13 ms per token,    26.22 tokens per second)\n",
            "llama_print_timings:       total time =    4317.61 ms /   207 tokens\n",
            " 13%|█▎        | 19/150 [00:53<07:27,  3.41s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      44.57 ms /    71 runs   (    0.63 ms per token,  1592.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     329.81 ms /    58 tokens (    5.69 ms per token,   175.86 tokens per second)\n",
            "llama_print_timings:        eval time =    2663.15 ms /    70 runs   (   38.05 ms per token,    26.28 tokens per second)\n",
            "llama_print_timings:       total time =    3108.23 ms /   128 tokens\n",
            " 13%|█▎        | 20/150 [00:56<07:12,  3.33s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      17.98 ms /    34 runs   (    0.53 ms per token,  1891.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     240.88 ms /    43 tokens (    5.60 ms per token,   178.51 tokens per second)\n",
            "llama_print_timings:        eval time =    1240.46 ms /    33 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    1525.01 ms /    76 tokens\n",
            " 14%|█▍        | 21/150 [00:57<06:00,  2.80s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      54.00 ms /    91 runs   (    0.59 ms per token,  1685.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     244.60 ms /    48 tokens (    5.10 ms per token,   196.24 tokens per second)\n",
            "llama_print_timings:        eval time =    3427.13 ms /    91 runs   (   37.66 ms per token,    26.55 tokens per second)\n",
            "llama_print_timings:       total time =    3794.97 ms /   139 tokens\n",
            " 15%|█▍        | 22/150 [01:01<06:37,  3.10s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.02 ms /    84 runs   (    0.57 ms per token,  1749.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     161.31 ms /    30 tokens (    5.38 ms per token,   185.97 tokens per second)\n",
            "llama_print_timings:        eval time =    3128.76 ms /    83 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
            "llama_print_timings:       total time =    3403.67 ms /   113 tokens\n",
            " 15%|█▌        | 23/150 [01:04<06:46,  3.20s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     123.48 ms /   188 runs   (    0.66 ms per token,  1522.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =     289.99 ms /    56 tokens (    5.18 ms per token,   193.11 tokens per second)\n",
            "llama_print_timings:        eval time =    7142.24 ms /   187 runs   (   38.19 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    7743.27 ms /   243 tokens\n",
            " 16%|█▌        | 24/150 [01:12<09:36,  4.57s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      35.77 ms /    64 runs   (    0.56 ms per token,  1789.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     201.17 ms /    36 tokens (    5.59 ms per token,   178.96 tokens per second)\n",
            "llama_print_timings:        eval time =    2375.03 ms /    63 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
            "llama_print_timings:       total time =    2660.07 ms /    99 tokens\n",
            " 17%|█▋        | 25/150 [01:15<08:20,  4.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      11.38 ms /    20 runs   (    0.57 ms per token,  1758.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =      78.55 ms /    11 tokens (    7.14 ms per token,   140.03 tokens per second)\n",
            "llama_print_timings:        eval time =     712.79 ms /    19 runs   (   37.52 ms per token,    26.66 tokens per second)\n",
            "llama_print_timings:       total time =     817.92 ms /    30 tokens\n",
            " 17%|█▋        | 26/150 [01:16<06:19,  3.06s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      61.41 ms /    99 runs   (    0.62 ms per token,  1612.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     241.91 ms /    43 tokens (    5.63 ms per token,   177.75 tokens per second)\n",
            "llama_print_timings:        eval time =    3714.51 ms /    98 runs   (   37.90 ms per token,    26.38 tokens per second)\n",
            "llama_print_timings:       total time =    4100.25 ms /   141 tokens\n",
            " 18%|█▊        | 27/150 [01:20<06:55,  3.38s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      75.51 ms /   114 runs   (    0.66 ms per token,  1509.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     165.68 ms /    32 tokens (    5.18 ms per token,   193.15 tokens per second)\n",
            "llama_print_timings:        eval time =    4341.74 ms /   114 runs   (   38.09 ms per token,    26.26 tokens per second)\n",
            "llama_print_timings:       total time =    4689.82 ms /   146 tokens\n",
            " 19%|█▊        | 28/150 [01:25<07:41,  3.78s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      43.17 ms /    71 runs   (    0.61 ms per token,  1644.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =     160.34 ms /    27 tokens (    5.94 ms per token,   168.39 tokens per second)\n",
            "llama_print_timings:        eval time =    2640.26 ms /    70 runs   (   37.72 ms per token,    26.51 tokens per second)\n",
            "llama_print_timings:       total time =    2898.42 ms /    97 tokens\n",
            " 19%|█▉        | 29/150 [01:28<07:06,  3.52s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      61.40 ms /   107 runs   (    0.57 ms per token,  1742.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =     286.64 ms /    52 tokens (    5.51 ms per token,   181.41 tokens per second)\n",
            "llama_print_timings:        eval time =    4012.93 ms /   106 runs   (   37.86 ms per token,    26.41 tokens per second)\n",
            "llama_print_timings:       total time =    4448.65 ms /   158 tokens\n",
            " 20%|██        | 30/150 [01:32<07:36,  3.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.43 ms /    63 runs   (    0.66 ms per token,  1520.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =     450.87 ms /    86 tokens (    5.24 ms per token,   190.74 tokens per second)\n",
            "llama_print_timings:        eval time =    2373.35 ms /    62 runs   (   38.28 ms per token,    26.12 tokens per second)\n",
            "llama_print_timings:       total time =    2923.84 ms /   148 tokens\n",
            " 21%|██        | 31/150 [01:35<07:02,  3.55s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.58 ms /    44 runs   (    0.72 ms per token,  1393.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     246.50 ms /    44 tokens (    5.60 ms per token,   178.50 tokens per second)\n",
            "llama_print_timings:        eval time =    1645.14 ms /    43 runs   (   38.26 ms per token,    26.14 tokens per second)\n",
            "llama_print_timings:       total time =    1972.08 ms /    87 tokens\n",
            " 21%|██▏       | 32/150 [01:37<06:04,  3.09s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      59.23 ms /   101 runs   (    0.59 ms per token,  1705.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     122.04 ms /    21 tokens (    5.81 ms per token,   172.07 tokens per second)\n",
            "llama_print_timings:        eval time =    3772.81 ms /   100 runs   (   37.73 ms per token,    26.51 tokens per second)\n",
            "llama_print_timings:       total time =    4022.94 ms /   121 tokens\n",
            " 22%|██▏       | 33/150 [01:41<06:34,  3.37s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.88 ms /    69 runs   (    0.59 ms per token,  1687.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =      80.94 ms /    16 tokens (    5.06 ms per token,   197.68 tokens per second)\n",
            "llama_print_timings:        eval time =    2596.20 ms /    69 runs   (   37.63 ms per token,    26.58 tokens per second)\n",
            "llama_print_timings:       total time =    2760.35 ms /    85 tokens\n",
            " 23%|██▎       | 34/150 [01:44<06:10,  3.20s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.58 ms /    82 runs   (    0.58 ms per token,  1723.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =     203.31 ms /    38 tokens (    5.35 ms per token,   186.90 tokens per second)\n",
            "llama_print_timings:        eval time =    3059.72 ms /    81 runs   (   37.77 ms per token,    26.47 tokens per second)\n",
            "llama_print_timings:       total time =    3360.08 ms /   119 tokens\n",
            " 23%|██▎       | 35/150 [01:47<06:14,  3.26s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      69.67 ms /    98 runs   (    0.71 ms per token,  1406.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     204.00 ms /    36 tokens (    5.67 ms per token,   176.47 tokens per second)\n",
            "llama_print_timings:        eval time =    3737.57 ms /    97 runs   (   38.53 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    4102.92 ms /   133 tokens\n",
            " 24%|██▍       | 36/150 [01:51<06:41,  3.52s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      68.41 ms /   113 runs   (    0.61 ms per token,  1651.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     164.30 ms /    32 tokens (    5.13 ms per token,   194.77 tokens per second)\n",
            "llama_print_timings:        eval time =    4294.15 ms /   113 runs   (   38.00 ms per token,    26.31 tokens per second)\n",
            "llama_print_timings:       total time =    4621.88 ms /   145 tokens\n",
            " 25%|██▍       | 37/150 [01:56<07:16,  3.86s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      63.58 ms /   107 runs   (    0.59 ms per token,  1682.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =      79.75 ms /    14 tokens (    5.70 ms per token,   175.55 tokens per second)\n",
            "llama_print_timings:        eval time =    4029.61 ms /   106 runs   (   38.02 ms per token,    26.31 tokens per second)\n",
            "llama_print_timings:       total time =    4257.56 ms /   120 tokens\n",
            " 25%|██▌       | 38/150 [02:00<07:26,  3.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.66 ms /    79 runs   (    0.67 ms per token,  1500.25 tokens per second)\n",
            "llama_print_timings: prompt eval time =     163.90 ms /    32 tokens (    5.12 ms per token,   195.25 tokens per second)\n",
            "llama_print_timings:        eval time =    3012.19 ms /    78 runs   (   38.62 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    3304.40 ms /   110 tokens\n",
            " 26%|██▌       | 39/150 [02:04<07:00,  3.79s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      58.09 ms /    85 runs   (    0.68 ms per token,  1463.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     123.73 ms /    23 tokens (    5.38 ms per token,   185.89 tokens per second)\n",
            "llama_print_timings:        eval time =    3234.68 ms /    84 runs   (   38.51 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    3494.49 ms /   107 tokens\n",
            " 27%|██▋       | 40/150 [02:07<06:48,  3.71s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.46 ms /    76 runs   (    0.61 ms per token,  1635.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     243.28 ms /    42 tokens (    5.79 ms per token,   172.64 tokens per second)\n",
            "llama_print_timings:        eval time =    2879.34 ms /    75 runs   (   38.39 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    3226.84 ms /   117 tokens\n",
            " 27%|██▋       | 41/150 [02:10<06:29,  3.57s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      53.00 ms /    90 runs   (    0.59 ms per token,  1698.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     243.96 ms /    42 tokens (    5.81 ms per token,   172.16 tokens per second)\n",
            "llama_print_timings:        eval time =    3426.83 ms /    89 runs   (   38.50 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    3792.92 ms /   131 tokens\n",
            " 28%|██▊       | 42/150 [02:14<06:33,  3.65s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      71.74 ms /   114 runs   (    0.63 ms per token,  1589.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     163.05 ms /    28 tokens (    5.82 ms per token,   171.72 tokens per second)\n",
            "llama_print_timings:        eval time =    4370.79 ms /   113 runs   (   38.68 ms per token,    25.85 tokens per second)\n",
            "llama_print_timings:       total time =    4721.80 ms /   141 tokens\n",
            " 29%|██▊       | 43/150 [02:19<07:05,  3.98s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.49 ms /    62 runs   (    0.60 ms per token,  1653.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     125.09 ms /    24 tokens (    5.21 ms per token,   191.86 tokens per second)\n",
            "llama_print_timings:        eval time =    2350.80 ms /    61 runs   (   38.54 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    2564.94 ms /    85 tokens\n",
            " 29%|██▉       | 44/150 [02:22<06:17,  3.56s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      13.60 ms /    24 runs   (    0.57 ms per token,  1765.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =      85.52 ms /    16 tokens (    5.34 ms per token,   187.09 tokens per second)\n",
            "llama_print_timings:        eval time =     931.25 ms /    24 runs   (   38.80 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    1048.33 ms /    40 tokens\n",
            " 30%|███       | 45/150 [02:23<04:55,  2.82s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      49.75 ms /    84 runs   (    0.59 ms per token,  1688.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.38 ms /    51 tokens (    5.65 ms per token,   176.85 tokens per second)\n",
            "llama_print_timings:        eval time =    3213.65 ms /    83 runs   (   38.72 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    3616.73 ms /   134 tokens\n",
            " 31%|███       | 46/150 [02:26<05:18,  3.06s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      27.09 ms /    48 runs   (    0.56 ms per token,  1772.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     375.06 ms /    67 tokens (    5.60 ms per token,   178.64 tokens per second)\n",
            "llama_print_timings:        eval time =    1815.13 ms /    47 runs   (   38.62 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    2252.55 ms /   114 tokens\n",
            " 31%|███▏      | 47/150 [02:29<04:51,  2.83s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.47 ms /    71 runs   (    0.68 ms per token,  1464.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =     245.56 ms /    42 tokens (    5.85 ms per token,   171.03 tokens per second)\n",
            "llama_print_timings:        eval time =    2721.32 ms /    70 runs   (   38.88 ms per token,    25.72 tokens per second)\n",
            "llama_print_timings:       total time =    3081.48 ms /   112 tokens\n",
            " 32%|███▏      | 48/150 [02:32<04:57,  2.91s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      44.68 ms /    70 runs   (    0.64 ms per token,  1566.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =     334.22 ms /    60 tokens (    5.57 ms per token,   179.52 tokens per second)\n",
            "llama_print_timings:        eval time =    2684.89 ms /    69 runs   (   38.91 ms per token,    25.70 tokens per second)\n",
            "llama_print_timings:       total time =    3136.35 ms /   129 tokens\n",
            " 33%|███▎      | 49/150 [02:35<05:01,  2.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.27 ms /    84 runs   (    0.56 ms per token,  1776.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     247.22 ms /    44 tokens (    5.62 ms per token,   177.98 tokens per second)\n",
            "llama_print_timings:        eval time =    3192.90 ms /    83 runs   (   38.47 ms per token,    26.00 tokens per second)\n",
            "llama_print_timings:       total time =    3558.81 ms /   127 tokens\n",
            " 33%|███▎      | 50/150 [02:38<05:16,  3.17s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      28.78 ms /    57 runs   (    0.50 ms per token,  1980.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     286.83 ms /    52 tokens (    5.52 ms per token,   181.29 tokens per second)\n",
            "llama_print_timings:        eval time =    2150.05 ms /    56 runs   (   38.39 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    2512.07 ms /   108 tokens\n",
            " 34%|███▍      | 51/150 [02:41<04:54,  2.98s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      54.78 ms /    89 runs   (    0.62 ms per token,  1624.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     203.78 ms /    36 tokens (    5.66 ms per token,   176.66 tokens per second)\n",
            "llama_print_timings:        eval time =    3381.53 ms /    88 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    3721.94 ms /   124 tokens\n",
            " 35%|███▍      | 52/150 [02:45<05:14,  3.21s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      28.66 ms /    37 runs   (    0.77 ms per token,  1290.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =     208.26 ms /    39 tokens (    5.34 ms per token,   187.27 tokens per second)\n",
            "llama_print_timings:        eval time =    1391.97 ms /    36 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    1664.87 ms /    75 tokens\n",
            " 35%|███▌      | 53/150 [02:46<04:27,  2.76s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      74.74 ms /   119 runs   (    0.63 ms per token,  1592.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     171.38 ms /    32 tokens (    5.36 ms per token,   186.72 tokens per second)\n",
            "llama_print_timings:        eval time =    4585.65 ms /   119 runs   (   38.53 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    4946.45 ms /   151 tokens\n",
            " 36%|███▌      | 54/150 [02:51<05:28,  3.42s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      23.76 ms /    40 runs   (    0.59 ms per token,  1683.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =     163.51 ms /    29 tokens (    5.64 ms per token,   177.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1489.84 ms /    39 runs   (   38.20 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    1712.85 ms /    68 tokens\n",
            " 37%|███▋      | 55/150 [02:53<04:36,  2.92s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.44 ms /    86 runs   (    0.61 ms per token,  1640.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     204.23 ms /    34 tokens (    6.01 ms per token,   166.48 tokens per second)\n",
            "llama_print_timings:        eval time =    3249.95 ms /    85 runs   (   38.23 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    3585.22 ms /   119 tokens\n",
            " 37%|███▋      | 56/150 [02:57<04:53,  3.12s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      38.59 ms /    56 runs   (    0.69 ms per token,  1451.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.60 ms /    56 tokens (    5.15 ms per token,   194.04 tokens per second)\n",
            "llama_print_timings:        eval time =    2114.54 ms /    55 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2500.78 ms /   111 tokens\n",
            " 38%|███▊      | 57/150 [02:59<04:33,  2.95s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      88.16 ms /   123 runs   (    0.72 ms per token,  1395.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     425.81 ms /    74 tokens (    5.75 ms per token,   173.79 tokens per second)\n",
            "llama_print_timings:        eval time =    4700.61 ms /   122 runs   (   38.53 ms per token,    25.95 tokens per second)\n",
            "llama_print_timings:       total time =    5350.21 ms /   196 tokens\n",
            " 39%|███▊      | 58/150 [03:05<05:38,  3.68s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      51.34 ms /    84 runs   (    0.61 ms per token,  1636.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.34 ms /    54 tokens (    5.32 ms per token,   187.93 tokens per second)\n",
            "llama_print_timings:        eval time =    3168.31 ms /    83 runs   (   38.17 ms per token,    26.20 tokens per second)\n",
            "llama_print_timings:       total time =    3577.50 ms /   137 tokens\n",
            " 39%|███▉      | 59/150 [03:08<05:32,  3.66s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      80.89 ms /   135 runs   (    0.60 ms per token,  1669.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.33 ms /    53 tokens (    5.42 ms per token,   184.46 tokens per second)\n",
            "llama_print_timings:        eval time =    5136.63 ms /   134 runs   (   38.33 ms per token,    26.09 tokens per second)\n",
            "llama_print_timings:       total time =    5634.65 ms /   187 tokens\n",
            " 40%|████      | 60/150 [03:14<06:23,  4.26s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.81 ms /    62 runs   (    0.76 ms per token,  1324.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     168.98 ms /    32 tokens (    5.28 ms per token,   189.37 tokens per second)\n",
            "llama_print_timings:        eval time =    2397.69 ms /    62 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    2679.67 ms /    94 tokens\n",
            " 41%|████      | 61/150 [03:17<05:37,  3.79s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      53.52 ms /    92 runs   (    0.58 ms per token,  1719.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     124.22 ms /    23 tokens (    5.40 ms per token,   185.16 tokens per second)\n",
            "llama_print_timings:        eval time =    3474.82 ms /    91 runs   (   38.18 ms per token,    26.19 tokens per second)\n",
            "llama_print_timings:       total time =    3729.71 ms /   114 tokens\n",
            " 41%|████▏     | 62/150 [03:20<05:32,  3.78s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      56.90 ms /   102 runs   (    0.56 ms per token,  1792.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =     243.54 ms /    42 tokens (    5.80 ms per token,   172.46 tokens per second)\n",
            "llama_print_timings:        eval time =    3858.12 ms /   101 runs   (   38.20 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    4240.14 ms /   143 tokens\n",
            " 42%|████▏     | 63/150 [03:25<05:41,  3.93s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.69 ms /    78 runs   (    0.62 ms per token,  1602.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     119.72 ms /    19 tokens (    6.30 ms per token,   158.70 tokens per second)\n",
            "llama_print_timings:        eval time =    2944.13 ms /    77 runs   (   38.24 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    3177.77 ms /    96 tokens\n",
            " 43%|████▎     | 64/150 [03:28<05:19,  3.71s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.06 ms /    44 runs   (    0.71 ms per token,  1416.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =      82.50 ms /    14 tokens (    5.89 ms per token,   169.69 tokens per second)\n",
            "llama_print_timings:        eval time =    1660.36 ms /    43 runs   (   38.61 ms per token,    25.90 tokens per second)\n",
            "llama_print_timings:       total time =    1817.00 ms /    57 tokens\n",
            " 43%|████▎     | 65/150 [03:30<04:27,  3.15s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.16 ms /    68 runs   (    0.68 ms per token,  1472.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     122.35 ms /    20 tokens (    6.12 ms per token,   163.46 tokens per second)\n",
            "llama_print_timings:        eval time =    2571.38 ms /    67 runs   (   38.38 ms per token,    26.06 tokens per second)\n",
            "llama_print_timings:       total time =    2801.55 ms /    87 tokens\n",
            " 44%|████▍     | 66/150 [03:32<04:16,  3.06s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.43 ms /    73 runs   (    0.55 ms per token,  1805.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =     162.17 ms /    27 tokens (    6.01 ms per token,   166.49 tokens per second)\n",
            "llama_print_timings:        eval time =    2749.96 ms /    72 runs   (   38.19 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    3011.98 ms /    99 tokens\n",
            " 45%|████▍     | 67/150 [03:36<04:13,  3.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.99 ms /    79 runs   (    0.61 ms per token,  1646.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     496.77 ms /    91 tokens (    5.46 ms per token,   183.18 tokens per second)\n",
            "llama_print_timings:        eval time =    2998.67 ms /    78 runs   (   38.44 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    3606.10 ms /   169 tokens\n",
            " 45%|████▌     | 68/150 [03:39<04:24,  3.23s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     166.00 ms /   255 runs   (    0.65 ms per token,  1536.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =     205.50 ms /    37 tokens (    5.55 ms per token,   180.05 tokens per second)\n",
            "llama_print_timings:        eval time =    9844.74 ms /   254 runs   (   38.76 ms per token,    25.80 tokens per second)\n",
            "llama_print_timings:       total time =   10480.67 ms /   291 tokens\n",
            " 46%|████▌     | 69/150 [03:50<07:18,  5.41s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      73.55 ms /   122 runs   (    0.60 ms per token,  1658.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =     329.85 ms /    59 tokens (    5.59 ms per token,   178.87 tokens per second)\n",
            "llama_print_timings:        eval time =    4651.55 ms /   121 runs   (   38.44 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    5150.54 ms /   180 tokens\n",
            " 47%|████▋     | 70/150 [03:55<07:07,  5.34s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       2.51 ms /     5 runs   (    0.50 ms per token,  1994.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     582.47 ms /   109 tokens (    5.34 ms per token,   187.13 tokens per second)\n",
            "llama_print_timings:        eval time =     152.16 ms /     4 runs   (   38.04 ms per token,    26.29 tokens per second)\n",
            "llama_print_timings:       total time =     744.25 ms /   113 tokens\n",
            " 47%|████▋     | 71/150 [03:56<05:13,  3.97s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     160.91 ms /   256 runs   (    0.63 ms per token,  1590.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =     338.89 ms /    61 tokens (    5.56 ms per token,   180.00 tokens per second)\n",
            "llama_print_timings:        eval time =    9888.86 ms /   255 runs   (   38.78 ms per token,    25.79 tokens per second)\n",
            "llama_print_timings:       total time =   10664.55 ms /   316 tokens\n",
            " 48%|████▊     | 72/150 [04:06<07:46,  5.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      73.23 ms /   113 runs   (    0.65 ms per token,  1543.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =     161.44 ms /    28 tokens (    5.77 ms per token,   173.44 tokens per second)\n",
            "llama_print_timings:        eval time =    4292.17 ms /   112 runs   (   38.32 ms per token,    26.09 tokens per second)\n",
            "llama_print_timings:       total time =    4628.34 ms /   140 tokens\n",
            " 49%|████▊     | 73/150 [04:11<07:10,  5.59s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     122.91 ms /   164 runs   (    0.75 ms per token,  1334.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     209.08 ms /    40 tokens (    5.23 ms per token,   191.32 tokens per second)\n",
            "llama_print_timings:        eval time =    6358.28 ms /   164 runs   (   38.77 ms per token,    25.79 tokens per second)\n",
            "llama_print_timings:       total time =    6890.15 ms /   204 tokens\n",
            " 49%|████▉     | 74/150 [04:18<07:35,  5.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      30.32 ms /    42 runs   (    0.72 ms per token,  1385.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     208.32 ms /    36 tokens (    5.79 ms per token,   172.81 tokens per second)\n",
            "llama_print_timings:        eval time =    1578.97 ms /    41 runs   (   38.51 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    1858.55 ms /    77 tokens\n",
            " 50%|█████     | 75/150 [04:20<05:56,  4.76s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      39.84 ms /    66 runs   (    0.60 ms per token,  1656.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =     497.97 ms /    94 tokens (    5.30 ms per token,   188.77 tokens per second)\n",
            "llama_print_timings:        eval time =    2501.47 ms /    65 runs   (   38.48 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    3097.44 ms /   159 tokens\n",
            " 51%|█████     | 76/150 [04:23<05:15,  4.27s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.28 ms /    70 runs   (    0.66 ms per token,  1512.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     123.09 ms /    22 tokens (    5.60 ms per token,   178.73 tokens per second)\n",
            "llama_print_timings:        eval time =    2643.62 ms /    69 runs   (   38.31 ms per token,    26.10 tokens per second)\n",
            "llama_print_timings:       total time =    2879.19 ms /    91 tokens\n",
            " 51%|█████▏    | 77/150 [04:26<04:41,  3.86s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      58.57 ms /    75 runs   (    0.78 ms per token,  1280.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     207.62 ms /    35 tokens (    5.93 ms per token,   168.57 tokens per second)\n",
            "llama_print_timings:        eval time =    2864.43 ms /    74 runs   (   38.71 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    3211.81 ms /   109 tokens\n",
            " 52%|█████▏    | 78/150 [04:29<04:24,  3.67s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      35.76 ms /    58 runs   (    0.62 ms per token,  1622.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     121.37 ms /    21 tokens (    5.78 ms per token,   173.02 tokens per second)\n",
            "llama_print_timings:        eval time =    2172.91 ms /    57 runs   (   38.12 ms per token,    26.23 tokens per second)\n",
            "llama_print_timings:       total time =    2374.79 ms /    78 tokens\n",
            " 53%|█████▎    | 79/150 [04:31<03:53,  3.29s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.63 ms /    87 runs   (    0.60 ms per token,  1652.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =     123.99 ms /    24 tokens (    5.17 ms per token,   193.57 tokens per second)\n",
            "llama_print_timings:        eval time =    3324.15 ms /    87 runs   (   38.21 ms per token,    26.17 tokens per second)\n",
            "llama_print_timings:       total time =    3574.08 ms /   111 tokens\n",
            " 53%|█████▎    | 80/150 [04:35<03:56,  3.38s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      56.05 ms /    90 runs   (    0.62 ms per token,  1605.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     161.61 ms /    26 tokens (    6.22 ms per token,   160.88 tokens per second)\n",
            "llama_print_timings:        eval time =    3399.93 ms /    89 runs   (   38.20 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    3692.50 ms /   115 tokens\n",
            " 54%|█████▍    | 81/150 [04:39<04:00,  3.48s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      76.39 ms /   110 runs   (    0.69 ms per token,  1440.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1158.88 ms /   215 tokens (    5.39 ms per token,   185.52 tokens per second)\n",
            "llama_print_timings:        eval time =    4242.15 ms /   109 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    5599.34 ms /   324 tokens\n",
            " 55%|█████▍    | 82/150 [04:44<04:40,  4.13s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      21.87 ms /    36 runs   (    0.61 ms per token,  1645.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =     328.29 ms /    58 tokens (    5.66 ms per token,   176.67 tokens per second)\n",
            "llama_print_timings:        eval time =    1338.62 ms /    35 runs   (   38.25 ms per token,    26.15 tokens per second)\n",
            "llama_print_timings:       total time =    1721.41 ms /    93 tokens\n",
            " 55%|█████▌    | 83/150 [04:46<03:48,  3.41s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      22.50 ms /    36 runs   (    0.62 ms per token,  1600.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     330.69 ms /    60 tokens (    5.51 ms per token,   181.44 tokens per second)\n",
            "llama_print_timings:        eval time =    1340.89 ms /    35 runs   (   38.31 ms per token,    26.10 tokens per second)\n",
            "llama_print_timings:       total time =    1724.90 ms /    95 tokens\n",
            " 56%|█████▌    | 84/150 [04:48<03:12,  2.91s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.34 ms /    69 runs   (    0.58 ms per token,  1710.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     418.04 ms /    80 tokens (    5.23 ms per token,   191.37 tokens per second)\n",
            "llama_print_timings:        eval time =    2649.26 ms /    69 runs   (   38.40 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    3166.88 ms /   149 tokens\n",
            " 57%|█████▋    | 85/150 [04:51<03:14,  3.00s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.44 ms /    72 runs   (    0.64 ms per token,  1550.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =     123.56 ms /    23 tokens (    5.37 ms per token,   186.15 tokens per second)\n",
            "llama_print_timings:        eval time =    2729.04 ms /    71 runs   (   38.44 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    2965.58 ms /    94 tokens\n",
            " 57%|█████▋    | 86/150 [04:54<03:11,  3.00s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      65.48 ms /    88 runs   (    0.74 ms per token,  1343.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     248.24 ms /    42 tokens (    5.91 ms per token,   169.19 tokens per second)\n",
            "llama_print_timings:        eval time =    3363.93 ms /    87 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    3777.88 ms /   129 tokens\n",
            " 58%|█████▊    | 87/150 [04:58<03:24,  3.24s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      71.56 ms /   118 runs   (    0.61 ms per token,  1648.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     327.39 ms /    58 tokens (    5.64 ms per token,   177.16 tokens per second)\n",
            "llama_print_timings:        eval time =    4489.33 ms /   117 runs   (   38.37 ms per token,    26.06 tokens per second)\n",
            "llama_print_timings:       total time =    4991.26 ms /   175 tokens\n",
            " 59%|█████▊    | 88/150 [05:03<03:53,  3.77s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.06 ms /    67 runs   (    0.61 ms per token,  1631.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =     161.51 ms /    26 tokens (    6.21 ms per token,   160.98 tokens per second)\n",
            "llama_print_timings:        eval time =    2518.85 ms /    66 runs   (   38.16 ms per token,    26.20 tokens per second)\n",
            "llama_print_timings:       total time =    2779.65 ms /    92 tokens\n",
            " 59%|█████▉    | 89/150 [05:06<03:32,  3.48s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      43.82 ms /    71 runs   (    0.62 ms per token,  1620.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     123.95 ms /    24 tokens (    5.16 ms per token,   193.63 tokens per second)\n",
            "llama_print_timings:        eval time =    2721.58 ms /    71 runs   (   38.33 ms per token,    26.09 tokens per second)\n",
            "llama_print_timings:       total time =    2956.51 ms /    95 tokens\n",
            " 60%|██████    | 90/150 [05:09<03:20,  3.33s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.73 ms /    65 runs   (    0.75 ms per token,  1333.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     167.45 ms /    32 tokens (    5.23 ms per token,   191.11 tokens per second)\n",
            "llama_print_timings:        eval time =    2511.17 ms /    65 runs   (   38.63 ms per token,    25.88 tokens per second)\n",
            "llama_print_timings:       total time =    2800.81 ms /    97 tokens\n",
            " 61%|██████    | 91/150 [05:11<03:07,  3.18s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     101.96 ms /   179 runs   (    0.57 ms per token,  1755.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     247.96 ms /    43 tokens (    5.77 ms per token,   173.41 tokens per second)\n",
            "llama_print_timings:        eval time =    6833.55 ms /   178 runs   (   38.39 ms per token,    26.05 tokens per second)\n",
            "llama_print_timings:       total time =    7343.04 ms /   221 tokens\n",
            " 61%|██████▏   | 92/150 [05:19<04:17,  4.44s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      50.71 ms /    80 runs   (    0.63 ms per token,  1577.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     207.52 ms /    40 tokens (    5.19 ms per token,   192.76 tokens per second)\n",
            "llama_print_timings:        eval time =    3074.51 ms /    80 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    3408.78 ms /   120 tokens\n",
            " 62%|██████▏   | 93/150 [05:22<03:56,  4.14s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      68.20 ms /   104 runs   (    0.66 ms per token,  1524.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     208.29 ms /    39 tokens (    5.34 ms per token,   187.24 tokens per second)\n",
            "llama_print_timings:        eval time =    3975.36 ms /   103 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    4368.85 ms /   142 tokens\n",
            " 63%|██████▎   | 94/150 [05:27<03:56,  4.22s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      20.58 ms /    33 runs   (    0.62 ms per token,  1603.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     123.29 ms /    24 tokens (    5.14 ms per token,   194.66 tokens per second)\n",
            "llama_print_timings:        eval time =    1260.48 ms /    33 runs   (   38.20 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    1433.30 ms /    57 tokens\n",
            " 63%|██████▎   | 95/150 [05:28<03:06,  3.39s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.30 ms /    83 runs   (    0.57 ms per token,  1754.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =     206.42 ms /    40 tokens (    5.16 ms per token,   193.78 tokens per second)\n",
            "llama_print_timings:        eval time =    3180.81 ms /    83 runs   (   38.32 ms per token,    26.09 tokens per second)\n",
            "llama_print_timings:       total time =    3503.83 ms /   123 tokens\n",
            " 64%|██████▍   | 96/150 [05:32<03:05,  3.43s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.14 ms /    69 runs   (    0.60 ms per token,  1677.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     287.81 ms /    51 tokens (    5.64 ms per token,   177.20 tokens per second)\n",
            "llama_print_timings:        eval time =    2608.12 ms /    68 runs   (   38.35 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    2997.57 ms /   119 tokens\n",
            " 65%|██████▍   | 97/150 [05:35<02:55,  3.31s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      69.73 ms /   101 runs   (    0.69 ms per token,  1448.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     122.97 ms /    24 tokens (    5.12 ms per token,   195.16 tokens per second)\n",
            "llama_print_timings:        eval time =    3904.28 ms /   101 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    4207.87 ms /   125 tokens\n",
            " 65%|██████▌   | 98/150 [05:39<03:06,  3.59s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      59.37 ms /    93 runs   (    0.64 ms per token,  1566.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     214.64 ms /    37 tokens (    5.80 ms per token,   172.38 tokens per second)\n",
            "llama_print_timings:        eval time =    3536.14 ms /    92 runs   (   38.44 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    3897.68 ms /   129 tokens\n",
            " 66%|██████▌   | 99/150 [05:43<03:08,  3.69s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.82 ms /    80 runs   (    0.61 ms per token,  1638.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =     497.37 ms /    91 tokens (    5.47 ms per token,   182.96 tokens per second)\n",
            "llama_print_timings:        eval time =    3049.42 ms /    79 runs   (   38.60 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    3666.82 ms /   170 tokens\n",
            " 67%|██████▋   | 100/150 [05:47<03:04,  3.69s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      95.02 ms /   144 runs   (    0.66 ms per token,  1515.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =     584.13 ms /   106 tokens (    5.51 ms per token,   181.46 tokens per second)\n",
            "llama_print_timings:        eval time =    5548.58 ms /   143 runs   (   38.80 ms per token,    25.77 tokens per second)\n",
            "llama_print_timings:       total time =    6386.19 ms /   249 tokens\n",
            " 67%|██████▋   | 101/150 [05:53<03:41,  4.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      61.73 ms /   101 runs   (    0.61 ms per token,  1636.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     378.21 ms /    67 tokens (    5.64 ms per token,   177.15 tokens per second)\n",
            "llama_print_timings:        eval time =    3849.13 ms /   100 runs   (   38.49 ms per token,    25.98 tokens per second)\n",
            "llama_print_timings:       total time =    4384.32 ms /   167 tokens\n",
            " 68%|██████▊   | 102/150 [05:57<03:35,  4.48s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      68.25 ms /   115 runs   (    0.59 ms per token,  1685.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =     288.64 ms /    54 tokens (    5.35 ms per token,   187.08 tokens per second)\n",
            "llama_print_timings:        eval time =    4378.64 ms /   114 runs   (   38.41 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    4831.73 ms /   168 tokens\n",
            " 69%|██████▊   | 103/150 [06:02<03:35,  4.60s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.77 ms /    53 runs   (    0.60 ms per token,  1668.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     245.63 ms /    43 tokens (    5.71 ms per token,   175.06 tokens per second)\n",
            "llama_print_timings:        eval time =    1989.17 ms /    52 runs   (   38.25 ms per token,    26.14 tokens per second)\n",
            "llama_print_timings:       total time =    2310.86 ms /    95 tokens\n",
            " 69%|██████▉   | 104/150 [06:05<03:00,  3.92s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      68.15 ms /    93 runs   (    0.73 ms per token,  1364.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     380.04 ms /    72 tokens (    5.28 ms per token,   189.45 tokens per second)\n",
            "llama_print_timings:        eval time =    3558.95 ms /    92 runs   (   38.68 ms per token,    25.85 tokens per second)\n",
            "llama_print_timings:       total time =    4106.02 ms /   164 tokens\n",
            " 70%|███████   | 105/150 [06:09<02:59,  3.98s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      35.13 ms /    63 runs   (    0.56 ms per token,  1793.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     122.91 ms /    24 tokens (    5.12 ms per token,   195.26 tokens per second)\n",
            "llama_print_timings:        eval time =    2362.33 ms /    62 runs   (   38.10 ms per token,    26.25 tokens per second)\n",
            "llama_print_timings:       total time =    2568.92 ms /    86 tokens\n",
            " 71%|███████   | 106/150 [06:11<02:36,  3.56s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       7.23 ms /    12 runs   (    0.60 ms per token,  1659.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     123.56 ms /    24 tokens (    5.15 ms per token,   194.23 tokens per second)\n",
            "llama_print_timings:        eval time =     456.08 ms /    12 runs   (   38.01 ms per token,    26.31 tokens per second)\n",
            "llama_print_timings:       total time =     597.92 ms /    36 tokens\n",
            " 71%|███████▏  | 107/150 [06:12<01:55,  2.68s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      18.80 ms /    32 runs   (    0.59 ms per token,  1702.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     209.88 ms /    37 tokens (    5.67 ms per token,   176.29 tokens per second)\n",
            "llama_print_timings:        eval time =    1186.37 ms /    31 runs   (   38.27 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =    1439.52 ms /    68 tokens\n",
            " 72%|███████▏  | 108/150 [06:13<01:37,  2.32s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       7.94 ms /    12 runs   (    0.66 ms per token,  1512.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =     123.17 ms /    22 tokens (    5.60 ms per token,   178.62 tokens per second)\n",
            "llama_print_timings:        eval time =     417.90 ms /    11 runs   (   37.99 ms per token,    26.32 tokens per second)\n",
            "llama_print_timings:       total time =     560.80 ms /    33 tokens\n",
            " 73%|███████▎  | 109/150 [06:14<01:13,  1.80s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      29.71 ms /    50 runs   (    0.59 ms per token,  1683.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     163.78 ms /    30 tokens (    5.46 ms per token,   183.18 tokens per second)\n",
            "llama_print_timings:        eval time =    1878.08 ms /    49 runs   (   38.33 ms per token,    26.09 tokens per second)\n",
            "llama_print_timings:       total time =    2112.70 ms /    79 tokens\n",
            " 73%|███████▎  | 110/150 [06:16<01:15,  1.90s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      34.81 ms /    59 runs   (    0.59 ms per token,  1694.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =     213.57 ms /    39 tokens (    5.48 ms per token,   182.61 tokens per second)\n",
            "llama_print_timings:        eval time =    2224.77 ms /    58 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    2527.50 ms /    97 tokens\n",
            " 74%|███████▍  | 111/150 [06:19<01:21,  2.10s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      55.90 ms /    75 runs   (    0.75 ms per token,  1341.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     168.90 ms /    28 tokens (    6.03 ms per token,   165.78 tokens per second)\n",
            "llama_print_timings:        eval time =    2861.70 ms /    74 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
            "llama_print_timings:       total time =    3168.70 ms /   102 tokens\n",
            " 75%|███████▍  | 112/150 [06:22<01:32,  2.43s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.28 ms /    54 runs   (    0.58 ms per token,  1726.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     505.79 ms /    90 tokens (    5.62 ms per token,   177.94 tokens per second)\n",
            "llama_print_timings:        eval time =    2039.41 ms /    53 runs   (   38.48 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2631.26 ms /   143 tokens\n",
            " 75%|███████▌  | 113/150 [06:25<01:32,  2.50s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      15.05 ms /    28 runs   (    0.54 ms per token,  1860.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     245.37 ms /    42 tokens (    5.84 ms per token,   171.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1031.82 ms /    27 runs   (   38.22 ms per token,    26.17 tokens per second)\n",
            "llama_print_timings:       total time =    1315.44 ms /    69 tokens\n",
            " 76%|███████▌  | 114/150 [06:26<01:17,  2.15s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      50.35 ms /    86 runs   (    0.59 ms per token,  1707.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     455.39 ms /    85 tokens (    5.36 ms per token,   186.65 tokens per second)\n",
            "llama_print_timings:        eval time =    3265.76 ms /    85 runs   (   38.42 ms per token,    26.03 tokens per second)\n",
            "llama_print_timings:       total time =    3847.35 ms /   170 tokens\n",
            " 77%|███████▋  | 115/150 [06:30<01:33,  2.67s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      58.16 ms /    95 runs   (    0.61 ms per token,  1633.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     205.52 ms /    40 tokens (    5.14 ms per token,   194.63 tokens per second)\n",
            "llama_print_timings:        eval time =    3614.43 ms /    94 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    3963.19 ms /   134 tokens\n",
            " 77%|███████▋  | 116/150 [06:34<01:44,  3.06s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      74.13 ms /   118 runs   (    0.63 ms per token,  1591.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =     426.06 ms /    80 tokens (    5.33 ms per token,   187.77 tokens per second)\n",
            "llama_print_timings:        eval time =    4570.32 ms /   118 runs   (   38.73 ms per token,    25.82 tokens per second)\n",
            "llama_print_timings:       total time =    5198.07 ms /   198 tokens\n",
            " 78%|███████▊  | 117/150 [06:39<02:02,  3.71s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      20.35 ms /    33 runs   (    0.62 ms per token,  1621.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     204.77 ms /    38 tokens (    5.39 ms per token,   185.57 tokens per second)\n",
            "llama_print_timings:        eval time =    1226.62 ms /    32 runs   (   38.33 ms per token,    26.09 tokens per second)\n",
            "llama_print_timings:       total time =    1480.10 ms /    70 tokens\n",
            " 79%|███████▊  | 118/150 [06:41<01:37,  3.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      56.94 ms /    93 runs   (    0.61 ms per token,  1633.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     210.21 ms /    40 tokens (    5.26 ms per token,   190.29 tokens per second)\n",
            "llama_print_timings:        eval time =    3565.11 ms /    93 runs   (   38.33 ms per token,    26.09 tokens per second)\n",
            "llama_print_timings:       total time =    3912.65 ms /   133 tokens\n",
            " 79%|███████▉  | 119/150 [06:44<01:42,  3.32s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      74.04 ms /   114 runs   (    0.65 ms per token,  1539.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     328.63 ms /    58 tokens (    5.67 ms per token,   176.49 tokens per second)\n",
            "llama_print_timings:        eval time =    4360.83 ms /   113 runs   (   38.59 ms per token,    25.91 tokens per second)\n",
            "llama_print_timings:       total time =    4884.97 ms /   171 tokens\n",
            " 80%|████████  | 120/150 [06:49<01:53,  3.79s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      43.90 ms /    65 runs   (    0.68 ms per token,  1480.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =     124.62 ms /    24 tokens (    5.19 ms per token,   192.59 tokens per second)\n",
            "llama_print_timings:        eval time =    2464.79 ms /    64 runs   (   38.51 ms per token,    25.97 tokens per second)\n",
            "llama_print_timings:       total time =    2702.02 ms /    88 tokens\n",
            " 81%|████████  | 121/150 [06:52<01:40,  3.48s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      15.31 ms /    27 runs   (    0.57 ms per token,  1763.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     164.80 ms /    31 tokens (    5.32 ms per token,   188.10 tokens per second)\n",
            "llama_print_timings:        eval time =     991.97 ms /    26 runs   (   38.15 ms per token,    26.21 tokens per second)\n",
            "llama_print_timings:       total time =    1192.62 ms /    57 tokens\n",
            " 81%|████████▏ | 122/150 [06:53<01:18,  2.80s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      32.18 ms /    54 runs   (    0.60 ms per token,  1678.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     246.06 ms /    45 tokens (    5.47 ms per token,   182.89 tokens per second)\n",
            "llama_print_timings:        eval time =    2033.09 ms /    53 runs   (   38.36 ms per token,    26.07 tokens per second)\n",
            "llama_print_timings:       total time =    2350.31 ms /    98 tokens\n",
            " 82%|████████▏ | 123/150 [06:56<01:12,  2.67s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      43.82 ms /    77 runs   (    0.57 ms per token,  1757.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     497.67 ms /    94 tokens (    5.29 ms per token,   188.88 tokens per second)\n",
            "llama_print_timings:        eval time =    2918.86 ms /    76 runs   (   38.41 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =    3511.98 ms /   170 tokens\n",
            " 83%|████████▎ | 124/150 [06:59<01:16,  2.93s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.79 ms /    69 runs   (    0.61 ms per token,  1651.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     329.94 ms /    63 tokens (    5.24 ms per token,   190.94 tokens per second)\n",
            "llama_print_timings:        eval time =    2613.23 ms /    68 runs   (   38.43 ms per token,    26.02 tokens per second)\n",
            "llama_print_timings:       total time =    3026.79 ms /   131 tokens\n",
            " 83%|████████▎ | 125/150 [07:02<01:14,  2.97s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.50 ms /    76 runs   (    0.61 ms per token,  1634.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     303.84 ms /    53 tokens (    5.73 ms per token,   174.43 tokens per second)\n",
            "llama_print_timings:        eval time =    2896.70 ms /    75 runs   (   38.62 ms per token,    25.89 tokens per second)\n",
            "llama_print_timings:       total time =    3298.88 ms /   128 tokens\n",
            " 84%|████████▍ | 126/150 [07:06<01:13,  3.07s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      51.44 ms /    89 runs   (    0.58 ms per token,  1730.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     166.84 ms /    32 tokens (    5.21 ms per token,   191.80 tokens per second)\n",
            "llama_print_timings:        eval time =    3354.87 ms /    88 runs   (   38.12 ms per token,    26.23 tokens per second)\n",
            "llama_print_timings:       total time =    3617.20 ms /   120 tokens\n",
            " 85%|████████▍ | 127/150 [07:09<01:14,  3.24s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.02 ms /    58 runs   (    0.53 ms per token,  1869.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =     206.34 ms /    36 tokens (    5.73 ms per token,   174.47 tokens per second)\n",
            "llama_print_timings:        eval time =    2172.10 ms /    57 runs   (   38.11 ms per token,    26.24 tokens per second)\n",
            "llama_print_timings:       total time =    2437.50 ms /    93 tokens\n",
            " 85%|████████▌ | 128/150 [07:12<01:06,  3.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.85 ms /    81 runs   (    0.58 ms per token,  1729.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     168.78 ms /    28 tokens (    6.03 ms per token,   165.90 tokens per second)\n",
            "llama_print_timings:        eval time =    3050.05 ms /    80 runs   (   38.13 ms per token,    26.23 tokens per second)\n",
            "llama_print_timings:       total time =    3306.48 ms /   108 tokens\n",
            " 86%|████████▌ | 129/150 [07:15<01:05,  3.11s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      56.86 ms /    78 runs   (    0.73 ms per token,  1371.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     125.26 ms /    24 tokens (    5.22 ms per token,   191.60 tokens per second)\n",
            "llama_print_timings:        eval time =    3020.10 ms /    78 runs   (   38.72 ms per token,    25.83 tokens per second)\n",
            "llama_print_timings:       total time =    3271.53 ms /   102 tokens\n",
            " 87%|████████▋ | 130/150 [07:18<01:03,  3.17s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      32.82 ms /    50 runs   (    0.66 ms per token,  1523.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =     125.26 ms /    21 tokens (    5.96 ms per token,   167.66 tokens per second)\n",
            "llama_print_timings:        eval time =    1884.06 ms /    49 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    2080.12 ms /    70 tokens\n",
            " 87%|████████▋ | 131/150 [07:20<00:54,  2.85s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      25.91 ms /    45 runs   (    0.58 ms per token,  1737.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     165.93 ms /    32 tokens (    5.19 ms per token,   192.86 tokens per second)\n",
            "llama_print_timings:        eval time =    1675.63 ms /    44 runs   (   38.08 ms per token,    26.26 tokens per second)\n",
            "llama_print_timings:       total time =    1892.80 ms /    76 tokens\n",
            " 88%|████████▊ | 132/150 [07:22<00:46,  2.57s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      14.82 ms /    26 runs   (    0.57 ms per token,  1754.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =     120.92 ms /    20 tokens (    6.05 ms per token,   165.41 tokens per second)\n",
            "llama_print_timings:        eval time =     950.84 ms /    25 runs   (   38.03 ms per token,    26.29 tokens per second)\n",
            "llama_print_timings:       total time =    1099.16 ms /    45 tokens\n",
            " 89%|████████▊ | 133/150 [07:23<00:36,  2.13s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.62 ms /    68 runs   (    0.55 ms per token,  1807.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     122.93 ms /    24 tokens (    5.12 ms per token,   195.23 tokens per second)\n",
            "llama_print_timings:        eval time =    2549.70 ms /    67 runs   (   38.06 ms per token,    26.28 tokens per second)\n",
            "llama_print_timings:       total time =    2743.65 ms /    91 tokens\n",
            " 89%|████████▉ | 134/150 [07:26<00:37,  2.33s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      42.48 ms /    70 runs   (    0.61 ms per token,  1647.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     168.82 ms /    32 tokens (    5.28 ms per token,   189.55 tokens per second)\n",
            "llama_print_timings:        eval time =    2672.48 ms /    70 runs   (   38.18 ms per token,    26.19 tokens per second)\n",
            "llama_print_timings:       total time =    2925.64 ms /   102 tokens\n",
            " 90%|█████████ | 135/150 [07:29<00:37,  2.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      45.58 ms /    66 runs   (    0.69 ms per token,  1447.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     122.74 ms /    23 tokens (    5.34 ms per token,   187.39 tokens per second)\n",
            "llama_print_timings:        eval time =    2500.66 ms /    65 runs   (   38.47 ms per token,    25.99 tokens per second)\n",
            "llama_print_timings:       total time =    2718.24 ms /    88 tokens\n",
            " 91%|█████████ | 136/150 [07:32<00:36,  2.58s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      32.84 ms /    47 runs   (    0.70 ms per token,  1431.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =     205.29 ms /    36 tokens (    5.70 ms per token,   175.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1778.33 ms /    46 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    2047.45 ms /    82 tokens\n",
            " 91%|█████████▏| 137/150 [07:34<00:31,  2.43s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.85 ms /    90 runs   (    0.59 ms per token,  1702.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =     162.48 ms /    30 tokens (    5.42 ms per token,   184.64 tokens per second)\n",
            "llama_print_timings:        eval time =    3397.76 ms /    89 runs   (   38.18 ms per token,    26.19 tokens per second)\n",
            "llama_print_timings:       total time =    3666.20 ms /   119 tokens\n",
            " 92%|█████████▏| 138/150 [07:38<00:33,  2.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      62.34 ms /    99 runs   (    0.63 ms per token,  1588.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =     162.06 ms /    26 tokens (    6.23 ms per token,   160.43 tokens per second)\n",
            "llama_print_timings:        eval time =    3744.64 ms /    98 runs   (   38.21 ms per token,    26.17 tokens per second)\n",
            "llama_print_timings:       total time =    4038.86 ms /   124 tokens\n",
            " 93%|█████████▎| 139/150 [07:42<00:35,  3.19s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      35.77 ms /    59 runs   (    0.61 ms per token,  1649.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     161.73 ms /    27 tokens (    5.99 ms per token,   166.95 tokens per second)\n",
            "llama_print_timings:        eval time =    2211.14 ms /    58 runs   (   38.12 ms per token,    26.23 tokens per second)\n",
            "llama_print_timings:       total time =    2440.25 ms /    85 tokens\n",
            " 93%|█████████▎| 140/150 [07:44<00:29,  2.97s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      29.89 ms /    40 runs   (    0.75 ms per token,  1338.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     214.20 ms /    37 tokens (    5.79 ms per token,   172.73 tokens per second)\n",
            "llama_print_timings:        eval time =    1509.49 ms /    39 runs   (   38.70 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    1791.95 ms /    76 tokens\n",
            " 94%|█████████▍| 141/150 [07:46<00:23,  2.63s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.83 ms /    73 runs   (    0.72 ms per token,  1381.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     166.34 ms /    30 tokens (    5.54 ms per token,   180.35 tokens per second)\n",
            "llama_print_timings:        eval time =    2776.20 ms /    72 runs   (   38.56 ms per token,    25.93 tokens per second)\n",
            "llama_print_timings:       total time =    3080.95 ms /   102 tokens\n",
            " 95%|█████████▍| 142/150 [07:49<00:22,  2.77s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      44.97 ms /    83 runs   (    0.54 ms per token,  1845.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =     261.58 ms /    48 tokens (    5.45 ms per token,   183.50 tokens per second)\n",
            "llama_print_timings:        eval time =    3179.14 ms /    83 runs   (   38.30 ms per token,    26.11 tokens per second)\n",
            "llama_print_timings:       total time =    3546.44 ms /   131 tokens\n",
            " 95%|█████████▌| 143/150 [07:53<00:21,  3.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      22.44 ms /    37 runs   (    0.61 ms per token,  1648.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =     165.49 ms /    32 tokens (    5.17 ms per token,   193.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1373.28 ms /    36 runs   (   38.15 ms per token,    26.21 tokens per second)\n",
            "llama_print_timings:       total time =    1584.38 ms /    68 tokens\n",
            " 96%|█████████▌| 144/150 [07:54<00:15,  2.59s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.26 ms /    81 runs   (    0.60 ms per token,  1678.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =     246.26 ms /    47 tokens (    5.24 ms per token,   190.86 tokens per second)\n",
            "llama_print_timings:        eval time =    3065.26 ms /    80 runs   (   38.32 ms per token,    26.10 tokens per second)\n",
            "llama_print_timings:       total time =    3414.17 ms /   127 tokens\n",
            " 97%|█████████▋| 145/150 [07:58<00:14,  2.85s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      77.21 ms /   132 runs   (    0.58 ms per token,  1709.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =     121.17 ms /    21 tokens (    5.77 ms per token,   173.31 tokens per second)\n",
            "llama_print_timings:        eval time =    5069.89 ms /   131 runs   (   38.70 ms per token,    25.84 tokens per second)\n",
            "llama_print_timings:       total time =    5388.57 ms /   152 tokens\n",
            " 97%|█████████▋| 146/150 [08:03<00:14,  3.62s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      33.94 ms /    61 runs   (    0.56 ms per token,  1797.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =     164.98 ms /    29 tokens (    5.69 ms per token,   175.77 tokens per second)\n",
            "llama_print_timings:        eval time =    2285.69 ms /    60 runs   (   38.09 ms per token,    26.25 tokens per second)\n",
            "llama_print_timings:       total time =    2523.16 ms /    89 tokens\n",
            " 98%|█████████▊| 147/150 [08:06<00:09,  3.30s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.37 ms /    67 runs   (    0.56 ms per token,  1792.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =     624.58 ms /   117 tokens (    5.34 ms per token,   187.33 tokens per second)\n",
            "llama_print_timings:        eval time =    2546.54 ms /    66 runs   (   38.58 ms per token,    25.92 tokens per second)\n",
            "llama_print_timings:       total time =    3252.63 ms /   183 tokens\n",
            " 99%|█████████▊| 148/150 [08:09<00:06,  3.29s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      42.33 ms /    76 runs   (    0.56 ms per token,  1795.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =     326.76 ms /    58 tokens (    5.63 ms per token,   177.50 tokens per second)\n",
            "llama_print_timings:        eval time =    2868.68 ms /    75 runs   (   38.25 ms per token,    26.14 tokens per second)\n",
            "llama_print_timings:       total time =    3293.12 ms /   133 tokens\n",
            " 99%|█████████▉| 149/150 [08:12<00:03,  3.30s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.33 ms /    56 runs   (    0.74 ms per token,  1355.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     541.06 ms /   104 tokens (    5.20 ms per token,   192.21 tokens per second)\n",
            "llama_print_timings:        eval time =    2179.89 ms /    56 runs   (   38.93 ms per token,    25.69 tokens per second)\n",
            "llama_print_timings:       total time =    2820.19 ms /   160 tokens\n",
            "100%|██████████| 150/150 [08:15<00:00,  3.30s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions['names_for_filter'] = None\n",
        "questions['names_for_filter'][:8] = '3M CO'\n",
        "questions['names_for_filter'][8:10] = 'Activision Blizzard, Inc.'\n",
        "questions['names_for_filter'][10:15] = 'ADOBE INC.'\n",
        "questions['names_for_filter'][15:18] = 'AES CORP'\n",
        "questions['names_for_filter'][18:21] = 'AMAZON COM INC'\n",
        "questions['names_for_filter'][21:30] = 'Amcor plc'\n",
        "questions['names_for_filter'][30:38] = 'ADVANCED MICRO DEVICES INC'\n",
        "questions['names_for_filter'][38:45] = 'AMERICAN EXPRESS CO'\n",
        "questions['names_for_filter'][45:48] = 'American Water Works Company, Inc.'\n",
        "questions['names_for_filter'][48:56] = 'BEST BUY CO INC'\n",
        "questions['names_for_filter'][56:59] = 'Square, Inc.'\n",
        "questions['names_for_filter'][59:67] = 'BOEING CO'\n",
        "questions['names_for_filter'][67:70] = 'COCA COLA CO'\n",
        "questions['names_for_filter'][70:74] = 'CORNING INC /NY'\n",
        "questions['names_for_filter'][74:75] = 'COSTCO WHOLESALE CORP /NEW'\n",
        "questions['names_for_filter'][75:79] = 'CVS HEALTH Corp'\n",
        "questions['names_for_filter'][79:81] = 'FOOT LOCKER, INC.'\n",
        "questions['names_for_filter'][81:85] = 'GENERAL MILLS INC'\n",
        "questions['names_for_filter'][85:94] = 'JOHNSON & JOHNSON'\n",
        "questions['names_for_filter'][94:99] = 'JPMORGAN CHASE & CO'\n",
        "questions['names_for_filter'][99:100] = 'Kraft Heinz Co'\n",
        "questions['names_for_filter'][100:103] = 'LOCKHEED MARTIN CORP'\n",
        "questions['names_for_filter'][103:110] = 'MGM Resorts International'\n",
        "questions['names_for_filter'][110:112] = 'MICROSOFT CORP'\n",
        "questions['names_for_filter'][112:114] = 'NETFLIX INC'\n",
        "questions['names_for_filter'][114:118] = 'NIKE, Inc.'\n",
        "questions['names_for_filter'][118:119] = 'PayPal Holdings, Inc.'\n",
        "questions['names_for_filter'][119:130] = 'PEPSICO INC'\n",
        "questions['names_for_filter'][130:136] = 'PFIZER INC'\n",
        "questions['names_for_filter'][136:142] = 'Ulta Beauty, Inc.'\n",
        "questions['names_for_filter'][142:147] = 'VERIZON COMMUNICATIONS INC'\n",
        "questions['names_for_filter'][147:150] = 'Walmart Inc.'"
      ],
      "metadata": {
        "id": "FOhEM2gF258h"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#questions.to_json(f'/content/drive/MyDrive/Thesis/rag_evaluation/financebench150/questions_metadata_names_retrieved.json')"
      ],
      "metadata": {
        "id": "zxAKbbk6_FZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "llm_generate = llm_phi3\n",
        "\n",
        "prompt_generate = PromptTemplate(\n",
        "    template=\"\"\"<|assistant|> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know. Keep the answer concise <|end|>\n",
        "    <|user|> Question: {question}. \\n Context: {documents} \\n Answer: <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"question\", \"documents\"],\n",
        ")\n",
        "\n",
        "rag_chain = prompt_generate | llm_generate | StrOutputParser()"
      ],
      "metadata": {
        "id": "hdn828f89VFM"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Hallucination Grader\n",
        "llm_hallucination_grader = llm_phi3\n",
        "\n",
        "# Prompt\n",
        "prompt_hallucination_grader = PromptTemplate(\n",
        "    template=\"\"\" <|assistant|> You are a grader assessing whether an answer is grounded in / supported by a set of facts.\n",
        "    Give a binary 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts.<|end|>\n",
        "    <|user|> Here are the facts: {documents} \\n Here is the answer: {generation}  <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"generation\", \"documents\"],\n",
        ")\n",
        "\n",
        "hallucination_grader = prompt_hallucination_grader | llm_hallucination_grader | StrOutputParser()"
      ],
      "metadata": {
        "id": "ZIub6HTG9VFM"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Answer Grader\n",
        "llm_answer_grader = llm_phi3\n",
        "\n",
        "# Prompt\n",
        "prompt_answer_grader = PromptTemplate(\n",
        "    template=\"\"\"<|assistant|> You are a grader assessing whether a generated answer is correct or incorrect, comparing a generated answer with the ground truth.\n",
        "    Give a binary score 'yes' or 'no' to indicate whether the generated answer equal to or contains the ground truth.<|end|>\n",
        "    <|user|> Here is the answer: {generation} \\n Here is the ground truth: {ground_truth} <|end|>\n",
        "    <|assistant|>\"\"\",\n",
        "    input_variables=[\"generation\", \"ground_truth\"],\n",
        ")\n",
        "\n",
        "answer_grader = prompt_answer_grader | llm_answer_grader | StrOutputParser()"
      ],
      "metadata": {
        "id": "vWM7EZxJ9VFN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_PAR_CHUNKS = 20\n",
        "N_DOCS_RETURN = 2\n",
        "\n",
        "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=256)\n",
        "\n",
        "results_list = []\n",
        "\n",
        "for i in tqdm(range(len(questions))):\n",
        "  query = questions['question'][i]\n",
        "  company = questions['names_for_filter'][i]\n",
        "\n",
        "  big_chunks_retriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore=store, child_splitter=child_splitter, parent_splitter=parent_splitter,\n",
        "                                                search_kwargs={'filter': {'company': company}, 'k': NUM_PAR_CHUNKS})\n",
        "  passage = big_chunks_retriever.invoke(query)\n",
        "\n",
        "  texts = []\n",
        "  for j in range(len(passage)):\n",
        "    texts.append([query, passage[j].page_content])\n",
        "\n",
        "  if not texts:\n",
        "    error_message = f\"Skipping question {i} due to empty texts list\"\n",
        "    print(error_message)\n",
        "    results_list.append(pd.DataFrame({\n",
        "                                      'question': [query],\n",
        "                                      'response': [error_message],\n",
        "                                      'context': [error_message],\n",
        "                                      'hallucination_grade': [error_message],\n",
        "                                      'answer_grade': [error_message],\n",
        "                                      'ground_truth': [questions['answer'][i]],\n",
        "                                      'evidence': [questions['evidence'][i]]\n",
        "                                      }))\n",
        "    continue\n",
        "\n",
        "  scores = reranker.compute_score(texts)\n",
        "  combined = list(zip(texts, scores))\n",
        "  sorted_combined = sorted(combined, key=lambda x: x[1], reverse=True)\n",
        "  top_texts = [item[0] for item in sorted_combined[:N_DOCS_RETURN]]\n",
        "  docs = [inner_list[1] for inner_list in top_texts if len(inner_list)>1]\n",
        "\n",
        "  generation = rag_chain.invoke({\"documents\": docs, \"question\": query})\n",
        "  hallucination_grade = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
        "  answer_grade = answer_grader.invoke({\"ground_truth\": questions['answer'][i], \"generation\": generation})\n",
        "\n",
        "  results_list.append(pd.DataFrame({\n",
        "                                    'question': [query],\n",
        "                                    'response': [generation],\n",
        "                                    'context': [docs],\n",
        "                                    'hallucination_grade': [hallucination_grade],\n",
        "                                    'answer_grade': [answer_grade],\n",
        "                                    'ground_truth': [questions['answer'][i]],\n",
        "                                    'evidence': [questions['evidence'][i]]\n",
        "                                    }))\n",
        "\n",
        "results = pd.concat(results_list, ignore_index=True)\n",
        "results.to_json(f'/content/drive/MyDrive/Thesis/rag_evaluation/financebench150/eval_v2.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro7GaCz69VFO",
        "outputId": "b74c0ddc-17ba-4c57-e0c8-491f297505c1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/150 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      92.70 ms /   122 runs   (    0.76 ms per token,  1316.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6796.71 ms /  1166 tokens (    5.83 ms per token,   171.55 tokens per second)\n",
            "llama_print_timings:        eval time =    4852.11 ms /   121 runs   (   40.10 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =   11887.78 ms /  1287 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.55 ms per token,  1828.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6922.78 ms /  1248 tokens (    5.55 ms per token,   180.27 tokens per second)\n",
            "llama_print_timings:        eval time =      78.27 ms /     2 runs   (   39.13 ms per token,    25.55 tokens per second)\n",
            "llama_print_timings:       total time =    7021.67 ms /  1250 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.58 ms per token,  1713.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1013.70 ms /   200 tokens (    5.07 ms per token,   197.30 tokens per second)\n",
            "llama_print_timings:        eval time =      76.45 ms /     2 runs   (   38.23 ms per token,    26.16 tokens per second)\n",
            "llama_print_timings:       total time =    1096.31 ms /   202 tokens\n",
            "  1%|          | 1/150 [00:57<2:23:32, 57.80s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      17.30 ms /    30 runs   (    0.58 ms per token,  1734.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7401.31 ms /  1346 tokens (    5.50 ms per token,   181.86 tokens per second)\n",
            "llama_print_timings:        eval time =    1145.38 ms /    29 runs   (   39.50 ms per token,    25.32 tokens per second)\n",
            "llama_print_timings:       total time =    8603.43 ms /  1375 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      73.84 ms /   114 runs   (    0.65 ms per token,  1543.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7087.63 ms /  1331 tokens (    5.33 ms per token,   187.79 tokens per second)\n",
            "llama_print_timings:        eval time =    4538.86 ms /   113 runs   (   40.17 ms per token,    24.90 tokens per second)\n",
            "llama_print_timings:       total time =   11810.65 ms /  1444 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      56.63 ms /    97 runs   (    0.58 ms per token,  1712.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =     536.49 ms /   106 tokens (    5.06 ms per token,   197.58 tokens per second)\n",
            "llama_print_timings:        eval time =    3532.99 ms /    96 runs   (   36.80 ms per token,    27.17 tokens per second)\n",
            "llama_print_timings:       total time =    4178.43 ms /   202 tokens\n",
            "  1%|▏         | 2/150 [01:36<1:54:42, 46.50s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      67.31 ms /   108 runs   (    0.62 ms per token,  1604.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5435.65 ms /  1021 tokens (    5.32 ms per token,   187.83 tokens per second)\n",
            "llama_print_timings:        eval time =    4199.92 ms /   107 runs   (   39.25 ms per token,    25.48 tokens per second)\n",
            "llama_print_timings:       total time =    9789.41 ms /  1128 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.57 ms per token,  1745.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5899.77 ms /  1118 tokens (    5.28 ms per token,   189.50 tokens per second)\n",
            "llama_print_timings:        eval time =      39.27 ms /     1 runs   (   39.27 ms per token,    25.47 tokens per second)\n",
            "llama_print_timings:       total time =    5957.33 ms /  1119 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.52 ms per token,  1941.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1224.36 ms /   245 tokens (    5.00 ms per token,   200.10 tokens per second)\n",
            "llama_print_timings:        eval time =      36.41 ms /     1 runs   (   36.41 ms per token,    27.47 tokens per second)\n",
            "llama_print_timings:       total time =    1265.77 ms /   246 tokens\n",
            "  2%|▏         | 3/150 [02:01<1:30:16, 36.85s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     117.85 ms /   189 runs   (    0.62 ms per token,  1603.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7959.12 ms /  1453 tokens (    5.48 ms per token,   182.56 tokens per second)\n",
            "llama_print_timings:        eval time =    7722.90 ms /   188 runs   (   41.08 ms per token,    24.34 tokens per second)\n",
            "llama_print_timings:       total time =   15976.64 ms /  1641 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     105.89 ms /   180 runs   (    0.59 ms per token,  1699.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8959.95 ms /  1611 tokens (    5.56 ms per token,   179.80 tokens per second)\n",
            "llama_print_timings:        eval time =    7476.74 ms /   179 runs   (   41.77 ms per token,    23.94 tokens per second)\n",
            "llama_print_timings:       total time =   16698.88 ms /  1790 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      55.05 ms /    81 runs   (    0.68 ms per token,  1471.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1771.15 ms /   349 tokens (    5.07 ms per token,   197.05 tokens per second)\n",
            "llama_print_timings:        eval time =    3092.72 ms /    80 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
            "llama_print_timings:       total time =    4985.38 ms /   429 tokens\n",
            "  3%|▎         | 4/150 [02:50<1:40:45, 41.41s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      63.26 ms /    97 runs   (    0.65 ms per token,  1533.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6253.12 ms /  1160 tokens (    5.39 ms per token,   185.51 tokens per second)\n",
            "llama_print_timings:        eval time =    3925.75 ms /    97 runs   (   40.47 ms per token,    24.71 tokens per second)\n",
            "llama_print_timings:       total time =   10319.80 ms /  1257 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1771.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6764.05 ms /  1237 tokens (    5.47 ms per token,   182.88 tokens per second)\n",
            "llama_print_timings:        eval time =      40.35 ms /     1 runs   (   40.35 ms per token,    24.78 tokens per second)\n",
            "llama_print_timings:       total time =    6824.37 ms /  1238 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     156.52 ms /   256 runs   (    0.61 ms per token,  1635.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     910.45 ms /   182 tokens (    5.00 ms per token,   199.90 tokens per second)\n",
            "llama_print_timings:        eval time =    9733.66 ms /   255 runs   (   38.17 ms per token,    26.20 tokens per second)\n",
            "llama_print_timings:       total time =   11005.48 ms /   437 tokens\n",
            "  3%|▎         | 5/150 [03:33<1:41:50, 42.14s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     136.20 ms /   228 runs   (    0.60 ms per token,  1673.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5517.89 ms /  1032 tokens (    5.35 ms per token,   187.03 tokens per second)\n",
            "llama_print_timings:        eval time =    9133.70 ms /   228 runs   (   40.06 ms per token,    24.96 tokens per second)\n",
            "llama_print_timings:       total time =   14963.08 ms /  1260 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.20 ms /    92 runs   (    0.57 ms per token,  1762.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6628.22 ms /  1222 tokens (    5.42 ms per token,   184.36 tokens per second)\n",
            "llama_print_timings:        eval time =    3677.19 ms /    91 runs   (   40.41 ms per token,    24.75 tokens per second)\n",
            "llama_print_timings:       total time =   10424.36 ms /  1313 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.21 ms /     2 runs   (    0.60 ms per token,  1655.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1681.08 ms /   335 tokens (    5.02 ms per token,   199.28 tokens per second)\n",
            "llama_print_timings:        eval time =      37.44 ms /     1 runs   (   37.44 ms per token,    26.71 tokens per second)\n",
            "llama_print_timings:       total time =    1726.14 ms /   336 tokens\n",
            "  4%|▍         | 6/150 [04:06<1:33:18, 38.88s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.71 ms /    75 runs   (    0.64 ms per token,  1571.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6527.96 ms /  1211 tokens (    5.39 ms per token,   185.51 tokens per second)\n",
            "llama_print_timings:        eval time =    2990.87 ms /    74 runs   (   40.42 ms per token,    24.74 tokens per second)\n",
            "llama_print_timings:       total time =    9631.21 ms /  1285 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      68.00 ms /   120 runs   (    0.57 ms per token,  1764.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6842.70 ms /  1260 tokens (    5.43 ms per token,   184.14 tokens per second)\n",
            "llama_print_timings:        eval time =    4834.11 ms /   119 runs   (   40.62 ms per token,    24.62 tokens per second)\n",
            "llama_print_timings:       total time =   11833.84 ms /  1379 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.35 ms /     2 runs   (    0.68 ms per token,  1479.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1240.33 ms /   246 tokens (    5.04 ms per token,   198.33 tokens per second)\n",
            "llama_print_timings:        eval time =      37.54 ms /     1 runs   (   37.54 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =    1284.73 ms /   247 tokens\n",
            "  5%|▍         | 7/150 [04:37<1:26:49, 36.43s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      36.27 ms /    67 runs   (    0.54 ms per token,  1847.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5619.08 ms /  1056 tokens (    5.32 ms per token,   187.93 tokens per second)\n",
            "llama_print_timings:        eval time =    2631.04 ms /    66 runs   (   39.86 ms per token,    25.09 tokens per second)\n",
            "llama_print_timings:       total time =    8337.68 ms /  1122 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.35 ms /    70 runs   (    0.53 ms per token,  1874.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6011.66 ms /  1118 tokens (    5.38 ms per token,   185.97 tokens per second)\n",
            "llama_print_timings:        eval time =    2754.92 ms /    69 runs   (   39.93 ms per token,    25.05 tokens per second)\n",
            "llama_print_timings:       total time =    8858.34 ms /  1187 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1867.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     864.76 ms /   171 tokens (    5.06 ms per token,   197.74 tokens per second)\n",
            "llama_print_timings:        eval time =      37.39 ms /     1 runs   (   37.39 ms per token,    26.75 tokens per second)\n",
            "llama_print_timings:       total time =     906.47 ms /   172 tokens\n",
            "  5%|▌         | 8/150 [05:01<1:16:50, 32.47s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     141.28 ms /   256 runs   (    0.55 ms per token,  1811.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11882.48 ms /  2076 tokens (    5.72 ms per token,   174.71 tokens per second)\n",
            "llama_print_timings:        eval time =   10931.03 ms /   255 runs   (   42.87 ms per token,    23.33 tokens per second)\n",
            "llama_print_timings:       total time =   23250.61 ms /  2331 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     139.34 ms /   256 runs   (    0.54 ms per token,  1837.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13025.49 ms /  2250 tokens (    5.79 ms per token,   172.74 tokens per second)\n",
            "llama_print_timings:        eval time =   11253.57 ms /   255 runs   (   44.13 ms per token,    22.66 tokens per second)\n",
            "llama_print_timings:       total time =   24690.63 ms /  2505 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      70.87 ms /   121 runs   (    0.59 ms per token,  1707.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1688.83 ms /   334 tokens (    5.06 ms per token,   197.77 tokens per second)\n",
            "llama_print_timings:        eval time =    4544.56 ms /   120 runs   (   37.87 ms per token,    26.41 tokens per second)\n",
            "llama_print_timings:       total time =    6382.22 ms /   454 tokens\n",
            "  6%|▌         | 9/150 [06:08<1:41:22, 43.14s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     136.09 ms /   256 runs   (    0.53 ms per token,  1881.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14756.90 ms /  2494 tokens (    5.92 ms per token,   169.01 tokens per second)\n",
            "llama_print_timings:        eval time =   12495.68 ms /   255 runs   (   49.00 ms per token,    20.41 tokens per second)\n",
            "llama_print_timings:       total time =   27681.70 ms /  2749 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     142.14 ms /   256 runs   (    0.56 ms per token,  1800.99 tokens per second)\n",
            "llama_print_timings: prompt eval time =   16301.25 ms /  2687 tokens (    6.07 ms per token,   164.83 tokens per second)\n",
            "llama_print_timings:        eval time =   12624.05 ms /   255 runs   (   49.51 ms per token,    20.20 tokens per second)\n",
            "llama_print_timings:       total time =   29357.87 ms /  2942 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1774.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1673.93 ms /   331 tokens (    5.06 ms per token,   197.74 tokens per second)\n",
            "llama_print_timings:        eval time =      37.59 ms /     1 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    1717.79 ms /   332 tokens\n",
            "  7%|▋         | 10/150 [07:11<1:55:13, 49.38s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.01 ms /    86 runs   (    0.54 ms per token,  1869.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9513.74 ms /  1704 tokens (    5.58 ms per token,   179.11 tokens per second)\n",
            "llama_print_timings:        eval time =    3584.66 ms /    86 runs   (   41.68 ms per token,    23.99 tokens per second)\n",
            "llama_print_timings:       total time =   13222.82 ms /  1790 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      60.99 ms /   106 runs   (    0.58 ms per token,  1738.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9646.09 ms /  1712 tokens (    5.63 ms per token,   177.48 tokens per second)\n",
            "llama_print_timings:        eval time =    4437.68 ms /   106 runs   (   41.86 ms per token,    23.89 tokens per second)\n",
            "llama_print_timings:       total time =   14241.15 ms /  1818 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1968.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     822.53 ms /   162 tokens (    5.08 ms per token,   196.95 tokens per second)\n",
            "llama_print_timings:        eval time =      36.73 ms /     1 runs   (   36.73 ms per token,    27.23 tokens per second)\n",
            "llama_print_timings:       total time =     863.38 ms /   163 tokens\n",
            "  7%|▋         | 11/150 [07:50<1:46:54, 46.15s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     112.15 ms /   214 runs   (    0.52 ms per token,  1908.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11503.64 ms /  2013 tokens (    5.71 ms per token,   174.99 tokens per second)\n",
            "llama_print_timings:        eval time =    9112.18 ms /   213 runs   (   42.78 ms per token,    23.38 tokens per second)\n",
            "llama_print_timings:       total time =   20908.62 ms /  2226 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      49.42 ms /    83 runs   (    0.60 ms per token,  1679.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12568.72 ms /  2176 tokens (    5.78 ms per token,   173.13 tokens per second)\n",
            "llama_print_timings:        eval time =    3593.40 ms /    83 runs   (   43.29 ms per token,    23.10 tokens per second)\n",
            "llama_print_timings:       total time =   16302.75 ms /  2259 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      49.14 ms /    81 runs   (    0.61 ms per token,  1648.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1487.87 ms /   291 tokens (    5.11 ms per token,   195.58 tokens per second)\n",
            "llama_print_timings:        eval time =    3015.75 ms /    80 runs   (   37.70 ms per token,    26.53 tokens per second)\n",
            "llama_print_timings:       total time =    4607.66 ms /   371 tokens\n",
            "  8%|▊         | 12/150 [08:40<1:49:13, 47.49s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     127.56 ms /   242 runs   (    0.53 ms per token,  1897.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9343.45 ms /  1680 tokens (    5.56 ms per token,   179.81 tokens per second)\n",
            "llama_print_timings:        eval time =   10097.05 ms /   242 runs   (   41.72 ms per token,    23.97 tokens per second)\n",
            "llama_print_timings:       total time =   19774.85 ms /  1922 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     126.46 ms /   200 runs   (    0.63 ms per token,  1581.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10575.27 ms /  1869 tokens (    5.66 ms per token,   176.73 tokens per second)\n",
            "llama_print_timings:        eval time =    8477.15 ms /   199 runs   (   42.60 ms per token,    23.47 tokens per second)\n",
            "llama_print_timings:       total time =   19394.56 ms /  2068 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1923.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1592.35 ms /   318 tokens (    5.01 ms per token,   199.70 tokens per second)\n",
            "llama_print_timings:        eval time =      37.24 ms /     1 runs   (   37.24 ms per token,    26.85 tokens per second)\n",
            "llama_print_timings:       total time =    1635.67 ms /   319 tokens\n",
            "  9%|▊         | 13/150 [09:24<1:45:39, 46.27s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     145.48 ms /   233 runs   (    0.62 ms per token,  1601.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10277.55 ms /  1819 tokens (    5.65 ms per token,   176.99 tokens per second)\n",
            "llama_print_timings:        eval time =    9909.37 ms /   232 runs   (   42.71 ms per token,    23.41 tokens per second)\n",
            "llama_print_timings:       total time =   20551.19 ms /  2051 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     156.69 ms /   256 runs   (    0.61 ms per token,  1633.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11627.62 ms /  2021 tokens (    5.75 ms per token,   173.81 tokens per second)\n",
            "llama_print_timings:        eval time =   10993.18 ms /   255 runs   (   43.11 ms per token,    23.20 tokens per second)\n",
            "llama_print_timings:       total time =   23040.96 ms /  2276 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.49 ms per token,  2024.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1794.83 ms /   358 tokens (    5.01 ms per token,   199.46 tokens per second)\n",
            "llama_print_timings:        eval time =      37.39 ms /     1 runs   (   37.39 ms per token,    26.74 tokens per second)\n",
            "llama_print_timings:       total time =    1838.42 ms /   359 tokens\n",
            "  9%|▉         | 14/150 [10:19<1:50:40, 48.82s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      39.88 ms /    72 runs   (    0.55 ms per token,  1805.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9086.13 ms /  1639 tokens (    5.54 ms per token,   180.38 tokens per second)\n",
            "llama_print_timings:        eval time =    2932.18 ms /    71 runs   (   41.30 ms per token,    24.21 tokens per second)\n",
            "llama_print_timings:       total time =   12130.44 ms /  1710 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     110.87 ms /   196 runs   (    0.57 ms per token,  1767.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9490.50 ms /  1699 tokens (    5.59 ms per token,   179.02 tokens per second)\n",
            "llama_print_timings:        eval time =    8141.21 ms /   195 runs   (   41.75 ms per token,    23.95 tokens per second)\n",
            "llama_print_timings:       total time =   17911.51 ms /  1894 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1673.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =     952.32 ms /   192 tokens (    4.96 ms per token,   201.61 tokens per second)\n",
            "llama_print_timings:        eval time =      37.92 ms /     1 runs   (   37.92 ms per token,    26.37 tokens per second)\n",
            "llama_print_timings:       total time =     995.31 ms /   193 tokens\n",
            " 10%|█         | 15/150 [10:57<1:43:08, 45.84s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      17.60 ms /    33 runs   (    0.53 ms per token,  1875.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10238.40 ms /  1820 tokens (    5.63 ms per token,   177.76 tokens per second)\n",
            "llama_print_timings:        eval time =    1334.55 ms /    32 runs   (   41.70 ms per token,    23.98 tokens per second)\n",
            "llama_print_timings:       total time =   11631.03 ms /  1852 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     149.67 ms /   256 runs   (    0.58 ms per token,  1710.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10232.11 ms /  1819 tokens (    5.63 ms per token,   177.77 tokens per second)\n",
            "llama_print_timings:        eval time =   10738.36 ms /   255 runs   (   42.11 ms per token,    23.75 tokens per second)\n",
            "llama_print_timings:       total time =   21341.52 ms /  2074 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.22 ms /     2 runs   (    0.61 ms per token,  1643.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     547.26 ms /   106 tokens (    5.16 ms per token,   193.69 tokens per second)\n",
            "llama_print_timings:        eval time =      36.84 ms /     1 runs   (   36.84 ms per token,    27.15 tokens per second)\n",
            "llama_print_timings:       total time =     591.38 ms /   107 tokens\n",
            " 11%|█         | 16/150 [11:40<1:39:50, 44.71s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      93.73 ms /   168 runs   (    0.56 ms per token,  1792.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10291.48 ms /  1834 tokens (    5.61 ms per token,   178.21 tokens per second)\n",
            "llama_print_timings:        eval time =    7003.56 ms /   167 runs   (   41.94 ms per token,    23.85 tokens per second)\n",
            "llama_print_timings:       total time =   17519.75 ms /  2001 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     153.06 ms /   225 runs   (    0.68 ms per token,  1470.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11126.74 ms /  1954 tokens (    5.69 ms per token,   175.61 tokens per second)\n",
            "llama_print_timings:        eval time =    9644.48 ms /   224 runs   (   43.06 ms per token,    23.23 tokens per second)\n",
            "llama_print_timings:       total time =   21176.54 ms /  2178 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      33.49 ms /    62 runs   (    0.54 ms per token,  1851.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1309.17 ms /   259 tokens (    5.05 ms per token,   197.83 tokens per second)\n",
            "llama_print_timings:        eval time =    2306.09 ms /    61 runs   (   37.80 ms per token,    26.45 tokens per second)\n",
            "llama_print_timings:       total time =    3679.79 ms /   320 tokens\n",
            " 11%|█▏        | 17/150 [12:31<1:43:23, 46.64s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.91 ms /    81 runs   (    0.58 ms per token,  1726.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10575.62 ms /  1856 tokens (    5.70 ms per token,   175.50 tokens per second)\n",
            "llama_print_timings:        eval time =    3470.56 ms /    81 runs   (   42.85 ms per token,    23.34 tokens per second)\n",
            "llama_print_timings:       total time =   14181.58 ms /  1937 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      65.00 ms /   104 runs   (    0.63 ms per token,  1599.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10650.93 ms /  1867 tokens (    5.70 ms per token,   175.29 tokens per second)\n",
            "llama_print_timings:        eval time =    4408.25 ms /   103 runs   (   42.80 ms per token,    23.37 tokens per second)\n",
            "llama_print_timings:       total time =   15231.98 ms /  1970 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1696.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     794.10 ms /   157 tokens (    5.06 ms per token,   197.71 tokens per second)\n",
            "llama_print_timings:        eval time =      37.48 ms /     1 runs   (   37.48 ms per token,    26.68 tokens per second)\n",
            "llama_print_timings:       total time =     836.93 ms /   158 tokens\n",
            " 12%|█▏        | 18/150 [13:06<1:35:05, 43.22s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     134.97 ms /   256 runs   (    0.53 ms per token,  1896.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13050.83 ms /  2248 tokens (    5.81 ms per token,   172.25 tokens per second)\n",
            "llama_print_timings:        eval time =   11398.23 ms /   255 runs   (   44.70 ms per token,    22.37 tokens per second)\n",
            "llama_print_timings:       total time =   24816.32 ms /  2503 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     132.24 ms /   256 runs   (    0.52 ms per token,  1935.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14312.95 ms /  2402 tokens (    5.96 ms per token,   167.82 tokens per second)\n",
            "llama_print_timings:        eval time =   12107.09 ms /   255 runs   (   47.48 ms per token,    21.06 tokens per second)\n",
            "llama_print_timings:       total time =   26785.82 ms /  2657 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      49.63 ms /    88 runs   (    0.56 ms per token,  1772.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1696.89 ms /   334 tokens (    5.08 ms per token,   196.83 tokens per second)\n",
            "llama_print_timings:        eval time =    3315.33 ms /    87 runs   (   38.11 ms per token,    26.24 tokens per second)\n",
            "llama_print_timings:       total time =    5115.48 ms /   421 tokens\n",
            " 13%|█▎        | 19/150 [14:17<1:52:36, 51.58s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      25.90 ms /    44 runs   (    0.59 ms per token,  1699.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10058.23 ms /  1784 tokens (    5.64 ms per token,   177.37 tokens per second)\n",
            "llama_print_timings:        eval time =    1805.30 ms /    43 runs   (   41.98 ms per token,    23.82 tokens per second)\n",
            "llama_print_timings:       total time =   11941.70 ms /  1827 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      81.65 ms /   145 runs   (    0.56 ms per token,  1775.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10014.72 ms /  1776 tokens (    5.64 ms per token,   177.34 tokens per second)\n",
            "llama_print_timings:        eval time =    6089.62 ms /   144 runs   (   42.29 ms per token,    23.65 tokens per second)\n",
            "llama_print_timings:       total time =   16324.41 ms /  1920 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1888.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     584.46 ms /   120 tokens (    4.87 ms per token,   205.32 tokens per second)\n",
            "llama_print_timings:        eval time =      74.78 ms /     2 runs   (   37.39 ms per token,    26.74 tokens per second)\n",
            "llama_print_timings:       total time =     663.45 ms /   122 tokens\n",
            " 13%|█▎        | 20/150 [14:52<1:40:52, 46.56s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      17.64 ms /    30 runs   (    0.59 ms per token,  1700.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11966.94 ms /  2072 tokens (    5.78 ms per token,   173.14 tokens per second)\n",
            "llama_print_timings:        eval time =    1302.53 ms /    30 runs   (   43.42 ms per token,    23.03 tokens per second)\n",
            "llama_print_timings:       total time =   13340.65 ms /  2102 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.61 ms /    84 runs   (    0.63 ms per token,  1596.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11981.97 ms /  2071 tokens (    5.79 ms per token,   172.84 tokens per second)\n",
            "llama_print_timings:        eval time =    3605.96 ms /    83 runs   (   43.45 ms per token,    23.02 tokens per second)\n",
            "llama_print_timings:       total time =   15760.80 ms /  2154 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.22 ms /     2 runs   (    0.61 ms per token,  1642.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =     557.26 ms /   110 tokens (    5.07 ms per token,   197.39 tokens per second)\n",
            "llama_print_timings:        eval time =      37.45 ms /     1 runs   (   37.45 ms per token,    26.70 tokens per second)\n",
            "llama_print_timings:       total time =     600.26 ms /   111 tokens\n",
            " 14%|█▍        | 21/150 [15:24<1:31:04, 42.36s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      54.87 ms /    82 runs   (    0.67 ms per token,  1494.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10332.53 ms /  1832 tokens (    5.64 ms per token,   177.30 tokens per second)\n",
            "llama_print_timings:        eval time =    3456.57 ms /    81 runs   (   42.67 ms per token,    23.43 tokens per second)\n",
            "llama_print_timings:       total time =   13942.88 ms /  1913 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      71.87 ms /   108 runs   (    0.67 ms per token,  1502.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10750.69 ms /  1876 tokens (    5.73 ms per token,   174.50 tokens per second)\n",
            "llama_print_timings:        eval time =    4611.04 ms /   107 runs   (   43.09 ms per token,    23.21 tokens per second)\n",
            "llama_print_timings:       total time =   15563.60 ms /  1983 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.58 ms per token,  1713.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =     794.64 ms /   160 tokens (    4.97 ms per token,   201.35 tokens per second)\n",
            "llama_print_timings:        eval time =      75.49 ms /     2 runs   (   37.74 ms per token,    26.49 tokens per second)\n",
            "llama_print_timings:       total time =     875.87 ms /   162 tokens\n",
            " 15%|█▍        | 22/150 [16:10<1:32:08, 43.19s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      57.52 ms /    82 runs   (    0.70 ms per token,  1425.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7228.43 ms /  1325 tokens (    5.46 ms per token,   183.30 tokens per second)\n",
            "llama_print_timings:        eval time =    3332.07 ms /    81 runs   (   41.14 ms per token,    24.31 tokens per second)\n",
            "llama_print_timings:       total time =   10723.54 ms /  1406 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.57 ms per token,  1746.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7653.99 ms /  1387 tokens (    5.52 ms per token,   181.21 tokens per second)\n",
            "llama_print_timings:        eval time =      40.74 ms /     1 runs   (   40.74 ms per token,    24.55 tokens per second)\n",
            "llama_print_timings:       total time =    7724.84 ms /  1388 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      62.55 ms /    98 runs   (    0.64 ms per token,  1566.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1274.99 ms /   254 tokens (    5.02 ms per token,   199.22 tokens per second)\n",
            "llama_print_timings:        eval time =    3699.02 ms /    97 runs   (   38.13 ms per token,    26.22 tokens per second)\n",
            "llama_print_timings:       total time =    5115.16 ms /   351 tokens\n",
            " 15%|█▌        | 23/150 [16:44<1:26:00, 40.64s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     117.59 ms /   206 runs   (    0.57 ms per token,  1751.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10983.06 ms /  1906 tokens (    5.76 ms per token,   173.54 tokens per second)\n",
            "llama_print_timings:        eval time =    8796.44 ms /   205 runs   (   42.91 ms per token,    23.30 tokens per second)\n",
            "llama_print_timings:       total time =   20110.18 ms /  2111 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     109.20 ms /   176 runs   (    0.62 ms per token,  1611.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12021.13 ms /  2067 tokens (    5.82 ms per token,   171.95 tokens per second)\n",
            "llama_print_timings:        eval time =    7609.44 ms /   175 runs   (   43.48 ms per token,    23.00 tokens per second)\n",
            "llama_print_timings:       total time =   19957.82 ms /  2242 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      51.89 ms /    99 runs   (    0.52 ms per token,  1907.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1598.41 ms /   320 tokens (    5.00 ms per token,   200.20 tokens per second)\n",
            "llama_print_timings:        eval time =    3725.02 ms /    98 runs   (   38.01 ms per token,    26.31 tokens per second)\n",
            "llama_print_timings:       total time =    5449.35 ms /   418 tokens\n",
            " 16%|█▌        | 24/150 [17:39<1:34:17, 44.90s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.62 ms /    57 runs   (    0.55 ms per token,  1802.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9325.84 ms /  1666 tokens (    5.60 ms per token,   178.64 tokens per second)\n",
            "llama_print_timings:        eval time =    2338.12 ms /    56 runs   (   41.75 ms per token,    23.95 tokens per second)\n",
            "llama_print_timings:       total time =   11762.07 ms /  1722 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      60.04 ms /    90 runs   (    0.67 ms per token,  1499.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9541.19 ms /  1698 tokens (    5.62 ms per token,   177.97 tokens per second)\n",
            "llama_print_timings:        eval time =    3766.31 ms /    89 runs   (   42.32 ms per token,    23.63 tokens per second)\n",
            "llama_print_timings:       total time =   13474.15 ms /  1787 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     157.54 ms /   256 runs   (    0.62 ms per token,  1624.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1113.79 ms /   220 tokens (    5.06 ms per token,   197.52 tokens per second)\n",
            "llama_print_timings:        eval time =    9688.61 ms /   255 runs   (   37.99 ms per token,    26.32 tokens per second)\n",
            "llama_print_timings:       total time =   11177.45 ms /   475 tokens\n",
            " 17%|█▋        | 25/150 [18:25<1:33:54, 45.08s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      29.64 ms /    53 runs   (    0.56 ms per token,  1788.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5373.32 ms /  1008 tokens (    5.33 ms per token,   187.59 tokens per second)\n",
            "llama_print_timings:        eval time =    2049.56 ms /    52 runs   (   39.41 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    7500.57 ms /  1060 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1779.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5675.69 ms /  1060 tokens (    5.35 ms per token,   186.76 tokens per second)\n",
            "llama_print_timings:        eval time =      39.85 ms /     1 runs   (   39.85 ms per token,    25.10 tokens per second)\n",
            "llama_print_timings:       total time =    5737.14 ms /  1061 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      72.48 ms /   106 runs   (    0.68 ms per token,  1462.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =     705.38 ms /   139 tokens (    5.07 ms per token,   197.06 tokens per second)\n",
            "llama_print_timings:        eval time =    3992.49 ms /   105 runs   (   38.02 ms per token,    26.30 tokens per second)\n",
            "llama_print_timings:       total time =    4880.39 ms /   244 tokens\n",
            " 17%|█▋        | 26/150 [18:53<1:23:06, 40.21s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     136.57 ms /   235 runs   (    0.58 ms per token,  1720.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9713.49 ms /  1727 tokens (    5.62 ms per token,   177.79 tokens per second)\n",
            "llama_print_timings:        eval time =    9857.42 ms /   234 runs   (   42.13 ms per token,    23.74 tokens per second)\n",
            "llama_print_timings:       total time =   19974.48 ms /  1961 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     147.15 ms /   238 runs   (    0.62 ms per token,  1617.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11028.96 ms /  1930 tokens (    5.71 ms per token,   174.99 tokens per second)\n",
            "llama_print_timings:        eval time =   10155.69 ms /   237 runs   (   42.85 ms per token,    23.34 tokens per second)\n",
            "llama_print_timings:       total time =   21626.55 ms /  2167 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1669.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1635.99 ms /   328 tokens (    4.99 ms per token,   200.49 tokens per second)\n",
            "llama_print_timings:        eval time =      75.34 ms /     2 runs   (   37.67 ms per token,    26.54 tokens per second)\n",
            "llama_print_timings:       total time =    1719.39 ms /   330 tokens\n",
            " 18%|█▊        | 27/150 [19:43<1:28:27, 43.15s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     108.90 ms /   175 runs   (    0.62 ms per token,  1606.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9879.28 ms /  1760 tokens (    5.61 ms per token,   178.15 tokens per second)\n",
            "llama_print_timings:        eval time =    7347.36 ms /   174 runs   (   42.23 ms per token,    23.68 tokens per second)\n",
            "llama_print_timings:       total time =   17515.58 ms /  1934 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     135.56 ms /   226 runs   (    0.60 ms per token,  1667.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10818.92 ms /  1912 tokens (    5.66 ms per token,   176.73 tokens per second)\n",
            "llama_print_timings:        eval time =    9648.13 ms /   226 runs   (   42.69 ms per token,    23.42 tokens per second)\n",
            "llama_print_timings:       total time =   20811.28 ms /  2138 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1662.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1308.19 ms /   264 tokens (    4.96 ms per token,   201.81 tokens per second)\n",
            "llama_print_timings:        eval time =      36.89 ms /     1 runs   (   36.89 ms per token,    27.11 tokens per second)\n",
            "llama_print_timings:       total time =    1350.84 ms /   265 tokens\n",
            " 19%|█▊        | 28/150 [20:28<1:28:26, 43.50s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      15.60 ms /    29 runs   (    0.54 ms per token,  1859.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8958.06 ms /  1612 tokens (    5.56 ms per token,   179.95 tokens per second)\n",
            "llama_print_timings:        eval time =    1157.69 ms /    28 runs   (   41.35 ms per token,    24.19 tokens per second)\n",
            "llama_print_timings:       total time =   10169.31 ms /  1640 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      70.08 ms /   129 runs   (    0.54 ms per token,  1840.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9028.61 ms /  1624 tokens (    5.56 ms per token,   179.87 tokens per second)\n",
            "llama_print_timings:        eval time =    5324.51 ms /   128 runs   (   41.60 ms per token,    24.04 tokens per second)\n",
            "llama_print_timings:       total time =   14525.82 ms /  1752 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.55 ms per token,  1806.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =     624.25 ms /   127 tokens (    4.92 ms per token,   203.44 tokens per second)\n",
            "llama_print_timings:        eval time =      37.12 ms /     1 runs   (   37.12 ms per token,    26.94 tokens per second)\n",
            "llama_print_timings:       total time =     665.25 ms /   128 tokens\n",
            " 19%|█▉        | 29/150 [20:56<1:18:45, 39.06s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.40 ms /    58 runs   (    0.54 ms per token,  1846.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6845.89 ms /  1262 tokens (    5.42 ms per token,   184.34 tokens per second)\n",
            "llama_print_timings:        eval time =    2310.01 ms /    57 runs   (   40.53 ms per token,    24.68 tokens per second)\n",
            "llama_print_timings:       total time =    9243.18 ms /  1319 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      91.20 ms /   151 runs   (    0.60 ms per token,  1655.65 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6958.18 ms /  1279 tokens (    5.44 ms per token,   183.81 tokens per second)\n",
            "llama_print_timings:        eval time =    6190.52 ms /   150 runs   (   41.27 ms per token,    24.23 tokens per second)\n",
            "llama_print_timings:       total time =   13377.62 ms /  1429 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1689.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     752.54 ms /   152 tokens (    4.95 ms per token,   201.98 tokens per second)\n",
            "llama_print_timings:        eval time =      75.79 ms /     2 runs   (   37.89 ms per token,    26.39 tokens per second)\n",
            "llama_print_timings:       total time =     832.42 ms /   154 tokens\n",
            " 20%|██        | 30/150 [21:33<1:16:21, 38.18s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     153.99 ms /   242 runs   (    0.64 ms per token,  1571.51 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9633.65 ms /  1700 tokens (    5.67 ms per token,   176.46 tokens per second)\n",
            "llama_print_timings:        eval time =   10226.93 ms /   241 runs   (   42.44 ms per token,    23.57 tokens per second)\n",
            "llama_print_timings:       total time =   20255.29 ms /  1941 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      55.62 ms /    91 runs   (    0.61 ms per token,  1636.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10638.14 ms /  1867 tokens (    5.70 ms per token,   175.50 tokens per second)\n",
            "llama_print_timings:        eval time =    3832.03 ms /    90 runs   (   42.58 ms per token,    23.49 tokens per second)\n",
            "llama_print_timings:       total time =   14607.96 ms /  1957 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.26 ms /     2 runs   (    0.63 ms per token,  1592.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1593.35 ms /   318 tokens (    5.01 ms per token,   199.58 tokens per second)\n",
            "llama_print_timings:        eval time =      37.29 ms /     1 runs   (   37.29 ms per token,    26.82 tokens per second)\n",
            "llama_print_timings:       total time =    1636.97 ms /   319 tokens\n",
            " 21%|██        | 31/150 [22:15<1:18:29, 39.58s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      91.71 ms /   141 runs   (    0.65 ms per token,  1537.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4531.79 ms /   866 tokens (    5.23 ms per token,   191.09 tokens per second)\n",
            "llama_print_timings:        eval time =    5526.61 ms /   140 runs   (   39.48 ms per token,    25.33 tokens per second)\n",
            "llama_print_timings:       total time =   10266.37 ms /  1006 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      56.45 ms /    95 runs   (    0.59 ms per token,  1682.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5155.83 ms /   974 tokens (    5.29 ms per token,   188.91 tokens per second)\n",
            "llama_print_timings:        eval time =    3734.51 ms /    94 runs   (   39.73 ms per token,    25.17 tokens per second)\n",
            "llama_print_timings:       total time =    9011.36 ms /  1068 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      33.10 ms /    50 runs   (    0.66 ms per token,  1510.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1314.44 ms /   260 tokens (    5.06 ms per token,   197.80 tokens per second)\n",
            "llama_print_timings:        eval time =    1870.42 ms /    49 runs   (   38.17 ms per token,    26.20 tokens per second)\n",
            "llama_print_timings:       total time =    3251.44 ms /   309 tokens\n",
            " 21%|██▏       | 32/150 [22:49<1:14:26, 37.85s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      66.40 ms /   116 runs   (    0.57 ms per token,  1747.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6877.40 ms /  1258 tokens (    5.47 ms per token,   182.92 tokens per second)\n",
            "llama_print_timings:        eval time =    4688.68 ms /   115 runs   (   40.77 ms per token,    24.53 tokens per second)\n",
            "llama_print_timings:       total time =   11725.87 ms /  1373 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      38.00 ms /    65 runs   (    0.58 ms per token,  1710.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7553.51 ms /  1364 tokens (    5.54 ms per token,   180.58 tokens per second)\n",
            "llama_print_timings:        eval time =    2642.14 ms /    64 runs   (   41.28 ms per token,    24.22 tokens per second)\n",
            "llama_print_timings:       total time =   10291.02 ms /  1428 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      45.97 ms /    82 runs   (    0.56 ms per token,  1783.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1603.25 ms /   318 tokens (    5.04 ms per token,   198.35 tokens per second)\n",
            "llama_print_timings:        eval time =    3092.12 ms /    81 runs   (   38.17 ms per token,    26.20 tokens per second)\n",
            "llama_print_timings:       total time =    4789.27 ms /   399 tokens\n",
            " 22%|██▏       | 33/150 [23:29<1:14:59, 38.45s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      65.53 ms /   117 runs   (    0.56 ms per token,  1785.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4904.96 ms /   928 tokens (    5.29 ms per token,   189.20 tokens per second)\n",
            "llama_print_timings:        eval time =    4622.12 ms /   117 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    9673.03 ms /  1045 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      32.72 ms /    55 runs   (    0.59 ms per token,  1680.88 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5566.07 ms /  1039 tokens (    5.36 ms per token,   186.67 tokens per second)\n",
            "llama_print_timings:        eval time =    2147.83 ms /    54 runs   (   39.77 ms per token,    25.14 tokens per second)\n",
            "llama_print_timings:       total time =    7792.51 ms /  1093 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     138.25 ms /   235 runs   (    0.59 ms per token,  1699.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1145.45 ms /   227 tokens (    5.05 ms per token,   198.17 tokens per second)\n",
            "llama_print_timings:        eval time =    8862.81 ms /   234 runs   (   37.88 ms per token,    26.40 tokens per second)\n",
            "llama_print_timings:       total time =   10336.12 ms /   461 tokens\n",
            " 23%|██▎       | 34/150 [24:01<1:10:21, 36.40s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      94.30 ms /   144 runs   (    0.65 ms per token,  1527.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3700.24 ms /   718 tokens (    5.15 ms per token,   194.04 tokens per second)\n",
            "llama_print_timings:        eval time =    5591.02 ms /   143 runs   (   39.10 ms per token,    25.58 tokens per second)\n",
            "llama_print_timings:       total time =    9513.87 ms /   861 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      54.38 ms /    97 runs   (    0.56 ms per token,  1783.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4340.80 ms /   832 tokens (    5.22 ms per token,   191.67 tokens per second)\n",
            "llama_print_timings:        eval time =    3823.23 ms /    97 runs   (   39.41 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    8286.42 ms /   929 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     135.64 ms /   211 runs   (    0.64 ms per token,  1555.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1229.33 ms /   244 tokens (    5.04 ms per token,   198.48 tokens per second)\n",
            "llama_print_timings:        eval time =    8032.69 ms /   210 runs   (   38.25 ms per token,    26.14 tokens per second)\n",
            "llama_print_timings:       total time =    9561.25 ms /   454 tokens\n",
            " 23%|██▎       | 35/150 [24:35<1:08:21, 35.67s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      43.31 ms /    69 runs   (    0.63 ms per token,  1593.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6656.04 ms /  1222 tokens (    5.45 ms per token,   183.59 tokens per second)\n",
            "llama_print_timings:        eval time =    2769.12 ms /    68 runs   (   40.72 ms per token,    24.56 tokens per second)\n",
            "llama_print_timings:       total time =    9534.49 ms /  1290 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      59.86 ms /    80 runs   (    0.75 ms per token,  1336.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6894.64 ms /  1266 tokens (    5.45 ms per token,   183.62 tokens per second)\n",
            "llama_print_timings:        eval time =    3230.24 ms /    79 runs   (   40.89 ms per token,    24.46 tokens per second)\n",
            "llama_print_timings:       total time =   10264.14 ms /  1345 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.42 ms /     2 runs   (    0.71 ms per token,  1409.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =     794.24 ms /   159 tokens (    5.00 ms per token,   200.19 tokens per second)\n",
            "llama_print_timings:        eval time =      37.50 ms /     1 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =     837.34 ms /   160 tokens\n",
            " 24%|██▍       | 36/150 [24:58<1:00:45, 31.98s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      20.01 ms /    30 runs   (    0.67 ms per token,  1499.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5425.41 ms /  1019 tokens (    5.32 ms per token,   187.82 tokens per second)\n",
            "llama_print_timings:        eval time =    1165.61 ms /    29 runs   (   40.19 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =    6648.36 ms /  1048 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     143.80 ms /   236 runs   (    0.61 ms per token,  1641.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5466.09 ms /  1027 tokens (    5.32 ms per token,   187.89 tokens per second)\n",
            "llama_print_timings:        eval time =    9462.76 ms /   235 runs   (   40.27 ms per token,    24.83 tokens per second)\n",
            "llama_print_timings:       total time =   15291.50 ms /  1262 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      71.97 ms /   131 runs   (    0.55 ms per token,  1820.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     510.83 ms /   103 tokens (    4.96 ms per token,   201.63 tokens per second)\n",
            "llama_print_timings:        eval time =    4885.04 ms /   130 runs   (   37.58 ms per token,    26.61 tokens per second)\n",
            "llama_print_timings:       total time =    5550.89 ms /   233 tokens\n",
            " 25%|██▍       | 37/150 [25:34<1:02:37, 33.26s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      12.23 ms /    23 runs   (    0.53 ms per token,  1880.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5345.29 ms /   998 tokens (    5.36 ms per token,   186.71 tokens per second)\n",
            "llama_print_timings:        eval time =     872.34 ms /    22 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    6256.83 ms /  1020 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1757.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5431.45 ms /  1018 tokens (    5.34 ms per token,   187.43 tokens per second)\n",
            "llama_print_timings:        eval time =      39.30 ms /     1 runs   (   39.30 ms per token,    25.45 tokens per second)\n",
            "llama_print_timings:       total time =    5486.19 ms /  1019 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1874.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     549.22 ms /   112 tokens (    4.90 ms per token,   203.93 tokens per second)\n",
            "llama_print_timings:        eval time =      37.08 ms /     1 runs   (   37.08 ms per token,    26.97 tokens per second)\n",
            "llama_print_timings:       total time =     590.05 ms /   113 tokens\n",
            " 25%|██▌       | 38/150 [25:51<52:35, 28.17s/it]  Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      32.63 ms /    40 runs   (    0.82 ms per token,  1225.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7321.10 ms /  1341 tokens (    5.46 ms per token,   183.17 tokens per second)\n",
            "llama_print_timings:        eval time =    1603.87 ms /    39 runs   (   41.12 ms per token,    24.32 tokens per second)\n",
            "llama_print_timings:       total time =    9011.81 ms /  1380 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     157.89 ms /   256 runs   (    0.62 ms per token,  1621.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7451.23 ms /  1360 tokens (    5.48 ms per token,   182.52 tokens per second)\n",
            "llama_print_timings:        eval time =   10495.84 ms /   255 runs   (   41.16 ms per token,    24.30 tokens per second)\n",
            "llama_print_timings:       total time =   18349.25 ms /  1615 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1870.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     581.63 ms /   114 tokens (    5.10 ms per token,   196.00 tokens per second)\n",
            "llama_print_timings:        eval time =      36.42 ms /     1 runs   (   36.42 ms per token,    27.46 tokens per second)\n",
            "llama_print_timings:       total time =     621.36 ms /   115 tokens\n",
            " 26%|██▌       | 39/150 [26:27<56:54, 30.76s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      36.49 ms /    64 runs   (    0.57 ms per token,  1754.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5302.68 ms /  1006 tokens (    5.27 ms per token,   189.72 tokens per second)\n",
            "llama_print_timings:        eval time =    2483.50 ms /    63 runs   (   39.42 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    7872.25 ms /  1069 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      86.20 ms /   144 runs   (    0.60 ms per token,  1670.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5656.33 ms /  1058 tokens (    5.35 ms per token,   187.05 tokens per second)\n",
            "llama_print_timings:        eval time =    5719.07 ms /   143 runs   (   39.99 ms per token,    25.00 tokens per second)\n",
            "llama_print_timings:       total time =   11564.04 ms /  1201 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.23 ms /     2 runs   (    0.61 ms per token,  1631.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     743.66 ms /   149 tokens (    4.99 ms per token,   200.36 tokens per second)\n",
            "llama_print_timings:        eval time =      36.89 ms /     1 runs   (   36.89 ms per token,    27.11 tokens per second)\n",
            "llama_print_timings:       total time =     785.64 ms /   150 tokens\n",
            " 27%|██▋       | 40/150 [26:56<54:58, 29.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     157.46 ms /   256 runs   (    0.62 ms per token,  1625.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4343.26 ms /   832 tokens (    5.22 ms per token,   191.56 tokens per second)\n",
            "llama_print_timings:        eval time =   10132.52 ms /   256 runs   (   39.58 ms per token,    25.27 tokens per second)\n",
            "llama_print_timings:       total time =   14853.62 ms /  1088 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      30.71 ms /    55 runs   (    0.56 ms per token,  1791.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5641.39 ms /  1059 tokens (    5.33 ms per token,   187.72 tokens per second)\n",
            "llama_print_timings:        eval time =    2140.99 ms /    54 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    7856.40 ms /  1113 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      55.30 ms /    82 runs   (    0.67 ms per token,  1482.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1683.17 ms /   335 tokens (    5.02 ms per token,   199.03 tokens per second)\n",
            "llama_print_timings:        eval time =    3114.30 ms /    81 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
            "llama_print_timings:       total time =    4922.55 ms /   416 tokens\n",
            " 27%|██▋       | 41/150 [27:31<57:18, 31.55s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      81.07 ms /   147 runs   (    0.55 ms per token,  1813.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8608.98 ms /  1558 tokens (    5.53 ms per token,   180.97 tokens per second)\n",
            "llama_print_timings:        eval time =    6014.84 ms /   146 runs   (   41.20 ms per token,    24.27 tokens per second)\n",
            "llama_print_timings:       total time =   14821.41 ms /  1704 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      58.29 ms /   105 runs   (    0.56 ms per token,  1801.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9347.84 ms /  1674 tokens (    5.58 ms per token,   179.08 tokens per second)\n",
            "llama_print_timings:        eval time =    4320.36 ms /   104 runs   (   41.54 ms per token,    24.07 tokens per second)\n",
            "llama_print_timings:       total time =   13814.65 ms /  1778 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      56.70 ms /    91 runs   (    0.62 ms per token,  1604.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1103.83 ms /   224 tokens (    4.93 ms per token,   202.93 tokens per second)\n",
            "llama_print_timings:        eval time =    3429.35 ms /    91 runs   (   37.69 ms per token,    26.54 tokens per second)\n",
            "llama_print_timings:       total time =    4652.60 ms /   315 tokens\n",
            " 28%|██▊       | 42/150 [28:11<1:01:16, 34.04s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      25.06 ms /    49 runs   (    0.51 ms per token,  1955.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8579.74 ms /  1559 tokens (    5.50 ms per token,   181.71 tokens per second)\n",
            "llama_print_timings:        eval time =    1967.20 ms /    48 runs   (   40.98 ms per token,    24.40 tokens per second)\n",
            "llama_print_timings:       total time =   10621.96 ms /  1607 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      66.92 ms /   120 runs   (    0.56 ms per token,  1793.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8783.55 ms /  1591 tokens (    5.52 ms per token,   181.13 tokens per second)\n",
            "llama_print_timings:        eval time =    4915.37 ms /   119 runs   (   41.31 ms per token,    24.21 tokens per second)\n",
            "llama_print_timings:       total time =   13865.60 ms /  1710 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.50 ms per token,  1990.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     817.09 ms /   163 tokens (    5.01 ms per token,   199.49 tokens per second)\n",
            "llama_print_timings:        eval time =      36.72 ms /     1 runs   (   36.72 ms per token,    27.23 tokens per second)\n",
            "llama_print_timings:       total time =     858.27 ms /   164 tokens\n",
            " 29%|██▊       | 43/150 [28:47<1:01:51, 34.69s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      35.59 ms /    63 runs   (    0.56 ms per token,  1770.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6865.56 ms /  1267 tokens (    5.42 ms per token,   184.54 tokens per second)\n",
            "llama_print_timings:        eval time =    2491.68 ms /    62 runs   (   40.19 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =    9444.29 ms /  1329 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      73.57 ms /   109 runs   (    0.67 ms per token,  1481.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7117.30 ms /  1317 tokens (    5.40 ms per token,   185.04 tokens per second)\n",
            "llama_print_timings:        eval time =    4404.18 ms /   108 runs   (   40.78 ms per token,    24.52 tokens per second)\n",
            "llama_print_timings:       total time =   11696.46 ms /  1425 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1893.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     664.45 ms /   136 tokens (    4.89 ms per token,   204.68 tokens per second)\n",
            "llama_print_timings:        eval time =      73.69 ms /     2 runs   (   36.84 ms per token,    27.14 tokens per second)\n",
            "llama_print_timings:       total time =     741.23 ms /   138 tokens\n",
            " 29%|██▉       | 44/150 [29:11<55:56, 31.66s/it]  Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      24.74 ms /    39 runs   (    0.63 ms per token,  1576.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6193.19 ms /  1160 tokens (    5.34 ms per token,   187.30 tokens per second)\n",
            "llama_print_timings:        eval time =    1527.12 ms /    38 runs   (   40.19 ms per token,    24.88 tokens per second)\n",
            "llama_print_timings:       total time =    7787.85 ms /  1198 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.11 ms /     2 runs   (    0.56 ms per token,  1795.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6364.61 ms /  1192 tokens (    5.34 ms per token,   187.29 tokens per second)\n",
            "llama_print_timings:        eval time =      80.28 ms /     2 runs   (   40.14 ms per token,    24.91 tokens per second)\n",
            "llama_print_timings:       total time =    6464.68 ms /  1194 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1972.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =     544.86 ms /   111 tokens (    4.91 ms per token,   203.72 tokens per second)\n",
            "llama_print_timings:        eval time =      36.64 ms /     1 runs   (   36.64 ms per token,    27.30 tokens per second)\n",
            "llama_print_timings:       total time =     585.21 ms /   112 tokens\n",
            " 30%|███       | 45/150 [29:30<48:47, 27.89s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.64 ms /    61 runs   (    0.52 ms per token,  1927.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10440.04 ms /  1844 tokens (    5.66 ms per token,   176.63 tokens per second)\n",
            "llama_print_timings:        eval time =    2544.88 ms /    60 runs   (   42.41 ms per token,    23.58 tokens per second)\n",
            "llama_print_timings:       total time =   13075.47 ms /  1904 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      75.37 ms /   128 runs   (    0.59 ms per token,  1698.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10679.86 ms /  1864 tokens (    5.73 ms per token,   174.53 tokens per second)\n",
            "llama_print_timings:        eval time =    5506.38 ms /   128 runs   (   43.02 ms per token,    23.25 tokens per second)\n",
            "llama_print_timings:       total time =   16379.20 ms /  1992 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1519.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =     671.32 ms /   136 tokens (    4.94 ms per token,   202.59 tokens per second)\n",
            "llama_print_timings:        eval time =      75.14 ms /     2 runs   (   37.57 ms per token,    26.62 tokens per second)\n",
            "llama_print_timings:       total time =     750.91 ms /   138 tokens\n",
            " 31%|███       | 46/150 [30:11<54:40, 31.55s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.32 ms /    56 runs   (    0.56 ms per token,  1787.82 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10966.32 ms /  1915 tokens (    5.73 ms per token,   174.63 tokens per second)\n",
            "llama_print_timings:        eval time =    2350.17 ms /    55 runs   (   42.73 ms per token,    23.40 tokens per second)\n",
            "llama_print_timings:       total time =   13410.33 ms /  1970 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     157.23 ms /   256 runs   (    0.61 ms per token,  1628.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10957.58 ms /  1915 tokens (    5.72 ms per token,   174.76 tokens per second)\n",
            "llama_print_timings:        eval time =   10936.92 ms /   255 runs   (   42.89 ms per token,    23.32 tokens per second)\n",
            "llama_print_timings:       total time =   22323.10 ms /  2170 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.35 ms /     2 runs   (    0.68 ms per token,  1481.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =     671.49 ms /   135 tokens (    4.97 ms per token,   201.05 tokens per second)\n",
            "llama_print_timings:        eval time =      37.59 ms /     1 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =     713.79 ms /   136 tokens\n",
            " 31%|███▏      | 47/150 [30:52<59:18, 34.55s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      75.57 ms /   128 runs   (    0.59 ms per token,  1693.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8408.23 ms /  1526 tokens (    5.51 ms per token,   181.49 tokens per second)\n",
            "llama_print_timings:        eval time =    5261.77 ms /   127 runs   (   41.43 ms per token,    24.14 tokens per second)\n",
            "llama_print_timings:       total time =   13850.70 ms /  1653 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     119.54 ms /   213 runs   (    0.56 ms per token,  1781.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9071.09 ms /  1623 tokens (    5.59 ms per token,   178.92 tokens per second)\n",
            "llama_print_timings:        eval time =    8938.77 ms /   212 runs   (   42.16 ms per token,    23.72 tokens per second)\n",
            "llama_print_timings:       total time =   18314.71 ms /  1835 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      43.82 ms /    64 runs   (    0.68 ms per token,  1460.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1154.30 ms /   227 tokens (    5.08 ms per token,   196.66 tokens per second)\n",
            "llama_print_timings:        eval time =    2411.16 ms /    63 runs   (   38.27 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =    3654.64 ms /   290 tokens\n",
            " 32%|███▏      | 48/150 [31:39<1:04:46, 38.11s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     136.97 ms /   256 runs   (    0.54 ms per token,  1869.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8323.66 ms /  1504 tokens (    5.53 ms per token,   180.69 tokens per second)\n",
            "llama_print_timings:        eval time =   10637.72 ms /   255 runs   (   41.72 ms per token,    23.97 tokens per second)\n",
            "llama_print_timings:       total time =   19330.46 ms /  1759 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     152.78 ms /   256 runs   (    0.60 ms per token,  1675.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9633.64 ms /  1712 tokens (    5.63 ms per token,   177.71 tokens per second)\n",
            "llama_print_timings:        eval time =   10817.98 ms /   255 runs   (   42.42 ms per token,    23.57 tokens per second)\n",
            "llama_print_timings:       total time =   20847.19 ms /  1967 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1970.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1678.03 ms /   333 tokens (    5.04 ms per token,   198.45 tokens per second)\n",
            "llama_print_timings:        eval time =      37.63 ms /     1 runs   (   37.63 ms per token,    26.57 tokens per second)\n",
            "llama_print_timings:       total time =    1722.18 ms /   334 tokens\n",
            " 33%|███▎      | 49/150 [32:30<1:10:46, 42.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      32.41 ms /    54 runs   (    0.60 ms per token,  1666.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12321.49 ms /  2132 tokens (    5.78 ms per token,   173.03 tokens per second)\n",
            "llama_print_timings:        eval time =    2295.58 ms /    53 runs   (   43.31 ms per token,    23.09 tokens per second)\n",
            "llama_print_timings:       total time =   14716.36 ms /  2185 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     149.27 ms /   256 runs   (    0.58 ms per token,  1714.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12458.02 ms /  2152 tokens (    5.79 ms per token,   172.74 tokens per second)\n",
            "llama_print_timings:        eval time =   11125.53 ms /   256 runs   (   43.46 ms per token,    23.01 tokens per second)\n",
            "llama_print_timings:       total time =   23981.53 ms /  2408 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       2.00 ms /     2 runs   (    1.00 ms per token,  1000.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     679.25 ms /   133 tokens (    5.11 ms per token,   195.81 tokens per second)\n",
            "llama_print_timings:        eval time =      37.01 ms /     1 runs   (   37.01 ms per token,    27.02 tokens per second)\n",
            "llama_print_timings:       total time =     722.63 ms /   134 tokens\n",
            " 33%|███▎      | 50/150 [33:16<1:12:15, 43.35s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     142.65 ms /   256 runs   (    0.56 ms per token,  1794.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8076.65 ms /  1471 tokens (    5.49 ms per token,   182.13 tokens per second)\n",
            "llama_print_timings:        eval time =   10554.04 ms /   255 runs   (   41.39 ms per token,    24.16 tokens per second)\n",
            "llama_print_timings:       total time =   18997.06 ms /  1726 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.33 ms /    79 runs   (    0.60 ms per token,  1669.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9443.38 ms /  1687 tokens (    5.60 ms per token,   178.64 tokens per second)\n",
            "llama_print_timings:        eval time =    3252.81 ms /    78 runs   (   41.70 ms per token,    23.98 tokens per second)\n",
            "llama_print_timings:       total time =   12819.28 ms /  1765 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1707.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1838.71 ms /   368 tokens (    5.00 ms per token,   200.14 tokens per second)\n",
            "llama_print_timings:        eval time =      76.00 ms /     2 runs   (   38.00 ms per token,    26.32 tokens per second)\n",
            "llama_print_timings:       total time =    1921.58 ms /   370 tokens\n",
            " 34%|███▍      | 51/150 [33:57<1:10:04, 42.47s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      70.27 ms /   113 runs   (    0.62 ms per token,  1608.13 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4978.41 ms /   942 tokens (    5.28 ms per token,   189.22 tokens per second)\n",
            "llama_print_timings:        eval time =    4422.39 ms /   112 runs   (   39.49 ms per token,    25.33 tokens per second)\n",
            "llama_print_timings:       total time =    9565.98 ms /  1054 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     152.34 ms /   256 runs   (    0.60 ms per token,  1680.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5476.78 ms /  1030 tokens (    5.32 ms per token,   188.07 tokens per second)\n",
            "llama_print_timings:        eval time =   10324.01 ms /   255 runs   (   40.49 ms per token,    24.70 tokens per second)\n",
            "llama_print_timings:       total time =   16178.67 ms /  1285 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1867.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1275.33 ms /   254 tokens (    5.02 ms per token,   199.16 tokens per second)\n",
            "llama_print_timings:        eval time =      37.02 ms /     1 runs   (   37.02 ms per token,    27.02 tokens per second)\n",
            "llama_print_timings:       total time =    1317.63 ms /   255 tokens\n",
            " 35%|███▍      | 52/150 [34:35<1:07:19, 41.22s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      27.75 ms /    41 runs   (    0.68 ms per token,  1477.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8359.65 ms /  1508 tokens (    5.54 ms per token,   180.39 tokens per second)\n",
            "llama_print_timings:        eval time =    1666.43 ms /    40 runs   (   41.66 ms per token,    24.00 tokens per second)\n",
            "llama_print_timings:       total time =   10102.34 ms /  1548 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      91.73 ms /   155 runs   (    0.59 ms per token,  1689.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8447.03 ms /  1520 tokens (    5.56 ms per token,   179.94 tokens per second)\n",
            "llama_print_timings:        eval time =    6456.33 ms /   155 runs   (   41.65 ms per token,    24.01 tokens per second)\n",
            "llama_print_timings:       total time =   15134.05 ms /  1675 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.22 ms /     2 runs   (    0.61 ms per token,  1646.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =     727.45 ms /   139 tokens (    5.23 ms per token,   191.08 tokens per second)\n",
            "llama_print_timings:        eval time =      37.36 ms /     1 runs   (   37.36 ms per token,    26.77 tokens per second)\n",
            "llama_print_timings:       total time =     769.31 ms /   140 tokens\n",
            " 35%|███▌      | 53/150 [35:09<1:03:00, 38.98s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.57 ms /    71 runs   (    0.67 ms per token,  1492.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6349.59 ms /  1176 tokens (    5.40 ms per token,   185.21 tokens per second)\n",
            "llama_print_timings:        eval time =    2844.79 ms /    70 runs   (   40.64 ms per token,    24.61 tokens per second)\n",
            "llama_print_timings:       total time =    9318.12 ms /  1246 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     156.12 ms /   253 runs   (    0.62 ms per token,  1620.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6606.61 ms /  1224 tokens (    5.40 ms per token,   185.27 tokens per second)\n",
            "llama_print_timings:        eval time =   10392.44 ms /   253 runs   (   41.08 ms per token,    24.34 tokens per second)\n",
            "llama_print_timings:       total time =   17374.54 ms /  1477 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.03 ms /     2 runs   (    0.51 ms per token,  1949.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =     867.97 ms /   173 tokens (    5.02 ms per token,   199.31 tokens per second)\n",
            "llama_print_timings:        eval time =      38.16 ms /     1 runs   (   38.16 ms per token,    26.21 tokens per second)\n",
            "llama_print_timings:       total time =     910.74 ms /   174 tokens\n",
            " 36%|███▌      | 54/150 [35:46<1:01:49, 38.64s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.31 ms /    74 runs   (    0.63 ms per token,  1598.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8048.56 ms /  1464 tokens (    5.50 ms per token,   181.90 tokens per second)\n",
            "llama_print_timings:        eval time =    3023.72 ms /    73 runs   (   41.42 ms per token,    24.14 tokens per second)\n",
            "llama_print_timings:       total time =   11191.98 ms /  1537 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1928.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8407.62 ms /  1517 tokens (    5.54 ms per token,   180.43 tokens per second)\n",
            "llama_print_timings:        eval time =      41.06 ms /     1 runs   (   41.06 ms per token,    24.36 tokens per second)\n",
            "llama_print_timings:       total time =    8467.94 ms /  1518 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      23.69 ms /    39 runs   (    0.61 ms per token,  1646.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     998.38 ms /   195 tokens (    5.12 ms per token,   195.32 tokens per second)\n",
            "llama_print_timings:        eval time =    1451.28 ms /    38 runs   (   38.19 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    2506.02 ms /   233 tokens\n",
            " 37%|███▋      | 55/150 [36:17<57:30, 36.33s/it]  Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      60.07 ms /   104 runs   (    0.58 ms per token,  1731.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8953.38 ms /  1607 tokens (    5.57 ms per token,   179.49 tokens per second)\n",
            "llama_print_timings:        eval time =    4303.49 ms /   103 runs   (   41.78 ms per token,    23.93 tokens per second)\n",
            "llama_print_timings:       total time =   13403.11 ms /  1710 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      94.90 ms /   157 runs   (    0.60 ms per token,  1654.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9484.59 ms /  1688 tokens (    5.62 ms per token,   177.97 tokens per second)\n",
            "llama_print_timings:        eval time =    6574.09 ms /   156 runs   (   42.14 ms per token,    23.73 tokens per second)\n",
            "llama_print_timings:       total time =   16280.35 ms /  1844 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      26.70 ms /    46 runs   (    0.58 ms per token,  1722.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1024.59 ms /   203 tokens (    5.05 ms per token,   198.13 tokens per second)\n",
            "llama_print_timings:        eval time =    1687.66 ms /    45 runs   (   37.50 ms per token,    26.66 tokens per second)\n",
            "llama_print_timings:       total time =    2762.64 ms /   248 tokens\n",
            " 37%|███▋      | 56/150 [36:56<58:07, 37.10s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      67.38 ms /   134 runs   (    0.50 ms per token,  1988.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9548.68 ms /  1701 tokens (    5.61 ms per token,   178.14 tokens per second)\n",
            "llama_print_timings:        eval time =    5592.29 ms /   133 runs   (   42.05 ms per token,    23.78 tokens per second)\n",
            "llama_print_timings:       total time =   15320.09 ms /  1834 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      62.77 ms /   117 runs   (    0.54 ms per token,  1863.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10203.01 ms /  1790 tokens (    5.70 ms per token,   175.44 tokens per second)\n",
            "llama_print_timings:        eval time =    4932.42 ms /   116 runs   (   42.52 ms per token,    23.52 tokens per second)\n",
            "llama_print_timings:       total time =   15296.35 ms /  1906 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.63 ms per token,  1577.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1080.04 ms /   210 tokens (    5.14 ms per token,   194.44 tokens per second)\n",
            "llama_print_timings:        eval time =      37.50 ms /     1 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =    1123.76 ms /   211 tokens\n",
            " 38%|███▊      | 57/150 [37:40<1:00:31, 39.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     142.53 ms /   256 runs   (    0.56 ms per token,  1796.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13158.26 ms /  2251 tokens (    5.85 ms per token,   171.07 tokens per second)\n",
            "llama_print_timings:        eval time =   11412.63 ms /   255 runs   (   44.76 ms per token,    22.34 tokens per second)\n",
            "llama_print_timings:       total time =   24975.17 ms /  2506 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     137.28 ms /   256 runs   (    0.54 ms per token,  1864.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14511.52 ms /  2445 tokens (    5.94 ms per token,   168.49 tokens per second)\n",
            "llama_print_timings:        eval time =   12295.77 ms /   255 runs   (   48.22 ms per token,    20.74 tokens per second)\n",
            "llama_print_timings:       total time =   27211.20 ms /  2700 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1968.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1680.49 ms /   335 tokens (    5.02 ms per token,   199.35 tokens per second)\n",
            "llama_print_timings:        eval time =      37.50 ms /     1 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =    1724.66 ms /   336 tokens\n",
            " 39%|███▊      | 58/150 [38:44<1:11:25, 46.58s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      13.16 ms /    26 runs   (    0.51 ms per token,  1975.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8795.03 ms /  1578 tokens (    5.57 ms per token,   179.42 tokens per second)\n",
            "llama_print_timings:        eval time =    1034.91 ms /    25 runs   (   41.40 ms per token,    24.16 tokens per second)\n",
            "llama_print_timings:       total time =    9880.24 ms /  1603 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.39 ms /    71 runs   (    0.53 ms per token,  1898.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8675.90 ms /  1560 tokens (    5.56 ms per token,   179.81 tokens per second)\n",
            "llama_print_timings:        eval time =    2954.63 ms /    71 runs   (   41.61 ms per token,    24.03 tokens per second)\n",
            "llama_print_timings:       total time =   11726.24 ms /  1631 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.96 ms /     2 runs   (    0.48 ms per token,  2089.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =     508.97 ms /   104 tokens (    4.89 ms per token,   204.33 tokens per second)\n",
            "llama_print_timings:        eval time =      36.80 ms /     1 runs   (   36.80 ms per token,    27.18 tokens per second)\n",
            "llama_print_timings:       total time =     548.68 ms /   105 tokens\n",
            " 39%|███▉      | 59/150 [39:13<1:02:33, 41.24s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      21.07 ms /    34 runs   (    0.62 ms per token,  1613.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12305.31 ms /  2123 tokens (    5.80 ms per token,   172.53 tokens per second)\n",
            "llama_print_timings:        eval time =    1436.75 ms /    33 runs   (   43.54 ms per token,    22.97 tokens per second)\n",
            "llama_print_timings:       total time =   13813.79 ms /  2156 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.37 ms /    72 runs   (    0.57 ms per token,  1740.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12329.30 ms /  2115 tokens (    5.83 ms per token,   171.54 tokens per second)\n",
            "llama_print_timings:        eval time =    3094.76 ms /    71 runs   (   43.59 ms per token,    22.94 tokens per second)\n",
            "llama_print_timings:       total time =   15549.30 ms /  2186 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.52 ms per token,  1908.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =     585.27 ms /   114 tokens (    5.13 ms per token,   194.78 tokens per second)\n",
            "llama_print_timings:        eval time =      36.69 ms /     1 runs   (   36.69 ms per token,    27.26 tokens per second)\n",
            "llama_print_timings:       total time =     625.81 ms /   115 tokens\n",
            " 40%|████      | 60/150 [39:52<1:00:52, 40.58s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      55.73 ms /    99 runs   (    0.56 ms per token,  1776.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8346.34 ms /  1495 tokens (    5.58 ms per token,   179.12 tokens per second)\n",
            "llama_print_timings:        eval time =    4046.65 ms /    98 runs   (   41.29 ms per token,    24.22 tokens per second)\n",
            "llama_print_timings:       total time =   12535.13 ms /  1593 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1512.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8764.92 ms /  1572 tokens (    5.58 ms per token,   179.35 tokens per second)\n",
            "llama_print_timings:        eval time =      41.50 ms /     1 runs   (   41.50 ms per token,    24.10 tokens per second)\n",
            "llama_print_timings:       total time =    8829.02 ms /  1573 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.73 ms /    70 runs   (    0.54 ms per token,  1855.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1280.57 ms /   250 tokens (    5.12 ms per token,   195.23 tokens per second)\n",
            "llama_print_timings:        eval time =    2600.03 ms /    69 runs   (   37.68 ms per token,    26.54 tokens per second)\n",
            "llama_print_timings:       total time =    3956.16 ms /   319 tokens\n",
            " 41%|████      | 61/150 [40:24<56:18, 37.96s/it]  Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      18.37 ms /    32 runs   (    0.57 ms per token,  1741.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5773.79 ms /  1075 tokens (    5.37 ms per token,   186.19 tokens per second)\n",
            "llama_print_timings:        eval time =    1236.51 ms /    31 runs   (   39.89 ms per token,    25.07 tokens per second)\n",
            "llama_print_timings:       total time =    7060.89 ms /  1106 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     129.13 ms /   203 runs   (    0.64 ms per token,  1572.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5842.41 ms /  1095 tokens (    5.34 ms per token,   187.42 tokens per second)\n",
            "llama_print_timings:        eval time =    8193.87 ms /   202 runs   (   40.56 ms per token,    24.65 tokens per second)\n",
            "llama_print_timings:       total time =   14332.81 ms /  1297 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      25.00 ms /    45 runs   (    0.56 ms per token,  1800.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =     709.24 ms /   140 tokens (    5.07 ms per token,   197.39 tokens per second)\n",
            "llama_print_timings:        eval time =    1651.95 ms /    44 runs   (   37.54 ms per token,    26.64 tokens per second)\n",
            "llama_print_timings:       total time =    2408.45 ms /   184 tokens\n",
            " 41%|████▏     | 62/150 [40:55<52:48, 36.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     116.31 ms /   193 runs   (    0.60 ms per token,  1659.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11952.92 ms /  2067 tokens (    5.78 ms per token,   172.93 tokens per second)\n",
            "llama_print_timings:        eval time =    8341.15 ms /   192 runs   (   43.44 ms per token,    23.02 tokens per second)\n",
            "llama_print_timings:       total time =   20592.76 ms /  2259 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     147.32 ms /   256 runs   (    0.58 ms per token,  1737.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13043.90 ms /  2229 tokens (    5.85 ms per token,   170.88 tokens per second)\n",
            "llama_print_timings:        eval time =   11249.59 ms /   255 runs   (   44.12 ms per token,    22.67 tokens per second)\n",
            "llama_print_timings:       total time =   24717.72 ms /  2484 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      23.01 ms /    45 runs   (    0.51 ms per token,  1955.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1758.24 ms /   351 tokens (    5.01 ms per token,   199.63 tokens per second)\n",
            "llama_print_timings:        eval time =    1666.23 ms /    44 runs   (   37.87 ms per token,    26.41 tokens per second)\n",
            "llama_print_timings:       total time =    3474.62 ms /   395 tokens\n",
            " 42%|████▏     | 63/150 [41:49<59:46, 41.22s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      75.61 ms /   119 runs   (    0.64 ms per token,  1573.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11545.83 ms /  2018 tokens (    5.72 ms per token,   174.78 tokens per second)\n",
            "llama_print_timings:        eval time =    5094.86 ms /   118 runs   (   43.18 ms per token,    23.16 tokens per second)\n",
            "llama_print_timings:       total time =   16833.85 ms /  2136 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.04 ms /    84 runs   (    0.55 ms per token,  1824.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12385.24 ms /  2128 tokens (    5.82 ms per token,   171.82 tokens per second)\n",
            "llama_print_timings:        eval time =    3649.58 ms /    84 runs   (   43.45 ms per token,    23.02 tokens per second)\n",
            "llama_print_timings:       total time =   16159.37 ms /  2212 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      44.88 ms /    81 runs   (    0.55 ms per token,  1804.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1230.11 ms /   243 tokens (    5.06 ms per token,   197.54 tokens per second)\n",
            "llama_print_timings:        eval time =    3021.33 ms /    80 runs   (   37.77 ms per token,    26.48 tokens per second)\n",
            "llama_print_timings:       total time =    4339.33 ms /   323 tokens\n",
            " 43%|████▎     | 64/150 [42:35<1:01:09, 42.67s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.41 ms /    71 runs   (    0.57 ms per token,  1757.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3888.80 ms /   747 tokens (    5.21 ms per token,   192.09 tokens per second)\n",
            "llama_print_timings:        eval time =    2741.78 ms /    70 runs   (   39.17 ms per token,    25.53 tokens per second)\n",
            "llama_print_timings:       total time =    6715.57 ms /   817 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      51.15 ms /    89 runs   (    0.57 ms per token,  1740.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4289.55 ms /   815 tokens (    5.26 ms per token,   190.00 tokens per second)\n",
            "llama_print_timings:        eval time =    3464.64 ms /    88 runs   (   39.37 ms per token,    25.40 tokens per second)\n",
            "llama_print_timings:       total time =    7872.35 ms /   903 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      44.64 ms /    83 runs   (    0.54 ms per token,  1859.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     864.24 ms /   173 tokens (    5.00 ms per token,   200.17 tokens per second)\n",
            "llama_print_timings:        eval time =    3080.36 ms /    82 runs   (   37.57 ms per token,    26.62 tokens per second)\n",
            "llama_print_timings:       total time =    4033.96 ms /   255 tokens\n",
            " 43%|████▎     | 65/150 [42:59<52:43, 37.22s/it]  Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      16.21 ms /    28 runs   (    0.58 ms per token,  1727.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4872.71 ms /   928 tokens (    5.25 ms per token,   190.45 tokens per second)\n",
            "llama_print_timings:        eval time =    1064.12 ms /    27 runs   (   39.41 ms per token,    25.37 tokens per second)\n",
            "llama_print_timings:       total time =    5977.41 ms /   955 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      23.77 ms /    34 runs   (    0.70 ms per token,  1430.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5049.05 ms /   947 tokens (    5.33 ms per token,   187.56 tokens per second)\n",
            "llama_print_timings:        eval time =    1314.81 ms /    33 runs   (   39.84 ms per token,    25.10 tokens per second)\n",
            "llama_print_timings:       total time =    6426.02 ms /   980 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     144.05 ms /   256 runs   (    0.56 ms per token,  1777.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =     669.10 ms /   135 tokens (    4.96 ms per token,   201.76 tokens per second)\n",
            "llama_print_timings:        eval time =    9628.41 ms /   255 runs   (   37.76 ms per token,    26.48 tokens per second)\n",
            "llama_print_timings:       total time =   10616.45 ms /   390 tokens\n",
            " 44%|████▍     | 66/150 [43:30<49:18, 35.22s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      29.72 ms /    39 runs   (    0.76 ms per token,  1312.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7695.90 ms /  1408 tokens (    5.47 ms per token,   182.95 tokens per second)\n",
            "llama_print_timings:        eval time =    1569.44 ms /    38 runs   (   41.30 ms per token,    24.21 tokens per second)\n",
            "llama_print_timings:       total time =    9341.13 ms /  1446 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.08 ms /     2 runs   (    0.54 ms per token,  1851.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7896.29 ms /  1431 tokens (    5.52 ms per token,   181.22 tokens per second)\n",
            "llama_print_timings:        eval time =      40.78 ms /     1 runs   (   40.78 ms per token,    24.52 tokens per second)\n",
            "llama_print_timings:       total time =    7958.62 ms /  1432 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      12.97 ms /    26 runs   (    0.50 ms per token,  2004.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     742.91 ms /   146 tokens (    5.09 ms per token,   196.53 tokens per second)\n",
            "llama_print_timings:        eval time =     937.58 ms /    25 runs   (   37.50 ms per token,    26.66 tokens per second)\n",
            "llama_print_timings:       total time =    1707.53 ms /   171 tokens\n",
            " 45%|████▍     | 67/150 [43:54<44:20, 32.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      73.60 ms /   144 runs   (    0.51 ms per token,  1956.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11697.88 ms /  2034 tokens (    5.75 ms per token,   173.88 tokens per second)\n",
            "llama_print_timings:        eval time =    6171.37 ms /   143 runs   (   43.16 ms per token,    23.17 tokens per second)\n",
            "llama_print_timings:       total time =   18076.29 ms /  2177 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      99.68 ms /   166 runs   (    0.60 ms per token,  1665.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12317.21 ms /  2098 tokens (    5.87 ms per token,   170.33 tokens per second)\n",
            "llama_print_timings:        eval time =    7203.86 ms /   165 runs   (   43.66 ms per token,    22.90 tokens per second)\n",
            "llama_print_timings:       total time =   19802.52 ms /  2263 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1974.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1108.86 ms /   220 tokens (    5.04 ms per token,   198.40 tokens per second)\n",
            "llama_print_timings:        eval time =      37.07 ms /     1 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
            "llama_print_timings:       total time =    1151.15 ms /   221 tokens\n",
            " 45%|████▌     | 68/150 [44:45<51:27, 37.65s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     147.11 ms /   256 runs   (    0.57 ms per token,  1740.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12850.52 ms /  2206 tokens (    5.83 ms per token,   171.67 tokens per second)\n",
            "llama_print_timings:        eval time =   11154.24 ms /   255 runs   (   43.74 ms per token,    22.86 tokens per second)\n",
            "llama_print_timings:       total time =   24416.43 ms /  2461 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     143.47 ms /   256 runs   (    0.56 ms per token,  1784.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14435.67 ms /  2435 tokens (    5.93 ms per token,   168.68 tokens per second)\n",
            "llama_print_timings:        eval time =   12224.99 ms /   255 runs   (   47.94 ms per token,    20.86 tokens per second)\n",
            "llama_print_timings:       total time =   27084.23 ms /  2690 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.57 ms per token,  1763.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1678.17 ms /   334 tokens (    5.02 ms per token,   199.03 tokens per second)\n",
            "llama_print_timings:        eval time =      37.38 ms /     1 runs   (   37.38 ms per token,    26.75 tokens per second)\n",
            "llama_print_timings:       total time =    1722.20 ms /   335 tokens\n",
            " 46%|████▌     | 69/150 [45:45<59:39, 44.19s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     109.04 ms /   180 runs   (    0.61 ms per token,  1650.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9938.49 ms /  1765 tokens (    5.63 ms per token,   177.59 tokens per second)\n",
            "llama_print_timings:        eval time =    7596.99 ms /   179 runs   (   42.44 ms per token,    23.56 tokens per second)\n",
            "llama_print_timings:       total time =   17822.04 ms /  1944 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      54.54 ms /    86 runs   (    0.63 ms per token,  1576.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10749.32 ms /  1884 tokens (    5.71 ms per token,   175.27 tokens per second)\n",
            "llama_print_timings:        eval time =    3641.28 ms /    85 runs   (   42.84 ms per token,    23.34 tokens per second)\n",
            "llama_print_timings:       total time =   14534.06 ms /  1969 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1960.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1272.38 ms /   255 tokens (    4.99 ms per token,   200.41 tokens per second)\n",
            "llama_print_timings:        eval time =      37.42 ms /     1 runs   (   37.42 ms per token,    26.72 tokens per second)\n",
            "llama_print_timings:       total time =    1314.52 ms /   256 tokens\n",
            " 47%|████▋     | 70/150 [46:25<57:20, 43.01s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      58.41 ms /   113 runs   (    0.52 ms per token,  1934.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8232.04 ms /  1495 tokens (    5.51 ms per token,   181.61 tokens per second)\n",
            "llama_print_timings:        eval time =    4638.89 ms /   112 runs   (   41.42 ms per token,    24.14 tokens per second)\n",
            "llama_print_timings:       total time =   13021.24 ms /  1607 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.65 ms /    63 runs   (    0.65 ms per token,  1549.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8405.23 ms /  1510 tokens (    5.57 ms per token,   179.65 tokens per second)\n",
            "llama_print_timings:        eval time =    2607.83 ms /    62 runs   (   42.06 ms per token,    23.77 tokens per second)\n",
            "llama_print_timings:       total time =   11121.90 ms /  1572 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1860.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     952.44 ms /   190 tokens (    5.01 ms per token,   199.49 tokens per second)\n",
            "llama_print_timings:        eval time =      37.73 ms /     1 runs   (   37.73 ms per token,    26.50 tokens per second)\n",
            "llama_print_timings:       total time =     996.31 ms /   191 tokens\n",
            " 47%|████▋     | 71/150 [47:05<55:26, 42.11s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     138.54 ms /   256 runs   (    0.54 ms per token,  1847.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10302.65 ms /  1824 tokens (    5.65 ms per token,   177.04 tokens per second)\n",
            "llama_print_timings:        eval time =   10963.75 ms /   256 runs   (   42.83 ms per token,    23.35 tokens per second)\n",
            "llama_print_timings:       total time =   21622.65 ms /  2080 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.98 ms /    61 runs   (    0.52 ms per token,  1907.44 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11731.57 ms /  2032 tokens (    5.77 ms per token,   173.21 tokens per second)\n",
            "llama_print_timings:        eval time =    2575.45 ms /    60 runs   (   42.92 ms per token,    23.30 tokens per second)\n",
            "llama_print_timings:       total time =   14400.93 ms /  2092 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1663.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1688.43 ms /   334 tokens (    5.06 ms per token,   197.82 tokens per second)\n",
            "llama_print_timings:        eval time =      37.71 ms /     1 runs   (   37.71 ms per token,    26.52 tokens per second)\n",
            "llama_print_timings:       total time =    1733.03 ms /   335 tokens\n",
            " 48%|████▊     | 72/150 [47:54<57:40, 44.37s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      19.97 ms /    39 runs   (    0.51 ms per token,  1953.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7599.23 ms /  1388 tokens (    5.47 ms per token,   182.65 tokens per second)\n",
            "llama_print_timings:        eval time =    1551.45 ms /    38 runs   (   40.83 ms per token,    24.49 tokens per second)\n",
            "llama_print_timings:       total time =    9213.01 ms /  1426 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      72.70 ms /   124 runs   (    0.59 ms per token,  1705.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7752.16 ms /  1410 tokens (    5.50 ms per token,   181.88 tokens per second)\n",
            "llama_print_timings:        eval time =    5108.79 ms /   123 runs   (   41.53 ms per token,    24.08 tokens per second)\n",
            "llama_print_timings:       total time =   13040.68 ms /  1533 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      16.18 ms /    33 runs   (    0.49 ms per token,  2039.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     709.01 ms /   144 tokens (    4.92 ms per token,   203.10 tokens per second)\n",
            "llama_print_timings:        eval time =    1244.34 ms /    33 runs   (   37.71 ms per token,    26.52 tokens per second)\n",
            "llama_print_timings:       total time =    1987.61 ms /   177 tokens\n",
            " 49%|████▊     | 73/150 [48:31<54:00, 42.08s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.76 ms /    79 runs   (    0.62 ms per token,  1620.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6690.47 ms /  1234 tokens (    5.42 ms per token,   184.44 tokens per second)\n",
            "llama_print_timings:        eval time =    3173.87 ms /    78 runs   (   40.69 ms per token,    24.58 tokens per second)\n",
            "llama_print_timings:       total time =    9987.59 ms /  1312 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     113.41 ms /   161 runs   (    0.70 ms per token,  1419.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7024.02 ms /  1283 tokens (    5.47 ms per token,   182.66 tokens per second)\n",
            "llama_print_timings:        eval time =    6638.35 ms /   160 runs   (   41.49 ms per token,    24.10 tokens per second)\n",
            "llama_print_timings:       total time =   13953.21 ms /  1443 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     119.86 ms /   211 runs   (    0.57 ms per token,  1760.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =     989.81 ms /   197 tokens (    5.02 ms per token,   199.03 tokens per second)\n",
            "llama_print_timings:        eval time =    7955.92 ms /   210 runs   (   37.89 ms per token,    26.40 tokens per second)\n",
            "llama_print_timings:       total time =    9196.94 ms /   407 tokens\n",
            " 49%|████▉     | 74/150 [49:18<54:55, 43.36s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      11.36 ms /    23 runs   (    0.49 ms per token,  2024.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11460.56 ms /  1997 tokens (    5.74 ms per token,   174.25 tokens per second)\n",
            "llama_print_timings:        eval time =     938.98 ms /    22 runs   (   42.68 ms per token,    23.43 tokens per second)\n",
            "llama_print_timings:       total time =   12453.01 ms /  2019 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     137.92 ms /   256 runs   (    0.54 ms per token,  1856.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11463.08 ms /  1995 tokens (    5.75 ms per token,   174.04 tokens per second)\n",
            "llama_print_timings:        eval time =   10997.84 ms /   255 runs   (   43.13 ms per token,    23.19 tokens per second)\n",
            "llama_print_timings:       total time =   22827.09 ms /  2250 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.18 ms /     2 runs   (    0.59 ms per token,  1699.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     514.88 ms /   103 tokens (    5.00 ms per token,   200.05 tokens per second)\n",
            "llama_print_timings:        eval time =      37.40 ms /     1 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
            "llama_print_timings:       total time =     556.25 ms /   104 tokens\n",
            " 50%|█████     | 75/150 [50:01<54:06, 43.28s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     143.03 ms /   256 runs   (    0.56 ms per token,  1789.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9338.20 ms /  1675 tokens (    5.58 ms per token,   179.37 tokens per second)\n",
            "llama_print_timings:        eval time =   10827.81 ms /   255 runs   (   42.46 ms per token,    23.55 tokens per second)\n",
            "llama_print_timings:       total time =   20538.06 ms /  1930 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      65.18 ms /   112 runs   (    0.58 ms per token,  1718.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10598.92 ms /  1848 tokens (    5.74 ms per token,   174.36 tokens per second)\n",
            "llama_print_timings:        eval time =    4787.09 ms /   112 runs   (   42.74 ms per token,    23.40 tokens per second)\n",
            "llama_print_timings:       total time =   15547.62 ms /  1960 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.51 ms per token,  1972.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1681.38 ms /   334 tokens (    5.03 ms per token,   198.65 tokens per second)\n",
            "llama_print_timings:        eval time =      37.41 ms /     1 runs   (   37.41 ms per token,    26.73 tokens per second)\n",
            "llama_print_timings:       total time =    1725.52 ms /   335 tokens\n",
            " 51%|█████     | 76/150 [50:54<57:01, 46.24s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      88.50 ms /   164 runs   (    0.54 ms per token,  1853.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10857.77 ms /  1907 tokens (    5.69 ms per token,   175.63 tokens per second)\n",
            "llama_print_timings:        eval time =    6940.29 ms /   163 runs   (   42.58 ms per token,    23.49 tokens per second)\n",
            "llama_print_timings:       total time =   18010.72 ms /  2070 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      39.24 ms /    68 runs   (    0.58 ms per token,  1732.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11901.19 ms /  2060 tokens (    5.78 ms per token,   173.09 tokens per second)\n",
            "llama_print_timings:        eval time =    2886.54 ms /    67 runs   (   43.08 ms per token,    23.21 tokens per second)\n",
            "llama_print_timings:       total time =   14904.60 ms /  2127 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.53 ms /    58 runs   (    0.70 ms per token,  1430.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1654.53 ms /   322 tokens (    5.14 ms per token,   194.62 tokens per second)\n",
            "llama_print_timings:        eval time =    2181.18 ms /    57 runs   (   38.27 ms per token,    26.13 tokens per second)\n",
            "llama_print_timings:       total time =    3923.56 ms /   379 tokens\n",
            " 51%|█████▏    | 77/150 [51:40<56:22, 46.34s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      72.51 ms /   120 runs   (    0.60 ms per token,  1654.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5024.09 ms /   949 tokens (    5.29 ms per token,   188.89 tokens per second)\n",
            "llama_print_timings:        eval time =    4722.82 ms /   119 runs   (   39.69 ms per token,    25.20 tokens per second)\n",
            "llama_print_timings:       total time =    9905.87 ms /  1068 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      42.32 ms /    67 runs   (    0.63 ms per token,  1583.06 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5568.55 ms /  1045 tokens (    5.33 ms per token,   187.66 tokens per second)\n",
            "llama_print_timings:        eval time =    2647.97 ms /    66 runs   (   40.12 ms per token,    24.92 tokens per second)\n",
            "llama_print_timings:       total time =    8314.50 ms /  1111 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     151.23 ms /   256 runs   (    0.59 ms per token,  1692.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1800.33 ms /   352 tokens (    5.11 ms per token,   195.52 tokens per second)\n",
            "llama_print_timings:        eval time =    9832.81 ms /   256 runs   (   38.41 ms per token,    26.04 tokens per second)\n",
            "llama_print_timings:       total time =   11960.03 ms /   608 tokens\n",
            " 52%|█████▏    | 78/150 [52:18<52:21, 43.64s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      24.84 ms /    44 runs   (    0.56 ms per token,  1771.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5831.62 ms /  1090 tokens (    5.35 ms per token,   186.91 tokens per second)\n",
            "llama_print_timings:        eval time =    1718.40 ms /    43 runs   (   39.96 ms per token,    25.02 tokens per second)\n",
            "llama_print_timings:       total time =    7608.48 ms /  1133 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      73.73 ms /   140 runs   (    0.53 ms per token,  1898.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6037.60 ms /  1120 tokens (    5.39 ms per token,   185.50 tokens per second)\n",
            "llama_print_timings:        eval time =    5589.44 ms /   139 runs   (   40.21 ms per token,    24.87 tokens per second)\n",
            "llama_print_timings:       total time =   11796.58 ms /  1259 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.98 ms /     2 runs   (    0.49 ms per token,  2036.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =     704.89 ms /   140 tokens (    5.03 ms per token,   198.61 tokens per second)\n",
            "llama_print_timings:        eval time =      36.77 ms /     1 runs   (   36.77 ms per token,    27.19 tokens per second)\n",
            "llama_print_timings:       total time =     745.62 ms /   141 tokens\n",
            " 53%|█████▎    | 79/150 [52:43<45:14, 38.24s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      43.74 ms /    80 runs   (    0.55 ms per token,  1828.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3849.04 ms /   744 tokens (    5.17 ms per token,   193.30 tokens per second)\n",
            "llama_print_timings:        eval time =    3129.12 ms /    80 runs   (   39.11 ms per token,    25.57 tokens per second)\n",
            "llama_print_timings:       total time =    7072.84 ms /   824 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      82.40 ms /   144 runs   (    0.57 ms per token,  1747.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4278.81 ms /   811 tokens (    5.28 ms per token,   189.54 tokens per second)\n",
            "llama_print_timings:        eval time =    5622.68 ms /   143 runs   (   39.32 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =   10085.23 ms /   954 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.36 ms /    71 runs   (    0.58 ms per token,  1716.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1027.81 ms /   202 tokens (    5.09 ms per token,   196.53 tokens per second)\n",
            "llama_print_timings:        eval time =    2635.55 ms /    70 runs   (   37.65 ms per token,    26.56 tokens per second)\n",
            "llama_print_timings:       total time =    3743.78 ms /   272 tokens\n",
            " 53%|█████▎    | 80/150 [53:10<40:37, 34.82s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      19.84 ms /    35 runs   (    0.57 ms per token,  1763.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3944.47 ms /   746 tokens (    5.29 ms per token,   189.13 tokens per second)\n",
            "llama_print_timings:        eval time =    1336.82 ms /    34 runs   (   39.32 ms per token,    25.43 tokens per second)\n",
            "llama_print_timings:       total time =    5332.45 ms /   780 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     133.93 ms /   214 runs   (    0.63 ms per token,  1597.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3988.90 ms /   766 tokens (    5.21 ms per token,   192.03 tokens per second)\n",
            "llama_print_timings:        eval time =    8444.61 ms /   213 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =   12767.87 ms /   979 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.53 ms /     2 runs   (    0.77 ms per token,  1304.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =     590.87 ms /   115 tokens (    5.14 ms per token,   194.63 tokens per second)\n",
            "llama_print_timings:        eval time =      37.13 ms /     1 runs   (   37.13 ms per token,    26.93 tokens per second)\n",
            "llama_print_timings:       total time =     632.75 ms /   116 tokens\n",
            " 54%|█████▍    | 81/150 [53:31<35:05, 30.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     141.91 ms /   256 runs   (    0.55 ms per token,  1804.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13724.76 ms /  2339 tokens (    5.87 ms per token,   170.42 tokens per second)\n",
            "llama_print_timings:        eval time =   11878.58 ms /   255 runs   (   46.58 ms per token,    21.47 tokens per second)\n",
            "llama_print_timings:       total time =   25994.75 ms /  2594 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     136.89 ms /   256 runs   (    0.53 ms per token,  1870.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14201.39 ms /  2392 tokens (    5.94 ms per token,   168.43 tokens per second)\n",
            "llama_print_timings:        eval time =   12049.83 ms /   255 runs   (   47.25 ms per token,    21.16 tokens per second)\n",
            "llama_print_timings:       total time =   26640.67 ms /  2647 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.89 ms /     2 runs   (    0.44 ms per token,  2249.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1676.37 ms /   332 tokens (    5.05 ms per token,   198.05 tokens per second)\n",
            "llama_print_timings:        eval time =      37.23 ms /     1 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
            "llama_print_timings:       total time =    1719.45 ms /   333 tokens\n",
            " 55%|█████▍    | 82/150 [54:38<47:06, 41.56s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     132.59 ms /   256 runs   (    0.52 ms per token,  1930.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13932.00 ms /  2376 tokens (    5.86 ms per token,   170.54 tokens per second)\n",
            "llama_print_timings:        eval time =   11974.99 ms /   256 runs   (   46.78 ms per token,    21.38 tokens per second)\n",
            "llama_print_timings:       total time =   26283.32 ms /  2632 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     145.05 ms /   256 runs   (    0.57 ms per token,  1764.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15526.83 ms /  2586 tokens (    6.00 ms per token,   166.55 tokens per second)\n",
            "llama_print_timings:        eval time =   12515.19 ms /   255 runs   (   49.08 ms per token,    20.38 tokens per second)\n",
            "llama_print_timings:       total time =   28449.01 ms /  2841 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.28 ms /     2 runs   (    0.64 ms per token,  1564.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1698.90 ms /   332 tokens (    5.12 ms per token,   195.42 tokens per second)\n",
            "llama_print_timings:        eval time =      37.59 ms /     1 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    1744.63 ms /   333 tokens\n",
            " 55%|█████▌    | 83/150 [55:39<52:52, 47.36s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      49.70 ms /    89 runs   (    0.56 ms per token,  1790.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12606.97 ms /  2171 tokens (    5.81 ms per token,   172.21 tokens per second)\n",
            "llama_print_timings:        eval time =    3809.86 ms /    88 runs   (   43.29 ms per token,    23.10 tokens per second)\n",
            "llama_print_timings:       total time =   16554.14 ms /  2259 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      45.14 ms /    82 runs   (    0.55 ms per token,  1816.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12924.81 ms /  2211 tokens (    5.85 ms per token,   171.07 tokens per second)\n",
            "llama_print_timings:        eval time =    3521.27 ms /    81 runs   (   43.47 ms per token,    23.00 tokens per second)\n",
            "llama_print_timings:       total time =   16569.54 ms /  2292 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1924.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     824.77 ms /   168 tokens (    4.91 ms per token,   203.69 tokens per second)\n",
            "llama_print_timings:        eval time =      37.09 ms /     1 runs   (   37.09 ms per token,    26.97 tokens per second)\n",
            "llama_print_timings:       total time =     865.88 ms /   169 tokens\n",
            " 56%|█████▌    | 84/150 [56:18<49:26, 44.94s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     137.38 ms /   256 runs   (    0.54 ms per token,  1863.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14263.90 ms /  2415 tokens (    5.91 ms per token,   169.31 tokens per second)\n",
            "llama_print_timings:        eval time =   12162.08 ms /   255 runs   (   47.69 ms per token,    20.97 tokens per second)\n",
            "llama_print_timings:       total time =   26832.70 ms /  2670 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     136.36 ms /   256 runs   (    0.53 ms per token,  1877.42 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15625.97 ms /  2602 tokens (    6.01 ms per token,   166.52 tokens per second)\n",
            "llama_print_timings:        eval time =   12518.05 ms /   255 runs   (   49.09 ms per token,    20.37 tokens per second)\n",
            "llama_print_timings:       total time =   28539.17 ms /  2857 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.49 ms per token,  2020.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1687.75 ms /   333 tokens (    5.07 ms per token,   197.30 tokens per second)\n",
            "llama_print_timings:        eval time =      37.43 ms /     1 runs   (   37.43 ms per token,    26.72 tokens per second)\n",
            "llama_print_timings:       total time =    1732.01 ms /   334 tokens\n",
            " 57%|█████▋    | 85/150 [57:20<54:10, 50.00s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.16 ms /    56 runs   (    0.56 ms per token,  1797.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7118.46 ms /  1320 tokens (    5.39 ms per token,   185.43 tokens per second)\n",
            "llama_print_timings:        eval time =    2230.08 ms /    55 runs   (   40.55 ms per token,    24.66 tokens per second)\n",
            "llama_print_timings:       total time =    9424.41 ms /  1375 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      38.35 ms /    62 runs   (    0.62 ms per token,  1616.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7500.80 ms /  1364 tokens (    5.50 ms per token,   181.85 tokens per second)\n",
            "llama_print_timings:        eval time =    2511.69 ms /    61 runs   (   41.18 ms per token,    24.29 tokens per second)\n",
            "llama_print_timings:       total time =   10114.62 ms /  1425 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.27 ms /     2 runs   (    0.64 ms per token,  1573.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =     838.03 ms /   166 tokens (    5.05 ms per token,   198.08 tokens per second)\n",
            "llama_print_timings:        eval time =      37.96 ms /     1 runs   (   37.96 ms per token,    26.34 tokens per second)\n",
            "llama_print_timings:       total time =     881.01 ms /   167 tokens\n",
            " 57%|█████▋    | 86/150 [57:52<47:37, 44.65s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     111.01 ms /   183 runs   (    0.61 ms per token,  1648.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8025.54 ms /  1453 tokens (    5.52 ms per token,   181.05 tokens per second)\n",
            "llama_print_timings:        eval time =    7568.26 ms /   182 runs   (   41.58 ms per token,    24.05 tokens per second)\n",
            "llama_print_timings:       total time =   15848.04 ms /  1635 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     139.08 ms /   237 runs   (    0.59 ms per token,  1704.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9027.69 ms /  1605 tokens (    5.62 ms per token,   177.79 tokens per second)\n",
            "llama_print_timings:        eval time =    9915.94 ms /   236 runs   (   42.02 ms per token,    23.80 tokens per second)\n",
            "llama_print_timings:       total time =   19279.85 ms /  1841 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      71.14 ms /   118 runs   (    0.60 ms per token,  1658.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1703.06 ms /   330 tokens (    5.16 ms per token,   193.77 tokens per second)\n",
            "llama_print_timings:        eval time =    4458.12 ms /   117 runs   (   38.10 ms per token,    26.24 tokens per second)\n",
            "llama_print_timings:       total time =    6306.73 ms /   447 tokens\n",
            " 58%|█████▊    | 87/150 [58:44<49:09, 46.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      61.34 ms /   111 runs   (    0.55 ms per token,  1809.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7752.87 ms /  1415 tokens (    5.48 ms per token,   182.51 tokens per second)\n",
            "llama_print_timings:        eval time =    4498.46 ms /   110 runs   (   40.90 ms per token,    24.45 tokens per second)\n",
            "llama_print_timings:       total time =   12392.56 ms /  1525 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.27 ms /    72 runs   (    0.57 ms per token,  1744.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8169.04 ms /  1479 tokens (    5.52 ms per token,   181.05 tokens per second)\n",
            "llama_print_timings:        eval time =    2925.44 ms /    71 runs   (   41.20 ms per token,    24.27 tokens per second)\n",
            "llama_print_timings:       total time =   11193.06 ms /  1550 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.12 ms /     2 runs   (    0.56 ms per token,  1782.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1025.51 ms /   202 tokens (    5.08 ms per token,   196.97 tokens per second)\n",
            "llama_print_timings:        eval time =      37.00 ms /     1 runs   (   37.00 ms per token,    27.03 tokens per second)\n",
            "llama_print_timings:       total time =    1067.26 ms /   203 tokens\n",
            " 59%|█████▊    | 88/150 [59:15<43:26, 42.05s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      38.83 ms /    62 runs   (    0.63 ms per token,  1596.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7244.04 ms /  1331 tokens (    5.44 ms per token,   183.74 tokens per second)\n",
            "llama_print_timings:        eval time =    2501.82 ms /    61 runs   (   41.01 ms per token,    24.38 tokens per second)\n",
            "llama_print_timings:       total time =    9842.28 ms /  1392 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      64.68 ms /   113 runs   (    0.57 ms per token,  1747.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7614.41 ms /  1378 tokens (    5.53 ms per token,   180.97 tokens per second)\n",
            "llama_print_timings:        eval time =    4623.89 ms /   112 runs   (   41.28 ms per token,    24.22 tokens per second)\n",
            "llama_print_timings:       total time =   12386.99 ms /  1490 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.23 ms /     2 runs   (    0.61 ms per token,  1627.34 tokens per second)\n",
            "llama_print_timings: prompt eval time =     871.09 ms /   176 tokens (    4.95 ms per token,   202.04 tokens per second)\n",
            "llama_print_timings:        eval time =      76.93 ms /     2 runs   (   38.47 ms per token,    26.00 tokens per second)\n",
            "llama_print_timings:       total time =     953.47 ms /   178 tokens\n",
            " 59%|█████▉    | 89/150 [59:45<39:08, 38.50s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      57.95 ms /    99 runs   (    0.59 ms per token,  1708.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5702.14 ms /  1072 tokens (    5.32 ms per token,   188.00 tokens per second)\n",
            "llama_print_timings:        eval time =    3970.29 ms /    99 runs   (   40.10 ms per token,    24.94 tokens per second)\n",
            "llama_print_timings:       total time =    9808.22 ms /  1171 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1865.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6250.45 ms /  1158 tokens (    5.40 ms per token,   185.27 tokens per second)\n",
            "llama_print_timings:        eval time =      39.93 ms /     1 runs   (   39.93 ms per token,    25.05 tokens per second)\n",
            "llama_print_timings:       total time =    6309.97 ms /  1159 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      50.74 ms /    94 runs   (    0.54 ms per token,  1852.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =     948.83 ms /   189 tokens (    5.02 ms per token,   199.19 tokens per second)\n",
            "llama_print_timings:        eval time =    3497.36 ms /    93 runs   (   37.61 ms per token,    26.59 tokens per second)\n",
            "llama_print_timings:       total time =    4545.68 ms /   282 tokens\n",
            " 60%|██████    | 90/150 [1:00:11<34:46, 34.78s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      36.47 ms /    63 runs   (    0.58 ms per token,  1727.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4146.96 ms /   795 tokens (    5.22 ms per token,   191.71 tokens per second)\n",
            "llama_print_timings:        eval time =    2425.48 ms /    62 runs   (   39.12 ms per token,    25.56 tokens per second)\n",
            "llama_print_timings:       total time =    6651.31 ms /   857 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      73.47 ms /   131 runs   (    0.56 ms per token,  1783.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4414.73 ms /   836 tokens (    5.28 ms per token,   189.37 tokens per second)\n",
            "llama_print_timings:        eval time =    5128.42 ms /   130 runs   (   39.45 ms per token,    25.35 tokens per second)\n",
            "llama_print_timings:       total time =    9701.78 ms /   966 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      38.87 ms /    68 runs   (    0.57 ms per token,  1749.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =     823.23 ms /   164 tokens (    5.02 ms per token,   199.21 tokens per second)\n",
            "llama_print_timings:        eval time =    2516.23 ms /    67 runs   (   37.56 ms per token,    26.63 tokens per second)\n",
            "llama_print_timings:       total time =    3412.70 ms /   231 tokens\n",
            " 61%|██████    | 91/150 [1:00:39<32:03, 32.60s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      69.30 ms /   119 runs   (    0.58 ms per token,  1717.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7724.46 ms /  1408 tokens (    5.49 ms per token,   182.28 tokens per second)\n",
            "llama_print_timings:        eval time =    4854.67 ms /   118 runs   (   41.14 ms per token,    24.31 tokens per second)\n",
            "llama_print_timings:       total time =   12739.25 ms /  1526 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      51.76 ms /    83 runs   (    0.62 ms per token,  1603.43 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8304.77 ms /  1495 tokens (    5.56 ms per token,   180.02 tokens per second)\n",
            "llama_print_timings:        eval time =    3423.92 ms /    82 runs   (   41.76 ms per token,    23.95 tokens per second)\n",
            "llama_print_timings:       total time =   11857.02 ms /  1577 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      38.30 ms /    67 runs   (    0.57 ms per token,  1749.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1074.94 ms /   214 tokens (    5.02 ms per token,   199.08 tokens per second)\n",
            "llama_print_timings:        eval time =    2493.77 ms /    66 runs   (   37.78 ms per token,    26.47 tokens per second)\n",
            "llama_print_timings:       total time =    3641.69 ms /   280 tokens\n",
            " 61%|██████▏   | 92/150 [1:01:17<33:05, 34.23s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.81 ms /    55 runs   (    0.58 ms per token,  1728.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5024.64 ms /   949 tokens (    5.29 ms per token,   188.87 tokens per second)\n",
            "llama_print_timings:        eval time =    2122.76 ms /    54 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    7227.32 ms /  1003 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.76 ms /    69 runs   (    0.68 ms per token,  1475.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5108.06 ms /   968 tokens (    5.28 ms per token,   189.50 tokens per second)\n",
            "llama_print_timings:        eval time =    2723.38 ms /    68 runs   (   40.05 ms per token,    24.97 tokens per second)\n",
            "llama_print_timings:       total time =    7955.77 ms /  1036 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      87.86 ms /   156 runs   (    0.56 ms per token,  1775.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =     749.76 ms /   150 tokens (    5.00 ms per token,   200.06 tokens per second)\n",
            "llama_print_timings:        eval time =    5827.86 ms /   155 runs   (   37.60 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    6757.44 ms /   305 tokens\n",
            " 62%|██████▏   | 93/150 [1:01:44<30:37, 32.23s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       7.09 ms /    13 runs   (    0.55 ms per token,  1832.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5759.92 ms /  1080 tokens (    5.33 ms per token,   187.50 tokens per second)\n",
            "llama_print_timings:        eval time =     480.97 ms /    12 runs   (   40.08 ms per token,    24.95 tokens per second)\n",
            "llama_print_timings:       total time =    6269.31 ms /  1092 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     166.34 ms /   256 runs   (    0.65 ms per token,  1539.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5659.27 ms /  1064 tokens (    5.32 ms per token,   188.01 tokens per second)\n",
            "llama_print_timings:        eval time =   10368.21 ms /   256 runs   (   40.50 ms per token,    24.69 tokens per second)\n",
            "llama_print_timings:       total time =   16443.13 ms /  1320 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.22 ms /     2 runs   (    0.61 ms per token,  1640.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =     636.74 ms /   128 tokens (    4.97 ms per token,   201.02 tokens per second)\n",
            "llama_print_timings:        eval time =      37.67 ms /     1 runs   (   37.67 ms per token,    26.55 tokens per second)\n",
            "llama_print_timings:       total time =     678.87 ms /   129 tokens\n",
            " 63%|██████▎   | 94/150 [1:02:13<29:09, 31.25s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      67.18 ms /   102 runs   (    0.66 ms per token,  1518.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8750.44 ms /  1583 tokens (    5.53 ms per token,   180.91 tokens per second)\n",
            "llama_print_timings:        eval time =    4227.27 ms /   101 runs   (   41.85 ms per token,    23.89 tokens per second)\n",
            "llama_print_timings:       total time =   13152.20 ms /  1684 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      74.37 ms /   118 runs   (    0.63 ms per token,  1586.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9420.07 ms /  1671 tokens (    5.64 ms per token,   177.39 tokens per second)\n",
            "llama_print_timings:        eval time =    4982.08 ms /   117 runs   (   42.58 ms per token,    23.48 tokens per second)\n",
            "llama_print_timings:       total time =   14602.83 ms /  1788 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.95 ms /    65 runs   (    0.58 ms per token,  1712.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =     967.77 ms /   188 tokens (    5.15 ms per token,   194.26 tokens per second)\n",
            "llama_print_timings:        eval time =    2433.70 ms /    64 runs   (   38.03 ms per token,    26.30 tokens per second)\n",
            "llama_print_timings:       total time =    3478.32 ms /   252 tokens\n",
            " 63%|██████▎   | 95/150 [1:03:02<33:19, 36.36s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      38.91 ms /    69 runs   (    0.56 ms per token,  1773.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5606.68 ms /  1051 tokens (    5.33 ms per token,   187.45 tokens per second)\n",
            "llama_print_timings:        eval time =    2709.41 ms /    68 runs   (   39.84 ms per token,    25.10 tokens per second)\n",
            "llama_print_timings:       total time =    8420.08 ms /  1119 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.21 ms /     2 runs   (    0.60 ms per token,  1657.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5902.22 ms /  1090 tokens (    5.41 ms per token,   184.68 tokens per second)\n",
            "llama_print_timings:        eval time =      39.75 ms /     1 runs   (   39.75 ms per token,    25.16 tokens per second)\n",
            "llama_print_timings:       total time =    5963.16 ms /  1091 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.25 ms /    73 runs   (    0.57 ms per token,  1769.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =     746.51 ms /   152 tokens (    4.91 ms per token,   203.61 tokens per second)\n",
            "llama_print_timings:        eval time =    2706.54 ms /    72 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    3545.12 ms /   224 tokens\n",
            " 64%|██████▍   | 96/150 [1:03:31<30:53, 34.32s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     146.16 ms /   256 runs   (    0.57 ms per token,  1751.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8036.98 ms /  1461 tokens (    5.50 ms per token,   181.78 tokens per second)\n",
            "llama_print_timings:        eval time =   10608.06 ms /   255 runs   (   41.60 ms per token,    24.04 tokens per second)\n",
            "llama_print_timings:       total time =   19032.31 ms /  1716 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     159.62 ms /   256 runs   (    0.62 ms per token,  1603.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9485.99 ms /  1678 tokens (    5.65 ms per token,   176.89 tokens per second)\n",
            "llama_print_timings:        eval time =   10830.59 ms /   255 runs   (   42.47 ms per token,    23.54 tokens per second)\n",
            "llama_print_timings:       total time =   20714.53 ms /  1933 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.41 ms /    69 runs   (    0.54 ms per token,  1844.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1722.31 ms /   344 tokens (    5.01 ms per token,   199.73 tokens per second)\n",
            "llama_print_timings:        eval time =    2587.16 ms /    68 runs   (   38.05 ms per token,    26.28 tokens per second)\n",
            "llama_print_timings:       total time =    4384.49 ms /   412 tokens\n",
            " 65%|██████▍   | 97/150 [1:04:29<36:40, 41.53s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      70.50 ms /   124 runs   (    0.57 ms per token,  1758.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9181.99 ms /  1646 tokens (    5.58 ms per token,   179.26 tokens per second)\n",
            "llama_print_timings:        eval time =    5131.61 ms /   123 runs   (   41.72 ms per token,    23.97 tokens per second)\n",
            "llama_print_timings:       total time =   14492.63 ms /  1769 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.79 ms /    73 runs   (    0.57 ms per token,  1746.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9951.63 ms /  1756 tokens (    5.67 ms per token,   176.45 tokens per second)\n",
            "llama_print_timings:        eval time =    3044.77 ms /    72 runs   (   42.29 ms per token,    23.65 tokens per second)\n",
            "llama_print_timings:       total time =   13105.96 ms /  1828 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     138.39 ms /   214 runs   (    0.65 ms per token,  1546.40 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1072.43 ms /   214 tokens (    5.01 ms per token,   199.55 tokens per second)\n",
            "llama_print_timings:        eval time =    8117.42 ms /   213 runs   (   38.11 ms per token,    26.24 tokens per second)\n",
            "llama_print_timings:       total time =    9490.97 ms /   427 tokens\n",
            " 65%|██████▌   | 98/150 [1:05:19<37:56, 43.78s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      26.83 ms /    46 runs   (    0.58 ms per token,  1714.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4726.35 ms /   892 tokens (    5.30 ms per token,   188.73 tokens per second)\n",
            "llama_print_timings:        eval time =    1774.51 ms /    45 runs   (   39.43 ms per token,    25.36 tokens per second)\n",
            "llama_print_timings:       total time =    6569.84 ms /   937 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.19 ms /    65 runs   (    0.57 ms per token,  1748.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4790.21 ms /   912 tokens (    5.25 ms per token,   190.39 tokens per second)\n",
            "llama_print_timings:        eval time =    2531.59 ms /    64 runs   (   39.56 ms per token,    25.28 tokens per second)\n",
            "llama_print_timings:       total time =    7409.07 ms /   976 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.25 ms /     2 runs   (    0.63 ms per token,  1596.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =     638.50 ms /   123 tokens (    5.19 ms per token,   192.64 tokens per second)\n",
            "llama_print_timings:        eval time =      37.30 ms /     1 runs   (   37.30 ms per token,    26.81 tokens per second)\n",
            "llama_print_timings:       total time =     680.48 ms /   124 tokens\n",
            " 66%|██████▌   | 99/150 [1:05:40<31:39, 37.24s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     146.77 ms /   256 runs   (    0.57 ms per token,  1744.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13446.74 ms /  2293 tokens (    5.86 ms per token,   170.52 tokens per second)\n",
            "llama_print_timings:        eval time =   11672.41 ms /   255 runs   (   45.77 ms per token,    21.85 tokens per second)\n",
            "llama_print_timings:       total time =   25583.79 ms /  2548 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     138.74 ms /   256 runs   (    0.54 ms per token,  1845.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14842.01 ms /  2470 tokens (    6.01 ms per token,   166.42 tokens per second)\n",
            "llama_print_timings:        eval time =   12430.96 ms /   255 runs   (   48.75 ms per token,    20.51 tokens per second)\n",
            "llama_print_timings:       total time =   27713.08 ms /  2725 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.13 ms /     2 runs   (    0.56 ms per token,  1773.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1676.80 ms /   333 tokens (    5.04 ms per token,   198.59 tokens per second)\n",
            "llama_print_timings:        eval time =      37.26 ms /     1 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
            "llama_print_timings:       total time =    1722.04 ms /   334 tokens\n",
            " 67%|██████▋   | 100/150 [1:06:49<38:55, 46.70s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     140.34 ms /   256 runs   (    0.55 ms per token,  1824.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11547.86 ms /  2023 tokens (    5.71 ms per token,   175.18 tokens per second)\n",
            "llama_print_timings:        eval time =   11051.01 ms /   255 runs   (   43.34 ms per token,    23.07 tokens per second)\n",
            "llama_print_timings:       total time =   22999.50 ms /  2278 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.50 ms per token,  1986.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12805.05 ms /  2184 tokens (    5.86 ms per token,   170.56 tokens per second)\n",
            "llama_print_timings:        eval time =      87.63 ms /     2 runs   (   43.81 ms per token,    22.82 tokens per second)\n",
            "llama_print_timings:       total time =   12925.59 ms /  2186 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      57.46 ms /   108 runs   (    0.53 ms per token,  1879.54 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1683.21 ms /   333 tokens (    5.05 ms per token,   197.84 tokens per second)\n",
            "llama_print_timings:        eval time =    4085.98 ms /   107 runs   (   38.19 ms per token,    26.19 tokens per second)\n",
            "llama_print_timings:       total time =    5893.52 ms /   440 tokens\n",
            " 67%|██████▋   | 101/150 [1:07:44<40:05, 49.09s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      67.78 ms /   117 runs   (    0.58 ms per token,  1726.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11149.09 ms /  1950 tokens (    5.72 ms per token,   174.90 tokens per second)\n",
            "llama_print_timings:        eval time =    4973.53 ms /   116 runs   (   42.88 ms per token,    23.32 tokens per second)\n",
            "llama_print_timings:       total time =   16313.09 ms /  2066 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      30.13 ms /    59 runs   (    0.51 ms per token,  1958.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11568.37 ms /  2011 tokens (    5.75 ms per token,   173.84 tokens per second)\n",
            "llama_print_timings:        eval time =    2479.97 ms /    58 runs   (   42.76 ms per token,    23.39 tokens per second)\n",
            "llama_print_timings:       total time =   14150.95 ms /  2069 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.98 ms /     2 runs   (    0.49 ms per token,  2042.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     985.05 ms /   196 tokens (    5.03 ms per token,   198.98 tokens per second)\n",
            "llama_print_timings:        eval time =      37.13 ms /     1 runs   (   37.13 ms per token,    26.93 tokens per second)\n",
            "llama_print_timings:       total time =    1027.45 ms /   197 tokens\n",
            " 68%|██████▊   | 102/150 [1:08:21<36:22, 45.46s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     145.95 ms /   256 runs   (    0.57 ms per token,  1754.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11003.65 ms /  1928 tokens (    5.71 ms per token,   175.21 tokens per second)\n",
            "llama_print_timings:        eval time =   10960.25 ms /   255 runs   (   42.98 ms per token,    23.27 tokens per second)\n",
            "llama_print_timings:       total time =   22388.25 ms /  2183 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     145.12 ms /   256 runs   (    0.57 ms per token,  1764.03 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12400.40 ms /  2134 tokens (    5.81 ms per token,   172.09 tokens per second)\n",
            "llama_print_timings:        eval time =   11109.38 ms /   255 runs   (   43.57 ms per token,    22.95 tokens per second)\n",
            "llama_print_timings:       total time =   23934.23 ms /  2389 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      23.33 ms /    44 runs   (    0.53 ms per token,  1885.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1705.69 ms /   333 tokens (    5.12 ms per token,   195.23 tokens per second)\n",
            "llama_print_timings:        eval time =    1634.56 ms /    43 runs   (   38.01 ms per token,    26.31 tokens per second)\n",
            "llama_print_timings:       total time =    3397.53 ms /   376 tokens\n",
            " 69%|██████▊   | 103/150 [1:09:15<37:36, 48.00s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      15.81 ms /    30 runs   (    0.53 ms per token,  1898.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12043.14 ms /  2096 tokens (    5.75 ms per token,   174.04 tokens per second)\n",
            "llama_print_timings:        eval time =    1247.70 ms /    29 runs   (   43.02 ms per token,    23.24 tokens per second)\n",
            "llama_print_timings:       total time =   13351.17 ms /  2125 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      46.94 ms /    88 runs   (    0.53 ms per token,  1874.77 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12176.13 ms /  2094 tokens (    5.81 ms per token,   171.98 tokens per second)\n",
            "llama_print_timings:        eval time =    3789.64 ms /    87 runs   (   43.56 ms per token,    22.96 tokens per second)\n",
            "llama_print_timings:       total time =   16087.49 ms /  2181 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      41.60 ms /    76 runs   (    0.55 ms per token,  1827.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     549.01 ms /   108 tokens (    5.08 ms per token,   196.72 tokens per second)\n",
            "llama_print_timings:        eval time =    2827.83 ms /    75 runs   (   37.70 ms per token,    26.52 tokens per second)\n",
            "llama_print_timings:       total time =    3456.94 ms /   183 tokens\n",
            " 69%|██████▉   | 104/150 [1:10:04<36:57, 48.20s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      21.38 ms /    43 runs   (    0.50 ms per token,  2011.70 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7891.95 ms /  1436 tokens (    5.50 ms per token,   181.96 tokens per second)\n",
            "llama_print_timings:        eval time =    1722.05 ms /    42 runs   (   41.00 ms per token,    24.39 tokens per second)\n",
            "llama_print_timings:       total time =    9675.22 ms /  1478 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.48 ms /    83 runs   (    0.58 ms per token,  1712.08 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7837.80 ms /  1418 tokens (    5.53 ms per token,   180.92 tokens per second)\n",
            "llama_print_timings:        eval time =    3384.06 ms /    82 runs   (   41.27 ms per token,    24.23 tokens per second)\n",
            "llama_print_timings:       total time =   11335.61 ms /  1500 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.10 ms /     2 runs   (    0.55 ms per token,  1813.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     587.42 ms /   119 tokens (    4.94 ms per token,   202.58 tokens per second)\n",
            "llama_print_timings:        eval time =      36.83 ms /     1 runs   (   36.83 ms per token,    27.15 tokens per second)\n",
            "llama_print_timings:       total time =     628.86 ms /   120 tokens\n",
            " 70%|███████   | 105/150 [1:10:40<33:25, 44.57s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.42 ms /    59 runs   (    0.53 ms per token,  1877.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12141.20 ms /  2112 tokens (    5.75 ms per token,   173.95 tokens per second)\n",
            "llama_print_timings:        eval time =    2542.27 ms /    59 runs   (   43.09 ms per token,    23.21 tokens per second)\n",
            "llama_print_timings:       total time =   14772.86 ms /  2171 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     154.49 ms /   256 runs   (    0.60 ms per token,  1657.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12601.15 ms /  2159 tokens (    5.84 ms per token,   171.33 tokens per second)\n",
            "llama_print_timings:        eval time =   11236.97 ms /   255 runs   (   44.07 ms per token,    22.69 tokens per second)\n",
            "llama_print_timings:       total time =   24256.03 ms /  2414 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.06 ms /    72 runs   (    0.56 ms per token,  1797.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =     786.27 ms /   156 tokens (    5.04 ms per token,   198.41 tokens per second)\n",
            "llama_print_timings:        eval time =    2675.91 ms /    71 runs   (   37.69 ms per token,    26.53 tokens per second)\n",
            "llama_print_timings:       total time =    3537.26 ms /   227 tokens\n",
            " 71%|███████   | 106/150 [1:11:37<35:28, 48.37s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      62.96 ms /   111 runs   (    0.57 ms per token,  1763.02 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12001.68 ms /  2083 tokens (    5.76 ms per token,   173.56 tokens per second)\n",
            "llama_print_timings:        eval time =    4743.07 ms /   110 runs   (   43.12 ms per token,    23.19 tokens per second)\n",
            "llama_print_timings:       total time =   16900.18 ms /  2193 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      38.97 ms /    66 runs   (    0.59 ms per token,  1693.52 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12713.57 ms /  2180 tokens (    5.83 ms per token,   171.47 tokens per second)\n",
            "llama_print_timings:        eval time =    2823.24 ms /    65 runs   (   43.43 ms per token,    23.02 tokens per second)\n",
            "llama_print_timings:       total time =   15640.05 ms /  2245 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      60.20 ms /    89 runs   (    0.68 ms per token,  1478.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1031.23 ms /   207 tokens (    4.98 ms per token,   200.73 tokens per second)\n",
            "llama_print_timings:        eval time =    3355.29 ms /    88 runs   (   38.13 ms per token,    26.23 tokens per second)\n",
            "llama_print_timings:       total time =    4516.11 ms /   295 tokens\n",
            " 71%|███████▏  | 107/150 [1:12:26<34:54, 48.71s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      49.85 ms /    91 runs   (    0.55 ms per token,  1825.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8065.57 ms /  1463 tokens (    5.51 ms per token,   181.39 tokens per second)\n",
            "llama_print_timings:        eval time =    3706.27 ms /    90 runs   (   41.18 ms per token,    24.28 tokens per second)\n",
            "llama_print_timings:       total time =   11888.98 ms /  1553 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.32 ms /     2 runs   (    0.66 ms per token,  1518.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8466.48 ms /  1528 tokens (    5.54 ms per token,   180.48 tokens per second)\n",
            "llama_print_timings:        eval time =      40.72 ms /     1 runs   (   40.72 ms per token,    24.56 tokens per second)\n",
            "llama_print_timings:       total time =    8528.98 ms /  1529 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.37 ms /     2 runs   (    0.68 ms per token,  1460.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =     872.99 ms /   174 tokens (    5.02 ms per token,   199.31 tokens per second)\n",
            "llama_print_timings:        eval time =      38.08 ms /     1 runs   (   38.08 ms per token,    26.26 tokens per second)\n",
            "llama_print_timings:       total time =     915.72 ms /   175 tokens\n",
            " 72%|███████▏  | 108/150 [1:12:55<29:52, 42.68s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      32.69 ms /    46 runs   (    0.71 ms per token,  1407.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6702.61 ms /  1245 tokens (    5.38 ms per token,   185.75 tokens per second)\n",
            "llama_print_timings:        eval time =    1837.02 ms /    45 runs   (   40.82 ms per token,    24.50 tokens per second)\n",
            "llama_print_timings:       total time =    8617.82 ms /  1290 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      42.04 ms /    78 runs   (    0.54 ms per token,  1855.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6984.88 ms /  1280 tokens (    5.46 ms per token,   183.25 tokens per second)\n",
            "llama_print_timings:        eval time =    3190.49 ms /    78 runs   (   40.90 ms per token,    24.45 tokens per second)\n",
            "llama_print_timings:       total time =   10273.58 ms /  1358 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      94.84 ms /   144 runs   (    0.66 ms per token,  1518.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     789.91 ms /   160 tokens (    4.94 ms per token,   202.55 tokens per second)\n",
            "llama_print_timings:        eval time =    5458.83 ms /   143 runs   (   38.17 ms per token,    26.20 tokens per second)\n",
            "llama_print_timings:       total time =    6446.46 ms /   303 tokens\n",
            " 73%|███████▎  | 109/150 [1:13:36<28:52, 42.26s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     112.62 ms /   179 runs   (    0.63 ms per token,  1589.36 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10248.65 ms /  1813 tokens (    5.65 ms per token,   176.90 tokens per second)\n",
            "llama_print_timings:        eval time =    7613.03 ms /   178 runs   (   42.77 ms per token,    23.38 tokens per second)\n",
            "llama_print_timings:       total time =   18145.13 ms /  1991 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     144.45 ms /   247 runs   (    0.58 ms per token,  1709.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11343.22 ms /  1972 tokens (    5.75 ms per token,   173.85 tokens per second)\n",
            "llama_print_timings:        eval time =   10569.45 ms /   246 runs   (   42.97 ms per token,    23.27 tokens per second)\n",
            "llama_print_timings:       total time =   22271.69 ms /  2218 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.30 ms /     2 runs   (    0.65 ms per token,  1533.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1404.02 ms /   275 tokens (    5.11 ms per token,   195.87 tokens per second)\n",
            "llama_print_timings:        eval time =      37.35 ms /     1 runs   (   37.35 ms per token,    26.78 tokens per second)\n",
            "llama_print_timings:       total time =    1448.39 ms /   276 tokens\n",
            " 73%|███████▎  | 110/150 [1:14:29<30:22, 45.56s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     136.89 ms /   256 runs   (    0.53 ms per token,  1870.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11210.14 ms /  1968 tokens (    5.70 ms per token,   175.56 tokens per second)\n",
            "llama_print_timings:        eval time =   11048.30 ms /   256 runs   (   43.16 ms per token,    23.17 tokens per second)\n",
            "llama_print_timings:       total time =   22626.10 ms /  2224 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      39.01 ms /    69 runs   (    0.57 ms per token,  1768.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12945.79 ms /  2198 tokens (    5.89 ms per token,   169.78 tokens per second)\n",
            "llama_print_timings:        eval time =    2979.29 ms /    68 runs   (   43.81 ms per token,    22.82 tokens per second)\n",
            "llama_print_timings:       total time =   16036.69 ms /  2266 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1755.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1683.03 ms /   336 tokens (    5.01 ms per token,   199.64 tokens per second)\n",
            "llama_print_timings:        eval time =      75.55 ms /     2 runs   (   37.77 ms per token,    26.47 tokens per second)\n",
            "llama_print_timings:       total time =    1765.05 ms /   338 tokens\n",
            " 74%|███████▍  | 111/150 [1:15:21<30:50, 47.45s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      66.01 ms /   106 runs   (    0.62 ms per token,  1605.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5724.77 ms /  1076 tokens (    5.32 ms per token,   187.96 tokens per second)\n",
            "llama_print_timings:        eval time =    4215.97 ms /   105 runs   (   40.15 ms per token,    24.91 tokens per second)\n",
            "llama_print_timings:       total time =   10092.21 ms /  1181 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      50.88 ms /    90 runs   (    0.57 ms per token,  1768.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6283.79 ms /  1165 tokens (    5.39 ms per token,   185.40 tokens per second)\n",
            "llama_print_timings:        eval time =    3580.18 ms /    89 runs   (   40.23 ms per token,    24.86 tokens per second)\n",
            "llama_print_timings:       total time =    9975.92 ms /  1254 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.94 ms /     2 runs   (    0.47 ms per token,  2139.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1027.43 ms /   208 tokens (    4.94 ms per token,   202.45 tokens per second)\n",
            "llama_print_timings:        eval time =      36.84 ms /     1 runs   (   36.84 ms per token,    27.14 tokens per second)\n",
            "llama_print_timings:       total time =    1068.65 ms /   209 tokens\n",
            " 75%|███████▍  | 112/150 [1:15:52<26:56, 42.53s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     139.58 ms /   256 runs   (    0.55 ms per token,  1834.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13542.51 ms /  2310 tokens (    5.86 ms per token,   170.57 tokens per second)\n",
            "llama_print_timings:        eval time =   11718.04 ms /   255 runs   (   45.95 ms per token,    21.76 tokens per second)\n",
            "llama_print_timings:       total time =   25657.35 ms /  2565 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     148.76 ms /   256 runs   (    0.58 ms per token,  1720.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14812.02 ms /  2488 tokens (    5.95 ms per token,   167.97 tokens per second)\n",
            "llama_print_timings:        eval time =   12380.55 ms /   255 runs   (   48.55 ms per token,    20.60 tokens per second)\n",
            "llama_print_timings:       total time =   27599.61 ms /  2743 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      21.25 ms /    42 runs   (    0.51 ms per token,  1976.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1673.87 ms /   333 tokens (    5.03 ms per token,   198.94 tokens per second)\n",
            "llama_print_timings:        eval time =    1549.99 ms /    41 runs   (   37.80 ms per token,    26.45 tokens per second)\n",
            "llama_print_timings:       total time =    3267.57 ms /   374 tokens\n",
            " 75%|███████▌  | 113/150 [1:16:58<30:27, 49.40s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     140.07 ms /   256 runs   (    0.55 ms per token,  1827.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13537.39 ms /  2317 tokens (    5.84 ms per token,   171.16 tokens per second)\n",
            "llama_print_timings:        eval time =   11656.27 ms /   255 runs   (   45.71 ms per token,    21.88 tokens per second)\n",
            "llama_print_timings:       total time =   25566.06 ms /  2572 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     131.91 ms /   256 runs   (    0.52 ms per token,  1940.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15159.98 ms /  2541 tokens (    5.97 ms per token,   167.61 tokens per second)\n",
            "llama_print_timings:        eval time =   12464.73 ms /   255 runs   (   48.88 ms per token,    20.46 tokens per second)\n",
            "llama_print_timings:       total time =   27985.43 ms /  2796 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.94 ms /     2 runs   (    0.47 ms per token,  2136.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1671.52 ms /   334 tokens (    5.00 ms per token,   199.82 tokens per second)\n",
            "llama_print_timings:        eval time =      37.20 ms /     1 runs   (   37.20 ms per token,    26.88 tokens per second)\n",
            "llama_print_timings:       total time =    1715.16 ms /   335 tokens\n",
            " 76%|███████▌  | 114/150 [1:17:58<31:29, 52.49s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      87.77 ms /   145 runs   (    0.61 ms per token,  1652.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12444.29 ms /  2157 tokens (    5.77 ms per token,   173.33 tokens per second)\n",
            "llama_print_timings:        eval time =    6297.33 ms /   144 runs   (   43.73 ms per token,    22.87 tokens per second)\n",
            "llama_print_timings:       total time =   18963.36 ms /  2301 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.33 ms /    86 runs   (    0.55 ms per token,  1817.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13133.44 ms /  2228 tokens (    5.89 ms per token,   169.64 tokens per second)\n",
            "llama_print_timings:        eval time =    3732.50 ms /    85 runs   (   43.91 ms per token,    22.77 tokens per second)\n",
            "llama_print_timings:       total time =   16988.57 ms /  2313 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.50 ms per token,  2010.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1114.16 ms /   222 tokens (    5.02 ms per token,   199.25 tokens per second)\n",
            "llama_print_timings:        eval time =      37.11 ms /     1 runs   (   37.11 ms per token,    26.95 tokens per second)\n",
            "llama_print_timings:       total time =    1155.92 ms /   223 tokens\n",
            " 77%|███████▋  | 115/150 [1:18:50<30:36, 52.47s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      19.04 ms /    27 runs   (    0.71 ms per token,  1418.14 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10800.64 ms /  1899 tokens (    5.69 ms per token,   175.82 tokens per second)\n",
            "llama_print_timings:        eval time =    1110.96 ms /    26 runs   (   42.73 ms per token,    23.40 tokens per second)\n",
            "llama_print_timings:       total time =   11971.67 ms /  1925 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.46 ms /    76 runs   (    0.69 ms per token,  1448.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10746.22 ms /  1896 tokens (    5.67 ms per token,   176.43 tokens per second)\n",
            "llama_print_timings:        eval time =    3257.54 ms /    76 runs   (   42.86 ms per token,    23.33 tokens per second)\n",
            "llama_print_timings:       total time =   14140.49 ms /  1972 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1885.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =     545.34 ms /   107 tokens (    5.10 ms per token,   196.21 tokens per second)\n",
            "llama_print_timings:        eval time =      36.70 ms /     1 runs   (   36.70 ms per token,    27.25 tokens per second)\n",
            "llama_print_timings:       total time =     585.83 ms /   108 tokens\n",
            " 77%|███████▋  | 116/150 [1:19:25<26:43, 47.18s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     137.70 ms /   256 runs   (    0.54 ms per token,  1859.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11802.92 ms /  2056 tokens (    5.74 ms per token,   174.19 tokens per second)\n",
            "llama_print_timings:        eval time =   11077.52 ms /   256 runs   (   43.27 ms per token,    23.11 tokens per second)\n",
            "llama_print_timings:       total time =   23244.49 ms /  2312 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     137.06 ms /   256 runs   (    0.54 ms per token,  1867.78 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13126.66 ms /  2244 tokens (    5.85 ms per token,   170.95 tokens per second)\n",
            "llama_print_timings:        eval time =   11311.23 ms /   255 runs   (   44.36 ms per token,    22.54 tokens per second)\n",
            "llama_print_timings:       total time =   24789.98 ms /  2499 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      62.73 ms /   100 runs   (    0.63 ms per token,  1594.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1692.42 ms /   333 tokens (    5.08 ms per token,   196.76 tokens per second)\n",
            "llama_print_timings:        eval time =    3781.96 ms /    99 runs   (   38.20 ms per token,    26.18 tokens per second)\n",
            "llama_print_timings:       total time =    5601.80 ms /   432 tokens\n",
            " 78%|███████▊  | 117/150 [1:20:22<27:39, 50.30s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      64.21 ms /   122 runs   (    0.53 ms per token,  1900.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10357.47 ms /  1832 tokens (    5.65 ms per token,   176.88 tokens per second)\n",
            "llama_print_timings:        eval time =    5121.46 ms /   121 runs   (   42.33 ms per token,    23.63 tokens per second)\n",
            "llama_print_timings:       total time =   15635.67 ms /  1953 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      73.52 ms /   136 runs   (    0.54 ms per token,  1849.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11007.85 ms /  1927 tokens (    5.71 ms per token,   175.06 tokens per second)\n",
            "llama_print_timings:        eval time =    5769.06 ms /   135 runs   (   42.73 ms per token,    23.40 tokens per second)\n",
            "llama_print_timings:       total time =   16958.78 ms /  2062 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.41 ms /     2 runs   (    0.70 ms per token,  1423.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1083.84 ms /   216 tokens (    5.02 ms per token,   199.29 tokens per second)\n",
            "llama_print_timings:        eval time =      37.37 ms /     1 runs   (   37.37 ms per token,    26.76 tokens per second)\n",
            "llama_print_timings:       total time =    1126.86 ms /   217 tokens\n",
            " 79%|███████▊  | 118/150 [1:21:01<24:58, 46.83s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      72.25 ms /   116 runs   (    0.62 ms per token,  1605.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5523.59 ms /  1039 tokens (    5.32 ms per token,   188.10 tokens per second)\n",
            "llama_print_timings:        eval time =    4589.12 ms /   115 runs   (   39.91 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =   10264.93 ms /  1154 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      89.51 ms /   139 runs   (    0.64 ms per token,  1552.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6029.67 ms /  1125 tokens (    5.36 ms per token,   186.58 tokens per second)\n",
            "llama_print_timings:        eval time =    5603.16 ms /   138 runs   (   40.60 ms per token,    24.63 tokens per second)\n",
            "llama_print_timings:       total time =   11837.83 ms /  1263 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     124.08 ms /   222 runs   (    0.56 ms per token,  1789.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1069.07 ms /   214 tokens (    5.00 ms per token,   200.17 tokens per second)\n",
            "llama_print_timings:        eval time =    8385.46 ms /   221 runs   (   37.94 ms per token,    26.36 tokens per second)\n",
            "llama_print_timings:       total time =    9714.20 ms /   435 tokens\n",
            " 79%|███████▉  | 119/150 [1:21:42<23:18, 45.12s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      94.89 ms /   165 runs   (    0.58 ms per token,  1738.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11267.88 ms /  1973 tokens (    5.71 ms per token,   175.10 tokens per second)\n",
            "llama_print_timings:        eval time =    7061.97 ms /   164 runs   (   43.06 ms per token,    23.22 tokens per second)\n",
            "llama_print_timings:       total time =   18571.89 ms /  2137 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      54.81 ms /    79 runs   (    0.69 ms per token,  1441.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =   12141.52 ms /  2091 tokens (    5.81 ms per token,   172.22 tokens per second)\n",
            "llama_print_timings:        eval time =    3394.09 ms /    78 runs   (   43.51 ms per token,    22.98 tokens per second)\n",
            "llama_print_timings:       total time =   15676.82 ms /  2169 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.92 ms /     2 runs   (    0.46 ms per token,  2169.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1192.24 ms /   240 tokens (    4.97 ms per token,   201.30 tokens per second)\n",
            "llama_print_timings:        eval time =      74.80 ms /     2 runs   (   37.40 ms per token,    26.74 tokens per second)\n",
            "llama_print_timings:       total time =    1271.96 ms /   242 tokens\n",
            " 80%|████████  | 120/150 [1:22:30<22:54, 45.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      91.86 ms /   142 runs   (    0.65 ms per token,  1545.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6229.27 ms /  1167 tokens (    5.34 ms per token,   187.34 tokens per second)\n",
            "llama_print_timings:        eval time =    5699.61 ms /   141 runs   (   40.42 ms per token,    24.74 tokens per second)\n",
            "llama_print_timings:       total time =   12133.14 ms /  1308 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     134.67 ms /   213 runs   (    0.63 ms per token,  1581.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7044.41 ms /  1295 tokens (    5.44 ms per token,   183.83 tokens per second)\n",
            "llama_print_timings:        eval time =    8737.10 ms /   212 runs   (   41.21 ms per token,    24.26 tokens per second)\n",
            "llama_print_timings:       total time =   16092.61 ms /  1507 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      98.03 ms /   181 runs   (    0.54 ms per token,  1846.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1309.43 ms /   261 tokens (    5.02 ms per token,   199.32 tokens per second)\n",
            "llama_print_timings:        eval time =    6831.48 ms /   180 runs   (   37.95 ms per token,    26.35 tokens per second)\n",
            "llama_print_timings:       total time =    8338.33 ms /   441 tokens\n",
            " 81%|████████  | 121/150 [1:23:18<22:30, 46.56s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      21.49 ms /    40 runs   (    0.54 ms per token,  1861.33 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11312.96 ms /  1978 tokens (    5.72 ms per token,   174.84 tokens per second)\n",
            "llama_print_timings:        eval time =    1663.09 ms /    39 runs   (   42.64 ms per token,    23.45 tokens per second)\n",
            "llama_print_timings:       total time =   13042.19 ms /  2017 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.16 ms /    99 runs   (    0.53 ms per token,  1898.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11537.57 ms /  1998 tokens (    5.77 ms per token,   173.17 tokens per second)\n",
            "llama_print_timings:        eval time =    4207.07 ms /    98 runs   (   42.93 ms per token,    23.29 tokens per second)\n",
            "llama_print_timings:       total time =   15875.96 ms /  2096 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1888.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =     625.73 ms /   125 tokens (    5.01 ms per token,   199.77 tokens per second)\n",
            "llama_print_timings:        eval time =      36.73 ms /     1 runs   (   36.73 ms per token,    27.23 tokens per second)\n",
            "llama_print_timings:       total time =     666.53 ms /   126 tokens\n",
            " 81%|████████▏ | 122/150 [1:23:57<20:40, 44.31s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      23.18 ms /    39 runs   (    0.59 ms per token,  1682.48 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7813.90 ms /  1426 tokens (    5.48 ms per token,   182.50 tokens per second)\n",
            "llama_print_timings:        eval time =    1569.74 ms /    38 runs   (   41.31 ms per token,    24.21 tokens per second)\n",
            "llama_print_timings:       total time =    9450.43 ms /  1464 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      36.68 ms /    64 runs   (    0.57 ms per token,  1745.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7869.94 ms /  1431 tokens (    5.50 ms per token,   181.83 tokens per second)\n",
            "llama_print_timings:        eval time =    2598.16 ms /    63 runs   (   41.24 ms per token,    24.25 tokens per second)\n",
            "llama_print_timings:       total time =   10553.27 ms /  1494 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.87 ms /    48 runs   (    0.66 ms per token,  1506.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =     667.97 ms /   135 tokens (    4.95 ms per token,   202.10 tokens per second)\n",
            "llama_print_timings:        eval time =    1781.53 ms /    47 runs   (   37.90 ms per token,    26.38 tokens per second)\n",
            "llama_print_timings:       total time =    2511.54 ms /   182 tokens\n",
            " 82%|████████▏ | 123/150 [1:24:28<18:10, 40.40s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     144.01 ms /   256 runs   (    0.56 ms per token,  1777.60 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10765.93 ms /  1896 tokens (    5.68 ms per token,   176.11 tokens per second)\n",
            "llama_print_timings:        eval time =   10957.60 ms /   255 runs   (   42.97 ms per token,    23.27 tokens per second)\n",
            "llama_print_timings:       total time =   22099.85 ms /  2151 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     144.15 ms /   256 runs   (    0.56 ms per token,  1775.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11959.96 ms /  2067 tokens (    5.79 ms per token,   172.83 tokens per second)\n",
            "llama_print_timings:        eval time =   11047.56 ms /   255 runs   (   43.32 ms per token,    23.08 tokens per second)\n",
            "llama_print_timings:       total time =   23360.46 ms /  2322 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.19 ms /     2 runs   (    0.60 ms per token,  1675.04 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1683.86 ms /   336 tokens (    5.01 ms per token,   199.54 tokens per second)\n",
            "llama_print_timings:        eval time =      37.97 ms /     1 runs   (   37.97 ms per token,    26.34 tokens per second)\n",
            "llama_print_timings:       total time =    1728.51 ms /   337 tokens\n",
            " 83%|████████▎ | 124/150 [1:25:24<19:32, 45.10s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      52.72 ms /    74 runs   (    0.71 ms per token,  1403.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10629.30 ms /  1878 tokens (    5.66 ms per token,   176.68 tokens per second)\n",
            "llama_print_timings:        eval time =    3106.82 ms /    73 runs   (   42.56 ms per token,    23.50 tokens per second)\n",
            "llama_print_timings:       total time =   13863.29 ms /  1951 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      45.00 ms /    67 runs   (    0.67 ms per token,  1488.76 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10702.79 ms /  1884 tokens (    5.68 ms per token,   176.03 tokens per second)\n",
            "llama_print_timings:        eval time =    2822.72 ms /    66 runs   (   42.77 ms per token,    23.38 tokens per second)\n",
            "llama_print_timings:       total time =   13637.58 ms /  1950 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.14 ms /     2 runs   (    0.57 ms per token,  1755.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     745.05 ms /   151 tokens (    4.93 ms per token,   202.67 tokens per second)\n",
            "llama_print_timings:        eval time =      37.04 ms /     1 runs   (   37.04 ms per token,    27.00 tokens per second)\n",
            "llama_print_timings:       total time =     786.94 ms /   152 tokens\n",
            " 83%|████████▎ | 125/150 [1:25:57<17:14, 41.37s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      39.72 ms /    75 runs   (    0.53 ms per token,  1888.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11558.75 ms /  2014 tokens (    5.74 ms per token,   174.24 tokens per second)\n",
            "llama_print_timings:        eval time =    3174.70 ms /    74 runs   (   42.90 ms per token,    23.31 tokens per second)\n",
            "llama_print_timings:       total time =   14836.84 ms /  2088 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      67.22 ms /   117 runs   (    0.57 ms per token,  1740.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11819.78 ms /  2047 tokens (    5.77 ms per token,   173.18 tokens per second)\n",
            "llama_print_timings:        eval time =    5007.94 ms /   116 runs   (   43.17 ms per token,    23.16 tokens per second)\n",
            "llama_print_timings:       total time =   16992.22 ms /  2163 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      55.41 ms /    84 runs   (    0.66 ms per token,  1515.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =     872.56 ms /   170 tokens (    5.13 ms per token,   194.83 tokens per second)\n",
            "llama_print_timings:        eval time =    3141.62 ms /    83 runs   (   37.85 ms per token,    26.42 tokens per second)\n",
            "llama_print_timings:       total time =    4123.84 ms /   253 tokens\n",
            " 84%|████████▍ | 126/150 [1:26:39<16:37, 41.55s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     117.08 ms /   200 runs   (    0.59 ms per token,  1708.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6361.08 ms /  1180 tokens (    5.39 ms per token,   185.50 tokens per second)\n",
            "llama_print_timings:        eval time =    8082.15 ms /   199 runs   (   40.61 ms per token,    24.62 tokens per second)\n",
            "llama_print_timings:       total time =   14713.40 ms /  1379 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      64.21 ms /   116 runs   (    0.55 ms per token,  1806.57 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7433.30 ms /  1359 tokens (    5.47 ms per token,   182.83 tokens per second)\n",
            "llama_print_timings:        eval time =    4719.56 ms /   115 runs   (   41.04 ms per token,    24.37 tokens per second)\n",
            "llama_print_timings:       total time =   12304.95 ms /  1474 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      47.14 ms /    97 runs   (    0.49 ms per token,  2057.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1427.51 ms /   285 tokens (    5.01 ms per token,   199.65 tokens per second)\n",
            "llama_print_timings:        eval time =    3631.13 ms /    96 runs   (   37.82 ms per token,    26.44 tokens per second)\n",
            "llama_print_timings:       total time =    5155.56 ms /   381 tokens\n",
            " 85%|████████▍ | 127/150 [1:27:15<15:16, 39.86s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      54.69 ms /   105 runs   (    0.52 ms per token,  1920.09 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6393.81 ms /  1184 tokens (    5.40 ms per token,   185.18 tokens per second)\n",
            "llama_print_timings:        eval time =    4191.65 ms /   104 runs   (   40.30 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =   10706.97 ms /  1288 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      60.06 ms /   114 runs   (    0.53 ms per token,  1898.17 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6872.42 ms /  1264 tokens (    5.44 ms per token,   183.92 tokens per second)\n",
            "llama_print_timings:        eval time =    4586.75 ms /   113 runs   (   40.59 ms per token,    24.64 tokens per second)\n",
            "llama_print_timings:       total time =   11594.71 ms /  1377 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      35.53 ms /    67 runs   (    0.53 ms per token,  1885.68 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1028.61 ms /   208 tokens (    4.95 ms per token,   202.21 tokens per second)\n",
            "llama_print_timings:        eval time =    2478.33 ms /    66 runs   (   37.55 ms per token,    26.63 tokens per second)\n",
            "llama_print_timings:       total time =    3574.92 ms /   274 tokens\n",
            " 85%|████████▌ | 128/150 [1:27:43<13:20, 36.37s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      45.81 ms /    80 runs   (    0.57 ms per token,  1746.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8273.11 ms /  1494 tokens (    5.54 ms per token,   180.58 tokens per second)\n",
            "llama_print_timings:        eval time =    3283.83 ms /    79 runs   (   41.57 ms per token,    24.06 tokens per second)\n",
            "llama_print_timings:       total time =   11674.92 ms /  1573 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      65.89 ms /   123 runs   (    0.54 ms per token,  1866.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8675.03 ms /  1555 tokens (    5.58 ms per token,   179.25 tokens per second)\n",
            "llama_print_timings:        eval time =    5070.96 ms /   122 runs   (   41.57 ms per token,    24.06 tokens per second)\n",
            "llama_print_timings:       total time =   13899.65 ms /  1677 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     154.82 ms /   249 runs   (    0.62 ms per token,  1608.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     826.64 ms /   166 tokens (    4.98 ms per token,   200.81 tokens per second)\n",
            "llama_print_timings:        eval time =    9387.05 ms /   248 runs   (   37.85 ms per token,    26.42 tokens per second)\n",
            "llama_print_timings:       total time =   10535.69 ms /   414 tokens\n",
            " 86%|████████▌ | 129/150 [1:28:22<13:02, 37.24s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.66 ms /    47 runs   (    0.67 ms per token,  1484.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9241.31 ms /  1662 tokens (    5.56 ms per token,   179.84 tokens per second)\n",
            "llama_print_timings:        eval time =    1933.01 ms /    46 runs   (   42.02 ms per token,    23.80 tokens per second)\n",
            "llama_print_timings:       total time =   11257.65 ms /  1708 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     128.87 ms /   211 runs   (    0.61 ms per token,  1637.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9451.67 ms /  1684 tokens (    5.61 ms per token,   178.17 tokens per second)\n",
            "llama_print_timings:        eval time =    8890.97 ms /   210 runs   (   42.34 ms per token,    23.62 tokens per second)\n",
            "llama_print_timings:       total time =   18651.21 ms /  1894 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.04 ms /     2 runs   (    0.52 ms per token,  1921.23 tokens per second)\n",
            "llama_print_timings: prompt eval time =     708.33 ms /   140 tokens (    5.06 ms per token,   197.65 tokens per second)\n",
            "llama_print_timings:        eval time =      36.83 ms /     1 runs   (   36.83 ms per token,    27.15 tokens per second)\n",
            "llama_print_timings:       total time =     749.10 ms /   141 tokens\n",
            " 87%|████████▋ | 130/150 [1:29:01<12:30, 37.54s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      25.99 ms /    41 runs   (    0.63 ms per token,  1577.59 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5443.57 ms /  1032 tokens (    5.27 ms per token,   189.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1636.35 ms /    41 runs   (   39.91 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =    7141.54 ms /  1073 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      33.12 ms /    61 runs   (    0.54 ms per token,  1841.73 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5699.24 ms /  1064 tokens (    5.36 ms per token,   186.69 tokens per second)\n",
            "llama_print_timings:        eval time =    2399.98 ms /    60 runs   (   40.00 ms per token,    25.00 tokens per second)\n",
            "llama_print_timings:       total time =    8176.72 ms /  1124 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1867.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =     622.90 ms /   124 tokens (    5.02 ms per token,   199.07 tokens per second)\n",
            "llama_print_timings:        eval time =      37.41 ms /     1 runs   (   37.41 ms per token,    26.73 tokens per second)\n",
            "llama_print_timings:       total time =     664.48 ms /   125 tokens\n",
            " 87%|████████▋ | 131/150 [1:29:29<11:01, 34.81s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.82 ms /    67 runs   (    0.56 ms per token,  1771.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7139.25 ms /  1306 tokens (    5.47 ms per token,   182.93 tokens per second)\n",
            "llama_print_timings:        eval time =    2687.63 ms /    66 runs   (   40.72 ms per token,    24.56 tokens per second)\n",
            "llama_print_timings:       total time =    9914.91 ms /  1372 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      77.56 ms /   119 runs   (    0.65 ms per token,  1534.38 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7392.66 ms /  1352 tokens (    5.47 ms per token,   182.88 tokens per second)\n",
            "llama_print_timings:        eval time =    4908.29 ms /   118 runs   (   41.60 ms per token,    24.04 tokens per second)\n",
            "llama_print_timings:       total time =   12477.18 ms /  1470 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.54 ms per token,  1869.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     754.33 ms /   152 tokens (    4.96 ms per token,   201.50 tokens per second)\n",
            "llama_print_timings:        eval time =      74.52 ms /     2 runs   (   37.26 ms per token,    26.84 tokens per second)\n",
            "llama_print_timings:       total time =     832.95 ms /   154 tokens\n",
            " 88%|████████▊ | 132/150 [1:30:06<10:37, 35.43s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      14.05 ms /    24 runs   (    0.59 ms per token,  1708.31 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5811.55 ms /  1093 tokens (    5.32 ms per token,   188.07 tokens per second)\n",
            "llama_print_timings:        eval time =     917.91 ms /    23 runs   (   39.91 ms per token,    25.06 tokens per second)\n",
            "llama_print_timings:       total time =    6767.48 ms /  1116 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      84.71 ms /   150 runs   (    0.56 ms per token,  1770.75 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5982.40 ms /  1108 tokens (    5.40 ms per token,   185.21 tokens per second)\n",
            "llama_print_timings:        eval time =    6005.94 ms /   149 runs   (   40.31 ms per token,    24.81 tokens per second)\n",
            "llama_print_timings:       total time =   12168.96 ms /  1257 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1886.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =     508.06 ms /   104 tokens (    4.89 ms per token,   204.70 tokens per second)\n",
            "llama_print_timings:        eval time =      74.84 ms /     2 runs   (   37.42 ms per token,    26.72 tokens per second)\n",
            "llama_print_timings:       total time =     588.95 ms /   106 tokens\n",
            " 89%|████████▊ | 133/150 [1:30:39<09:51, 34.80s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      25.87 ms /    45 runs   (    0.57 ms per token,  1739.74 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4332.24 ms /   826 tokens (    5.24 ms per token,   190.66 tokens per second)\n",
            "llama_print_timings:        eval time =    1729.73 ms /    44 runs   (   39.31 ms per token,    25.44 tokens per second)\n",
            "llama_print_timings:       total time =    6117.26 ms /   870 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     104.14 ms /   181 runs   (    0.58 ms per token,  1737.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4536.34 ms /   858 tokens (    5.29 ms per token,   189.14 tokens per second)\n",
            "llama_print_timings:        eval time =    7112.20 ms /   180 runs   (   39.51 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =   11877.10 ms /  1038 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.07 ms /     2 runs   (    0.53 ms per token,  1870.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     624.36 ms /   122 tokens (    5.12 ms per token,   195.40 tokens per second)\n",
            "llama_print_timings:        eval time =      36.73 ms /     1 runs   (   36.73 ms per token,    27.22 tokens per second)\n",
            "llama_print_timings:       total time =     664.90 ms /   123 tokens\n",
            " 89%|████████▉ | 134/150 [1:31:04<08:26, 31.64s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.00 ms /    74 runs   (    0.54 ms per token,  1850.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =    9629.51 ms /  1720 tokens (    5.60 ms per token,   178.62 tokens per second)\n",
            "llama_print_timings:        eval time =    3105.13 ms /    74 runs   (   41.96 ms per token,    23.83 tokens per second)\n",
            "llama_print_timings:       total time =   12832.92 ms /  1794 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      36.09 ms /    66 runs   (    0.55 ms per token,  1828.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10066.60 ms /  1773 tokens (    5.68 ms per token,   176.13 tokens per second)\n",
            "llama_print_timings:        eval time =    2756.48 ms /    65 runs   (   42.41 ms per token,    23.58 tokens per second)\n",
            "llama_print_timings:       total time =   12914.33 ms /  1838 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.17 ms /     2 runs   (    0.59 ms per token,  1707.94 tokens per second)\n",
            "llama_print_timings: prompt eval time =     748.38 ms /   151 tokens (    4.96 ms per token,   201.77 tokens per second)\n",
            "llama_print_timings:        eval time =      37.62 ms /     1 runs   (   37.62 ms per token,    26.58 tokens per second)\n",
            "llama_print_timings:       total time =     790.15 ms /   152 tokens\n",
            " 90%|█████████ | 135/150 [1:31:42<08:26, 33.75s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      35.35 ms /    57 runs   (    0.62 ms per token,  1612.26 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8451.55 ms /  1532 tokens (    5.52 ms per token,   181.27 tokens per second)\n",
            "llama_print_timings:        eval time =    2332.33 ms /    56 runs   (   41.65 ms per token,    24.01 tokens per second)\n",
            "llama_print_timings:       total time =   10874.71 ms /  1588 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      49.70 ms /    88 runs   (    0.56 ms per token,  1770.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8846.34 ms /  1576 tokens (    5.61 ms per token,   178.15 tokens per second)\n",
            "llama_print_timings:        eval time =    3669.04 ms /    88 runs   (   41.69 ms per token,    23.98 tokens per second)\n",
            "llama_print_timings:       total time =   12634.45 ms /  1664 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.57 ms per token,  1746.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =     705.14 ms /   140 tokens (    5.04 ms per token,   198.54 tokens per second)\n",
            "llama_print_timings:        eval time =      36.73 ms /     1 runs   (   36.73 ms per token,    27.22 tokens per second)\n",
            "llama_print_timings:       total time =     745.65 ms /   141 tokens\n",
            " 91%|█████████ | 136/150 [1:32:15<07:47, 33.41s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      29.59 ms /    53 runs   (    0.56 ms per token,  1791.39 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8776.24 ms /  1580 tokens (    5.55 ms per token,   180.03 tokens per second)\n",
            "llama_print_timings:        eval time =    2154.25 ms /    52 runs   (   41.43 ms per token,    24.14 tokens per second)\n",
            "llama_print_timings:       total time =   11005.48 ms /  1632 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      53.52 ms /    89 runs   (    0.60 ms per token,  1662.96 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8982.66 ms /  1608 tokens (    5.59 ms per token,   179.01 tokens per second)\n",
            "llama_print_timings:        eval time =    3694.98 ms /    88 runs   (   41.99 ms per token,    23.82 tokens per second)\n",
            "llama_print_timings:       total time =   12799.84 ms /  1696 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.15 ms /     2 runs   (    0.58 ms per token,  1733.10 tokens per second)\n",
            "llama_print_timings: prompt eval time =     627.67 ms /   127 tokens (    4.94 ms per token,   202.34 tokens per second)\n",
            "llama_print_timings:        eval time =      37.23 ms /     1 runs   (   37.23 ms per token,    26.86 tokens per second)\n",
            "llama_print_timings:       total time =     669.03 ms /   128 tokens\n",
            " 91%|█████████▏| 137/150 [1:32:50<07:22, 34.03s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      27.17 ms /    51 runs   (    0.53 ms per token,  1876.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7244.62 ms /  1336 tokens (    5.42 ms per token,   184.41 tokens per second)\n",
            "llama_print_timings:        eval time =    2035.68 ms /    50 runs   (   40.71 ms per token,    24.56 tokens per second)\n",
            "llama_print_timings:       total time =    9347.47 ms /  1386 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     101.14 ms /   177 runs   (    0.57 ms per token,  1750.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7514.21 ms /  1368 tokens (    5.49 ms per token,   182.06 tokens per second)\n",
            "llama_print_timings:        eval time =    7266.80 ms /   176 runs   (   41.29 ms per token,    24.22 tokens per second)\n",
            "llama_print_timings:       total time =   15008.59 ms /  1544 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.19 ms /     2 runs   (    0.59 ms per token,  1683.50 tokens per second)\n",
            "llama_print_timings: prompt eval time =     755.79 ms /   148 tokens (    5.11 ms per token,   195.82 tokens per second)\n",
            "llama_print_timings:        eval time =      37.52 ms /     1 runs   (   37.52 ms per token,    26.65 tokens per second)\n",
            "llama_print_timings:       total time =     797.66 ms /   149 tokens\n",
            " 92%|█████████▏| 138/150 [1:33:25<06:48, 34.08s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      66.99 ms /   112 runs   (    0.60 ms per token,  1671.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6533.92 ms /  1212 tokens (    5.39 ms per token,   185.49 tokens per second)\n",
            "llama_print_timings:        eval time =    4511.56 ms /   111 runs   (   40.64 ms per token,    24.60 tokens per second)\n",
            "llama_print_timings:       total time =   11214.47 ms /  1323 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      40.63 ms /    65 runs   (    0.63 ms per token,  1599.72 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7111.90 ms /  1308 tokens (    5.44 ms per token,   183.92 tokens per second)\n",
            "llama_print_timings:        eval time =    2618.81 ms /    64 runs   (   40.92 ms per token,    24.44 tokens per second)\n",
            "llama_print_timings:       total time =    9826.55 ms /  1372 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     149.11 ms /   256 runs   (    0.58 ms per token,  1716.81 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1242.62 ms /   248 tokens (    5.01 ms per token,   199.58 tokens per second)\n",
            "llama_print_timings:        eval time =    9736.93 ms /   256 runs   (   38.03 ms per token,    26.29 tokens per second)\n",
            "llama_print_timings:       total time =   11290.92 ms /   504 tokens\n",
            " 93%|█████████▎| 139/150 [1:34:04<06:32, 35.72s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      66.19 ms /   118 runs   (    0.56 ms per token,  1782.69 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5502.75 ms /  1027 tokens (    5.36 ms per token,   186.63 tokens per second)\n",
            "llama_print_timings:        eval time =    4659.23 ms /   117 runs   (   39.82 ms per token,    25.11 tokens per second)\n",
            "llama_print_timings:       total time =   10304.43 ms /  1144 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      48.24 ms /    87 runs   (    0.55 ms per token,  1803.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6058.05 ms /  1126 tokens (    5.38 ms per token,   185.87 tokens per second)\n",
            "llama_print_timings:        eval time =    3455.08 ms /    86 runs   (   40.18 ms per token,    24.89 tokens per second)\n",
            "llama_print_timings:       total time =    9620.83 ms /  1212 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.50 ms per token,  2012.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1306.06 ms /   259 tokens (    5.04 ms per token,   198.31 tokens per second)\n",
            "llama_print_timings:        eval time =      37.09 ms /     1 runs   (   37.09 ms per token,    26.96 tokens per second)\n",
            "llama_print_timings:       total time =    1347.95 ms /   260 tokens\n",
            " 93%|█████████▎| 140/150 [1:34:28<05:21, 32.15s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      44.13 ms /    82 runs   (    0.54 ms per token,  1857.98 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7391.93 ms /  1350 tokens (    5.48 ms per token,   182.63 tokens per second)\n",
            "llama_print_timings:        eval time =    3305.41 ms /    81 runs   (   40.81 ms per token,    24.51 tokens per second)\n",
            "llama_print_timings:       total time =   10800.14 ms /  1431 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      84.82 ms /   135 runs   (    0.63 ms per token,  1591.62 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7703.04 ms /  1405 tokens (    5.48 ms per token,   182.40 tokens per second)\n",
            "llama_print_timings:        eval time =    5531.38 ms /   134 runs   (   41.28 ms per token,    24.23 tokens per second)\n",
            "llama_print_timings:       total time =   13423.23 ms /  1539 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.02 ms /     2 runs   (    0.51 ms per token,  1951.22 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1024.74 ms /   204 tokens (    5.02 ms per token,   199.07 tokens per second)\n",
            "llama_print_timings:        eval time =      37.10 ms /     1 runs   (   37.10 ms per token,    26.95 tokens per second)\n",
            "llama_print_timings:       total time =    1066.80 ms /   205 tokens\n",
            " 94%|█████████▍| 141/150 [1:34:56<04:37, 30.87s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      22.08 ms /    32 runs   (    0.69 ms per token,  1449.28 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5132.18 ms /   965 tokens (    5.32 ms per token,   188.03 tokens per second)\n",
            "llama_print_timings:        eval time =    1232.90 ms /    31 runs   (   39.77 ms per token,    25.14 tokens per second)\n",
            "llama_print_timings:       total time =    6420.40 ms /   996 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      22.46 ms /    40 runs   (    0.56 ms per token,  1780.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5190.98 ms /   978 tokens (    5.31 ms per token,   188.40 tokens per second)\n",
            "llama_print_timings:        eval time =    1546.13 ms /    39 runs   (   39.64 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    6789.83 ms /  1017 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       0.99 ms /     2 runs   (    0.50 ms per token,  2012.07 tokens per second)\n",
            "llama_print_timings: prompt eval time =     824.63 ms /   168 tokens (    4.91 ms per token,   203.73 tokens per second)\n",
            "llama_print_timings:        eval time =      75.42 ms /     2 runs   (   37.71 ms per token,    26.52 tokens per second)\n",
            "llama_print_timings:       total time =     903.97 ms /   170 tokens\n",
            " 95%|█████████▍| 142/150 [1:35:14<03:35, 26.99s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      21.24 ms /    37 runs   (    0.57 ms per token,  1741.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4766.57 ms /   906 tokens (    5.26 ms per token,   190.07 tokens per second)\n",
            "llama_print_timings:        eval time =    1427.52 ms /    36 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
            "llama_print_timings:       total time =    6245.40 ms /   942 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      68.89 ms /   120 runs   (    0.57 ms per token,  1741.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4785.17 ms /   904 tokens (    5.29 ms per token,   188.92 tokens per second)\n",
            "llama_print_timings:        eval time =    4740.52 ms /   120 runs   (   39.50 ms per token,    25.31 tokens per second)\n",
            "llama_print_timings:       total time =    9669.31 ms /  1024 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1673.64 tokens per second)\n",
            "llama_print_timings: prompt eval time =     628.97 ms /   127 tokens (    4.95 ms per token,   201.92 tokens per second)\n",
            "llama_print_timings:        eval time =      37.22 ms /     1 runs   (   37.22 ms per token,    26.87 tokens per second)\n",
            "llama_print_timings:       total time =     669.94 ms /   128 tokens\n",
            " 95%|█████████▌| 143/150 [1:35:39<03:05, 26.51s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      25.95 ms /    40 runs   (    0.65 ms per token,  1541.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8050.66 ms /  1462 tokens (    5.51 ms per token,   181.60 tokens per second)\n",
            "llama_print_timings:        eval time =    1619.37 ms /    39 runs   (   41.52 ms per token,    24.08 tokens per second)\n",
            "llama_print_timings:       total time =    9739.33 ms /  1501 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     168.91 ms /   256 runs   (    0.66 ms per token,  1515.63 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8163.93 ms /  1480 tokens (    5.52 ms per token,   181.29 tokens per second)\n",
            "llama_print_timings:        eval time =   10744.85 ms /   256 runs   (   41.97 ms per token,    23.83 tokens per second)\n",
            "llama_print_timings:       total time =   19340.66 ms /  1736 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      37.11 ms /    66 runs   (    0.56 ms per token,  1778.35 tokens per second)\n",
            "llama_print_timings: prompt eval time =     704.84 ms /   141 tokens (    5.00 ms per token,   200.05 tokens per second)\n",
            "llama_print_timings:        eval time =    2437.65 ms /    65 runs   (   37.50 ms per token,    26.67 tokens per second)\n",
            "llama_print_timings:       total time =    3210.65 ms /   206 tokens\n",
            " 96%|█████████▌| 144/150 [1:36:21<03:06, 31.10s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     150.83 ms /   249 runs   (    0.61 ms per token,  1650.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11782.11 ms /  2055 tokens (    5.73 ms per token,   174.42 tokens per second)\n",
            "llama_print_timings:        eval time =   10765.67 ms /   248 runs   (   43.41 ms per token,    23.04 tokens per second)\n",
            "llama_print_timings:       total time =   22930.71 ms /  2303 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     141.37 ms /   256 runs   (    0.55 ms per token,  1810.84 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13321.95 ms /  2268 tokens (    5.87 ms per token,   170.25 tokens per second)\n",
            "llama_print_timings:        eval time =   11429.80 ms /   255 runs   (   44.82 ms per token,    22.31 tokens per second)\n",
            "llama_print_timings:       total time =   25105.85 ms /  2523 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.21 ms /     2 runs   (    0.61 ms per token,  1647.45 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1784.10 ms /   351 tokens (    5.08 ms per token,   196.74 tokens per second)\n",
            "llama_print_timings:        eval time =      37.97 ms /     1 runs   (   37.97 ms per token,    26.34 tokens per second)\n",
            "llama_print_timings:       total time =    1829.52 ms /   352 tokens\n",
            " 97%|█████████▋| 145/150 [1:37:22<03:20, 40.07s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      93.57 ms /   160 runs   (    0.58 ms per token,  1709.97 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10696.47 ms /  1887 tokens (    5.67 ms per token,   176.41 tokens per second)\n",
            "llama_print_timings:        eval time =    6777.51 ms /   159 runs   (   42.63 ms per token,    23.46 tokens per second)\n",
            "llama_print_timings:       total time =   17694.38 ms /  2046 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      33.27 ms /    52 runs   (    0.64 ms per token,  1563.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =   11781.51 ms /  2037 tokens (    5.78 ms per token,   172.90 tokens per second)\n",
            "llama_print_timings:        eval time =    2207.45 ms /    51 runs   (   43.28 ms per token,    23.10 tokens per second)\n",
            "llama_print_timings:       total time =   14081.14 ms /  2088 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      53.71 ms /    89 runs   (    0.60 ms per token,  1656.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1452.71 ms /   284 tokens (    5.12 ms per token,   195.50 tokens per second)\n",
            "llama_print_timings:        eval time =    3353.41 ms /    88 runs   (   38.11 ms per token,    26.24 tokens per second)\n",
            "llama_print_timings:       total time =    4913.35 ms /   372 tokens\n",
            " 97%|█████████▋| 146/150 [1:38:08<02:47, 41.84s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      42.15 ms /    80 runs   (    0.53 ms per token,  1897.80 tokens per second)\n",
            "llama_print_timings: prompt eval time =    5855.15 ms /  1096 tokens (    5.34 ms per token,   187.19 tokens per second)\n",
            "llama_print_timings:        eval time =    3189.50 ms /    80 runs   (   39.87 ms per token,    25.08 tokens per second)\n",
            "llama_print_timings:       total time =    9139.33 ms /  1176 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.23 ms /     2 runs   (    0.61 ms per token,  1628.66 tokens per second)\n",
            "llama_print_timings: prompt eval time =    6243.75 ms /  1159 tokens (    5.39 ms per token,   185.63 tokens per second)\n",
            "llama_print_timings:        eval time =      40.15 ms /     1 runs   (   40.15 ms per token,    24.90 tokens per second)\n",
            "llama_print_timings:       total time =    6300.67 ms /  1160 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      31.52 ms /    55 runs   (    0.57 ms per token,  1745.15 tokens per second)\n",
            "llama_print_timings: prompt eval time =     835.34 ms /   168 tokens (    4.97 ms per token,   201.12 tokens per second)\n",
            "llama_print_timings:        eval time =    2030.12 ms /    54 runs   (   37.59 ms per token,    26.60 tokens per second)\n",
            "llama_print_timings:       total time =    2929.13 ms /   222 tokens\n",
            " 98%|█████████▊| 147/150 [1:38:35<01:52, 37.55s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     140.94 ms /   256 runs   (    0.55 ms per token,  1816.32 tokens per second)\n",
            "llama_print_timings: prompt eval time =   14025.96 ms /  2378 tokens (    5.90 ms per token,   169.54 tokens per second)\n",
            "llama_print_timings:        eval time =   12065.34 ms /   255 runs   (   47.32 ms per token,    21.13 tokens per second)\n",
            "llama_print_timings:       total time =   26475.30 ms /  2633 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     138.06 ms /   256 runs   (    0.54 ms per token,  1854.27 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15114.29 ms /  2528 tokens (    5.98 ms per token,   167.26 tokens per second)\n",
            "llama_print_timings:        eval time =   12435.19 ms /   255 runs   (   48.77 ms per token,    20.51 tokens per second)\n",
            "llama_print_timings:       total time =   27937.05 ms /  2783 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      20.91 ms /    38 runs   (    0.55 ms per token,  1817.49 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1671.67 ms /   333 tokens (    5.02 ms per token,   199.20 tokens per second)\n",
            "llama_print_timings:        eval time =    1399.77 ms /    37 runs   (   37.83 ms per token,    26.43 tokens per second)\n",
            "llama_print_timings:       total time =    3114.01 ms /   370 tokens\n",
            " 99%|█████████▊| 148/150 [1:39:46<01:34, 47.34s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =      27.63 ms /    46 runs   (    0.60 ms per token,  1664.92 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10127.89 ms /  1807 tokens (    5.60 ms per token,   178.42 tokens per second)\n",
            "llama_print_timings:        eval time =    1905.04 ms /    45 runs   (   42.33 ms per token,    23.62 tokens per second)\n",
            "llama_print_timings:       total time =   12115.24 ms /  1852 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     106.83 ms /   165 runs   (    0.65 ms per token,  1544.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =   10212.14 ms /  1806 tokens (    5.65 ms per token,   176.85 tokens per second)\n",
            "llama_print_timings:        eval time =    7069.21 ms /   164 runs   (   43.10 ms per token,    23.20 tokens per second)\n",
            "llama_print_timings:       total time =   17565.62 ms /  1970 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.05 ms /     2 runs   (    0.52 ms per token,  1912.05 tokens per second)\n",
            "llama_print_timings: prompt eval time =     630.00 ms /   122 tokens (    5.16 ms per token,   193.65 tokens per second)\n",
            "llama_print_timings:        eval time =      36.93 ms /     1 runs   (   36.93 ms per token,    27.08 tokens per second)\n",
            "llama_print_timings:       total time =     670.20 ms /   123 tokens\n",
            " 99%|█████████▉| 149/150 [1:40:27<00:45, 45.63s/it]Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     133.65 ms /   256 runs   (    0.52 ms per token,  1915.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =   13969.77 ms /  2366 tokens (    5.90 ms per token,   169.37 tokens per second)\n",
            "llama_print_timings:        eval time =   11951.62 ms /   255 runs   (   46.87 ms per token,    21.34 tokens per second)\n",
            "llama_print_timings:       total time =   26281.47 ms /  2621 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =     141.94 ms /   256 runs   (    0.55 ms per token,  1803.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15043.12 ms /  2528 tokens (    5.95 ms per token,   168.05 tokens per second)\n",
            "llama_print_timings:        eval time =   12479.60 ms /   256 runs   (   48.75 ms per token,    20.51 tokens per second)\n",
            "llama_print_timings:       total time =   27892.03 ms /  2784 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =     521.11 ms\n",
            "llama_print_timings:      sample time =       1.06 ms /     2 runs   (    0.53 ms per token,  1886.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    1671.00 ms /   333 tokens (    5.02 ms per token,   199.28 tokens per second)\n",
            "llama_print_timings:        eval time =      37.16 ms /     1 runs   (   37.16 ms per token,    26.91 tokens per second)\n",
            "llama_print_timings:       total time =    1713.82 ms /   334 tokens\n",
            "100%|██████████| 150/150 [1:41:29<00:00, 40.60s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhwJcYwIJE3U",
        "outputId": "c7d43e6e-0121-44b9-91f3-43d64ca37d85"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    }
  ]
}